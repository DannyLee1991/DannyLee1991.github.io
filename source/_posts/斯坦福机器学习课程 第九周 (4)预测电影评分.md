title: 斯坦福机器学习课程 第九周 (4)预测电影评分
tags:
  - 算法
categories:
  - 算法
  - 机器学习
comments: true
date: 2017-05-28 08:44:00
mathjax: true
---

# 预测电影评分

Simple inline $a = b + c$.

## 问题制定

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/Rhg6r/problem-formulation)

在接下来的内容中，我想介绍给你们有关**推荐系统(recommender systems)**的内容。

我想讨论推荐系统有两个原因：

- **原因一：对工业界有至关重要的作用**

	在过去的几年中，我有时会去参观硅谷的各种科技类公司。我经常在那些公司里与开发机器学习应用的人交流。然后我问他们什么才是机器学习最重要的应用？或者什么样的机器学习的应用是你最想让它的表现得到改进的？我最常听到的回答其中之一就是 现在硅谷有好多个团队正试图建立更好的推荐系统。

	亚马逊、Netflix、eBay又或者苹果公司的iTunes Genius等，很多网站或者很多系统，试图向用户推荐新产品。比如说亚马逊向你推荐新书，Netflix 向你推荐新电影...。而这些推荐系统可能会看看你以前购买过什么书，或者以前你给哪些电影进行过评分。
	
	这些系统贡献了现今亚马逊收入的相当大一部分。而对像Netflix这样的公司，他们向用户推荐的电影占了用户观看的电影的相当大一部分。于是一个推荐系统其表现的一些改进，就能带来显著且即刻产生的影响。这种影响关系到许多公司的最终业绩。
	
	推荐系统在机器学习学术界是个很有意思的问题。如果我们去参加一个学术类的机器学习会议，
推荐系统的问题几乎得不到什么关注至少它是学术界当前动向里较小的一部分。但如果你看看正在发生的事对很多科技类公司而言，建立这些推荐系统似乎是优先要办的事。这就是我想在这门课中谈论推荐系统的原因之一。

- **原因二：自动学习到优良特征**
	
	对于机器学习来说，特征量是重要的。你选择的特征对你学习算法的表现有很大影响。
	
	在机器学习领域有这么一个宏大的想法，就是对于一些问题而言（可能不是所有问题），存在一些算法能**试图自动地替你学习到一组优良的特征量**。这样与其手动设计或者手动编写特征，你或许能够采用一种算法来自动学习到使用什么特征量。而推荐系统就是这种情形的一个例子。
	
### 推荐系统 问题表述	

以预测电影评分这个时兴的问题为例，假想你是一个销售或出租电影的网站，你让用户使用1至5颗星 给不同的电影评分：

![](/img/17_05_28/001.png)

假设下面的表格是几个用户针对五部电影给出的评分。其中"?"代表用户没有给出评分：

|电影|Alice(1)|Bob(2)|Carol(3)|Dave(4)|
|:-:|:-:|:-:|:-:|:-:|
|《爱到最后》|5|5|0|0|
|《浪漫永远》|5|?|?|0|
|《小爱犬》|?|4|0|?|
|《无尽狂飙》|0|0|5|4|
|《剑与空手道》|0|0|5|?|

引入以下几个变量：

$$
n\_u=用户数量 \\\\
n\_m=电影数量 \\\\
r(i,j)=如果用户j对电影i投过票，则记为1 \\\\
y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）
$$


推荐系统问题就是：

在给定上面的这些数据（即$r(i,j)$和$y^{(i,j)}$）时，然后视图去预测上面表格中那些"?"的值。这样我们就可以向用户推荐他们可能还没看过的新电影了。

## 基于内容的推荐

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/uG59z/content-based-recommendations)

> 上一节，我们引出了**推荐系统**的问题，这一节，我将介绍一种**基于内容的推荐**方法。

这是上一节中的数据：

|电影|Alice(1)|Bob(2)|Carol(3)|Dave(4)|$x\_1$(爱情片)|$x\_2$(动作片)|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|《爱到最后》|5|5|0|0|0.9|0|
|《浪漫永远》|5|?|?|0|1.0|0.01|
|《小爱犬》|?|4|0|?|0.99|0|
|《无尽狂飙》|0|0|5|4|0.1|1.0|
|《剑与空手道》|0|0|5|?|0|0.9|

重申一下之前的定义：

$$
n\_u=用户数量 \\\\
n\_m=电影数量 \\\\
r(i,j)=如果用户j对电影i投过票，则记为1 \\\\
y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）
$$

在这里的数据中：

$$
n\_u=4 \\\\
n\_m=5
$$

并且我们又追加了两列特征$x\_1$和$x\_2$，分别表示当前电影属于**爱情片**和**动作片**的程度。我们用$n=2$来代表特征数（这里不包括$x\_0$）。

有了每部电影的类型特征数据，我们就可以用一个特征矩阵表示某一部电影了。假设上面五部电影，从上到下我们依次使用数字$1、2、3、4、5$来代替。那么对于第一部电影《爱到最后》的特征向量表示为：

$$
\begin{equation}
x^{(1)}=\left[
\begin{matrix}
1\\\\
0.9\\\\
0
\end{matrix}
\right]
\end{equation}
$$

> 这三个数分别代表三个特征的值:$x\_0$、$x\_1$和$x\_2$。其中$x\_0$是截距特征变量，值为1。

### 使用一个已经训练好的模型进行预测

为了进行预测，我们可以把对每个观众打分的预测当成一个独立的线性回归问题。

具体来说，比如每一个用户$j$，都学习出一个参数$\theta^{(j)}$（一般情况下，$\theta^{(j)}$的维度都是$n+1$，$n$是特征数，不包括截距项$x\_0$）。

然后我们要根据参数向量$\theta^{(j)}$与特征$x^{(i)}$的内积来预测用户对电影$i$的评分：

$$
(\theta^{(j)})^Tx^{(i)}
$$

#### 举例

我们以用户1 Alice为例，Alice对应的特征向量应该是$\theta^{(1)}$，我们要预测Alice对于电影《小爱犬》的评分，这部电影的特征向量为：

$$
\begin{equation}
x^{(3)}=\left[
\begin{matrix}
1\\\\
0.99\\\\
0
\end{matrix}
\right]
\end{equation}
$$

假如你已经得到了Alice的特征参数向量$\theta^{(1)}$的值（后面我们会具体讲如何得到这个值）：

$$
\begin{equation}
\theta^{(1)}=\left[
\begin{matrix}
0\\\\
5\\\\
0
\end{matrix}
\right]
\end{equation}
$$

那么对于Alice关于《小爱犬》的平分预测如下：

$$
(\theta^{(1)})^Tx^{(3)}=5×0.99=4.95
$$

这里我们实际在做的事情就是：**对每个用户应用不同的线性回归模型**。

### 更正式的描述

关于这个计算过程，更正式的描述如下：

已知：

$$
r(i,j)=如果用户j对电影i投过票，则记为1 \\\\
y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）\\\\
\theta^{(j)} = 用户j的参数向量 \\\\
x^{(i)} = 电影i的特征向量
$$

对于用户$j$关于电影$i$的预测评分为：

$$
(\theta^{(j)})^Tx^{(i)}
$$

### 训练预测模型：计算参数向量$\theta$

通过以下运算，可以学习到参数$\theta^{(j)}$：

![](/img/17_05_28/002.gif)

> **公式解读：**
> 
> 其中$\sum\_{i:r(i,j)=1}$代表对所有满足$r(i,j)=1$的元素进行求和。
> 
> $(\theta^{(j)})^T(x^{(i)})-y^{(i,j)}$代表预测用户$j$对电影$i$的平分减去用户对这部电影的实际评分。
> 
> $\frac{\lambda}{2m^{(j)}}\sum\_{k=1}^n((\theta\_k^{(j)}))^2$是正则化项。

对上面的式子通过求最小化，我们最终可以得到一个表现良好的$\theta^{(j)}$。

为了让公式变得更简单一些，我们可以去除掉常数项$\frac{1}{m^{(j)}}$，对最终结果无影响：

![](/img/17_05_28/003.gif)

#### 完整的描述

接下来对训练过程进行完整的描述一遍：

##### 优化目标

为了学习第$j$个用户的参数$\theta^{(j)}$，我们执行以下运算：

![](/img/17_05_28/003.gif)

为了学习全部用户的参数$\theta^{(1)},\theta^{(2)},...,\theta^{(n\_u)}$，我们执行以下运算：

![](/img/17_05_28/004.gif)

通过这一运算，我们就能得出所有用户的参数向量$\theta^{(1)},\theta^{(2)},...,\theta^{(n\_u)}$，从而对所有用户作出预测了。

##### 优化算法

![](/img/17_05_28/004.gif)

是我们最优化目标，记为$J(\theta^{(1)},\theta^{(2)},...,\theta^{(n\_u)})$。

为了最小化这个目标函数$J(\theta^{(1)},\theta^{(2)},...,\theta^{(n\_u)})$，我们需要使用梯度下降算法来计算：

- **k=0时**

![](/img/17_05_28/005.gif)

- **k≠0时**

![](/img/17_05_28/006.gif)

> **注意：**其中$\alpha$是学习率。

上面的过程实际上是在对优化目标函数$J(\theta^{(1)},\theta^{(2)},...,\theta^{(n\_u)})$求偏微分的过程：

$$
\frac{\partial}{\partial\theta\_k^{j}}
J(\theta^{(1)},\theta^{(2)},...,\theta^{(n\_u)})
$$

以上就是通过梯度下降来最小化代价函数$J$的全过程，当然，如果你愿意的话，你可以尝试使用更高级的优化算比如**聚类下降**或者*L-BFGS(Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm)**或者别的方法来最小化代价函数J。

> 以上内容，我们介绍了使用一种线性回归的变体来预测不同用户对不同电影评分的算法，这种算法被称作**“基于内容的推荐”**，因为我们已知了不同电影的特征（比如电影是爱情片的程度，是动作片的程度等），从而来进行预测。但事实上，对于很多电影，我们并没有这些特征，或者很难得到这些特征。所以，在下一节中，我们将介绍一种**“不基于内容的推荐系统”**。

# 协同过滤

## 协同过滤

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/2WoBV/collaborative-filtering)

> 这一节，我们将介绍一种叫做**协同过滤(collaborative filtering)**的推荐算法。
> 
> 关于这个算法值得一提的一个特点，那就是它能实现**对特征的学习**。就是说这个算法可以实现自动学习要使用的特征值。

在之前的电影评分的例子中，我们有下面的这组数据：

|电影|Alice(1)|Bob(2)|Carol(3)|Dave(4)|$x\_1$(爱情片)|$x\_2$(动作片)|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|《爱到最后》|5|5|0|0|0.9|0|
|《浪漫永远》|5|?|?|0|1.0|0.01|
|《小爱犬》|?|4|0|?|0.99|0|
|《无尽狂飙》|0|0|5|4|0.1|1.0|
|《剑与空手道》|0|0|5|?|0|0.9|

我们除了有每个用户对不同电影的评分之外，同时也有每个电影属于不同类别特征的程度值$x\_1$和$x\_2$。

但是，在实际情况中，这样做的难度很大，因为我们很难对所有的电影给出相关特征的评分。而且通常情况，我们还希望能得到除了这两个特征之外的其他特征。

为了能做到这一点，我们换一种形式，假设我们并不知道每部电影具体的特征值是多少：

|电影|Alice(1)|Bob(2)|Carol(3)|Dave(4)|$x\_1$(爱情片)|$x\_2$(动作片)|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|《爱到最后》|5|5|0|0|?|?|
|《浪漫永远》|5|?|?|0|?|?|
|《小爱犬》|?|4|0|?|?|?|
|《无尽狂飙》|0|0|5|4|?|?|
|《剑与空手道》|0|0|5|?|?|?|

假设我们采访了每一位用户，而且每一位用户都告诉我们他们是否喜欢爱情电影；以及他们是否喜欢动作电影，这样Alice、Bob、Carol以及Dave就有了他们对应的参数：

|Alice(1)|Bob(2)|Carol(3)|Dave(4)|
|:-:|:-:|:-:|:-:|
|$\begin{equation}\theta^{(1)}=\left[\begin{matrix}0\\\\5\\\\0\end{matrix}\right]\end{equation}$|$\begin{equation}\theta^{(2)}=\left[\begin{matrix}0\\\\5\\\\0\end{matrix}\right]\end{equation}$|$\begin{equation}\theta^{(3)}=\left[\begin{matrix}0\\\\0\\\\5\end{matrix}\right]\end{equation}$|$\begin{equation}\theta^{(4)}=\left[\begin{matrix}0\\\\0\\\\5\end{matrix}\right]\end{equation}$|

每个特征向量的第二个元素表示对爱情片的喜欢程度，第三个元素表示对动作片的喜欢程度。可以看出来Alice和Bob更喜欢爱情片，而Carol和Dave更喜欢动作片。

有了这些数据，我们就可以着眼于用户，看看任意用户$j$对应的不同题材电影的喜欢程度$\theta^{(j)}$。有了这些参数值，理论上我们就能推测出每部电影的特征变量$x\_1$和$x\_2$的值。

举个例子：

我们看第一个电影，假设我们不知道这部电影的主要内容，我们只知道Alice和Bob喜欢这部电影，而Carol和Dave不喜欢它。从他们四个人的特征向量就可以看出，Alice和Bob给出了5分，而Carol和Dave给出了0分。因此我们可以推断，这部电影可能是一部爱情片，而不太可也能是动作片。所以可能$x\_1=1.0$而$x\_2=0.0$。

其实这说明了我们在寻找能使得以下式子成立的$x^{(1)}$：

$$
(\theta^{(1)})^Tx^{(1)}≈5 \\\\
(\theta^{(2)})^Tx^{(1)}≈5 \\\\
(\theta^{(3)})^Tx^{(1)}≈0 \\\\
(\theta^{(4)})^Tx^{(1)}≈0
$$

因此，我们可以得出$x^{(1)}$的值为：

$$
\begin{equation}
x^{(1)}
=\left[
\begin{matrix}
1\\\\
1.0\\\\
0.0
\end{matrix}
\right]
\end{equation}
$$

> 第一个元素$x^{(1)}\_0$是截距。

我们可以按照这种方式把其他电影的特征预测出来。

### 协同过滤算法的正式描述

**优化算法：**

在已知给定参数$\theta^{(1)},...,\theta^{(n\_u)}$的情况下，对下面函数最小化，得到特征值$x^{(i)}$：

![](/img/17_05_28/007.gif)

这就是我们如何从一部特定的电影中学习到特征的方法。

那么对于所有的电影，计算所有的特征，我们通过最小化下面的函数可以得到：

![](/img/17_05_28/008.gif)

这里我们对所有$n\_m$个电影的特征代价函数求和，然后最小化这个整体的代价函数，这样就能得到针对所有电影的合理的特征值了。

### 鸡生蛋？蛋生鸡？

在上一节中，我们提到了如果给定了一系列的特征值$x^{(1)},...,x^{(n\_m)}$，以及用户对电影的评分，我们就可以得到不同用户对应的参数向量$\theta^{(1)},...,\theta^{(n\_u)}$。

在本节内容中，我们又讲到了在给定参数$\theta^{(1)},...,\theta^{(n\_u)}$的情况下，我们可以预测每个电影的特征向量$x^{(1)},...,x^{(n\_m)}$。

这有点类似**先有鸡还是先有蛋**的问题。我们知道了$x$就能预测出$\theta$，反之，如果我们知道了$\theta$，我们就能预测出$x$。

这样一来，我们能做到的就是首先随机初始化参数向量$\theta$，然后根据这些初始化得到的$\theta$来得到不同电影的特征值$x$:

$$
Guess \ \ \ \ \theta → x
$$

有了这些特征值$x$，我们就可以得到对参数$\theta$更好的估计：

$$
x → \theta
$$

同样，我们可以根据这个更好的$\theta$来得到更好的特征集$x$：

$$
\theta → x
$$

如此循环往复。

如果你一直重复上述的计算过程，你的算法将会收敛到一组合理的特征值$x$，以及一组合理的对不同用户参数的估计值$\theta$。

这就是基本的**协同过滤算法**，但这实际上不是我们最终使用的算法。在下一节中，我们将改进这个算法，让其在计算时更为高效。但这节课希望能让你基本了解这个算法的基本原理。

> 总结一下，在本节，我们了解了最基本的协同过滤算法。
> 
> 协同过滤算法指的是当你针对一大批用户数据执行这个算法时，这些用户实际上在高效地进行了协同合作来得到每个人对电影的评分值。只要用户对某几部电影进行评分，每个用户就都在帮助算法更好的学习出特征。这样以来，通过自己对几部电影评分之后，我就能帮助系统更好的学习到特征。这些特征可以被系统运用，为其他人做出更准确的电影预测。
> 
> 协同的另一层意思是说每位用户都在为了大家的利益，而学习出更好的特征。这就是协同过滤。

## 协同过滤算法

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/f26nH/collaborative-filtering-algorithm)

> 在前几节中，我们谈到几个概念。
> 
> 首先，如果给你几个特征表示电影，我们可以使用这些资料去获得用户的参数数据。
> 
> 第二，如果给你用户的参数数据，你可以使用这些资料去获得电影的特征。
> 
> 本节中，我们将会使用这些概念，并且将它们合并成**协同过滤算法 (Collaborative Filtering Algorithm)**。

在之前的内容中，我们介绍了通过最小化以下目标函数，来在已知电影特征向量$x^{(1)},...,x^{(n_m)}$的情况下，预测每个用户对应的参数向量$\theta^{(1)},...,\theta^{(n_u)}$：

![](/img/17_05_28/004.gif)

也介绍了通过最小化以下目标函数，来在已知每个用户参数向量$\theta^{(1)},...,\theta^{(n_u)}$的情况下，来预测每个电影的特征向量$x^{(1)},...,x^{(n_m)}$：

![](/img/17_05_28/008.gif)

我们首先通过随机初始化一组参数$\theta$，然后来推导特征向量$x$；有了特征向量$x$，我们再去反推更好的参数向量$\theta$，然后如此循环往复，直到收敛。

而实际上，我们有一个更有效率的算法，让我们不必再这样不停地来回计算，而是能同时把$x$和$\theta$同时计算出来。

我们做的就是把上面两个式子合而为一：

同时最小化$x^{(1)},...,x^{(n_m)}$和$\theta^{(1)},...,\theta^{(n_u)}$：

![](/img/17_05_28/009.gif)

优化目标函数：

![](/img/17_05_28/010.gif)

### 公式解读

接下来，我们对上面这个式子的推导过程进行详细的解读。

#### 求和部分

首先需要说明的是，在之前的这两个式子中：

![](/img/17_05_28/004.gif)

![](/img/17_05_28/008.gif)

仔细观察这两部分，其实是一样的：

![](/img/17_05_28/011.gif)

![](/img/17_05_28/012.gif)

第一个是：所有的用户的评过分的电影中，预测评分和真实评分的差值平方总和。

第二个是：所有的电影中对它评过分的用户，预测的评分和真实评分的差值平方总和。

这两者的计算结果其实是一样的，只不过条件先后不同导致计算顺序不同而已。

因此，在最终的代价函数中：

![](/img/17_05_28/009.gif)

这一部分：

![](/img/17_05_28/013.gif)

其实这个表达式就是上面那两种表达式的整体的表示形式。计算结果与上面那两个式子也是相同的。

#### 正则化部分

最终的代价函数

![](/img/17_05_28/009.gif)

的后半段：

![](/img/17_05_28/014.gif)

其实就是把这两个式子中的正则化部分求和得出的：

![](/img/17_05_28/004.gif)

![](/img/17_05_28/008.gif)

如果你把

![](/img/17_05_28/009.gif)

中$x^{(1)},...,x^{(n_m)}$部分看做是常数，然后关于$\theta$做优化，那么这个式子其实就相当于：

![](/img/17_05_28/004.gif)


反之，如果你把$\theta^{(1)},...,\theta^{(n_u)}$部分看做常数，然后关于$x$做优化，那么这个式子其实就相当于是：

![](/img/17_05_28/008.gif)

#### 总结

最终，我们通过合并之前的两个式子，得到了同时关于$x$和$\theta$的代价函数表达式：

![](/img/17_05_28/009.gif)
![](/img/17_05_28/010.gif)

这和之前的算法之间唯一的不同就是**不需要反复计算**（就是前面提到的先关于$\theta$最小化，然后再关于$x$最小化，然后再次关于$\theta$最小化，反复直到收敛）。在新版本里头 不需要不断地在$x$和$θ$这两个参数之间不停折腾，我们所要做的是将这两组参数同时化简。

#### 注意点

最后值得提醒的一件事，当我们以这样的方法学习特征量时，我们必须要保证去掉$x_0$项，这样来保证我们学习到的特征量$x$是$n$维的，而不是$n+1$维的。

同样地，因为参数$θ$与特征向量$x$是在同一个维度上，所以$θ$也是$n$维的。因为如果没有$x_0$，那么$θ_0$也不再需要。

### 协同过滤算法的正式描述

把上面所有的过程总结下来，就是所谓的**协同过滤算法**。下面是对这一算法的具体步骤描述：

- 1.首先用较小的初始值来随机初始化参数$x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}$。
> 这一步有点像训练神经网络，在神经网络中的训练中，我们也是用小的随机数来初始化权值。
- 2.通过使用梯度下降算法（或者其他更高级的优化算法）来最小化代价函数$J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})$。
	- 例如循环每一个$j=1,...,n_u,i=1,...,n_m$ 来执行对应的梯度下降：

![](/img/17_05_28/015.png)

其中这一部分：

![](/img/17_05_28/016.png)

就是对代价函数的偏微分项：

$$
\frac{\partial}{\partial x_k^{(i)}}
J(x^{(1)},...,x^{(n_m)})
$$

这一部分：

![](/img/17_05_28/017.png)

对应也是下面这个代价函数的偏微分项：

$$
\frac{\partial}{\partial \theta_k^{(j)}}
J(\theta^{(1)},...,\theta^{(n_u)})
$$

> 值得提醒的是，在这个公式中，我们不再用到$x\_0=1$这一项，因此$x$是$n$维的（$x\in R^n$）而不是$n+1$维；$\theta$也是$n$维（$\theta\in R^n$）。
> 
> 所以我们在做正则化的时候，也是对这$n$维的参数进行正则化，并不包括$x_0$和$\theta_0$。

- 3.最终，通过用户对应的参数向量$\theta$和训练得出的特征向量$x$，来预测用户给出的评分$\theta^Tx$。

# 低秩矩阵分解

## 矢量化：低秩矩阵分解

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/CEXN0/vectorization-low-rank-matrix-factorization)

> 在上几节中，我们谈到了**协同过滤算法**，本节视频中我将会讲到有关该算法的向量化实现，以及说说有关该算法你可以做的其他事情。
> 
> 比如说，你可以做到**相似产品推荐**的功能。

还是以之前的电影评分为例：

|电影|Alice(1)|Bob(2)|Carol(3)|Dave(4)|
|:-:|:-:|:-:|:-:|:-:|
|《爱到最后》|5|5|0|0|
|《浪漫永远》|5|?|?|0|
|《小爱犬》|?|4|0|?|
|《无尽狂飙》|0|0|5|4|
|《剑与空手道》|0|0|5|?|

我们将这些数据存储到矩阵$Y$中：

$$
\begin{equation}
Y=\left[
\begin{matrix}
5&5&0&0\\\\
5&?&?&0\\\\
?&4&0&?\\\\
0&0&5&4\\\\
0&0&5&0
\end{matrix}
\right]
\end{equation}
$$

$Y^{(i,j)}$代表第i行第j列的数据，即第i部电影的第j个用户的评分。

同时，我们也可以得出下面这个预测评分矩阵：

$$
\begin{equation}
\left[
\begin{matrix}
(\theta^{(1)})^T(x^{(1)})&(\theta^{(2)})^T(x^{(1)})&...&(\theta^{(n_u)})^T(x^{(1)})\\\\
(\theta^{(1)})^T(x^{(2)})&(\theta^{(2)})^T(x^{(2)})&...&(\theta^{(n_u)})^T(x^{(2)})\\\\
┋&┋&┋&┋\\\\
(\theta^{(1)})^T(x^{(n_m)})&(\theta^{(2)})^T(x^{(n_m)})&...&(\theta^{(n_u)})^T(x^{(n_m)})
\end{matrix}
\right]
\end{equation}
$$

其中$(\theta^{(j)})^T(x^{(i)})$代表第i个电影中第j个用户，预计给出的评分。

这个预测矩阵可以看做是**电影的特征矩阵**和**用户的参数矩阵的转置**的乘积。

电影特征矩阵如下：

$$
\begin{equation}
X=\left[
\begin{matrix}
(x^{(1)})^T\\\\
(x^{(2)})^T\\\\
┋\\\\
(x^{(n_m)})^T
\end{matrix}
\right]
\end{equation}
$$

其中每一个元素都是一部电影的特征向量。

用户的参数矩阵如下：

$$
\begin{equation}
\Theta=\left[
\begin{matrix}
(\theta^{(1)})^T\\\\
(\theta^{(2)})^T\\\\
┋\\\\
(\theta^{(n_u)})^T
\end{matrix}
\right]
\end{equation}
$$

其中每个元素都是一个用户的参数向量。

有了所有电影的特征矩阵$X$和所有用户的参数矩阵$\Theta$后，我们就可以得到上面的预测矩阵了：

$$
\begin{equation}
X\Theta^T=\left[
\begin{matrix}
(\theta^{(1)})^T(x^{(1)})&(\theta^{(2)})^T(x^{(1)})&...&(\theta^{(n_u)})^T(x^{(1)})\\\\
(\theta^{(1)})^T(x^{(2)})&(\theta^{(2)})^T(x^{(2)})&...&(\theta^{(n_u)})^T(x^{(2)})\\\\
┋&┋&┋&┋\\\\
(\theta^{(1)})^T(x^{(n_m)})&(\theta^{(2)})^T(x^{(n_m)})&...&(\theta^{(n_u)})^T(x^{(n_m)})
\end{matrix}
\right]
\end{equation}
$$

> 我们的协同过滤算法还有另外一个名字：**低秩矩阵分解（Low Rank Matrix Factorization）**。

### 其他功能

最后，我们还可以通过使用协同过滤算法产生的结果做一些额外的事情。比如说**找到相关的电影**。

如何做到针对用户喜欢的某一部电影，然后推荐给他另一部类似的电影呢？

对于每一部电影，我们都可以学习到他的特征向量$x^{(i)} \in R^n$。

假设我们想找到和电影$i$最类似的一部电影，有一种方式就是求出所有电影的特征向量，然后和电影$i$的特征向量求欧氏距离，距离最小的那个就是最类似的：

$$
Small ||x^{(i)}-x^{(j)}||  → 电影i和电影j最类似
$$

类似的，如果你想找到与电影$i$最类似的5部电影，你只需求与特征向量$x^{(i)}$欧式距离最小的五部电影即可。

## 实现细节：均值归一化

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/Adk8G/implementational-detail-mean-normalization)

> 到目前为止，你已经了解到了推荐系统算法或者 协同过滤算法的所有要点。
> 
> 在本节中，我想分享最后一点实现过程中的细节，这一点就是**均值归一化**。有时它可以让算法运行得更好。

### 动机

为了了解均值归一化这个想法的动机，我们考虑这样一个例子。

有一个用户Eve没有给任何电影评分：

|电影|Alice(1)|Bob(2)|Carol(3)|Dave(4)|Eve(5)|
|:-:|:-:|:-:|:-:|:-:|:-:|
|《爱到最后》|5|5|0|0|**?**|
|《浪漫永远》|5|?|?|0|**?**|
|《小爱犬》|?|4|0|?|**?**|
|《无尽狂飙》|0|0|5|4|**?**|
|《剑与空手道》|0|0|5|?|**?**|

$$
\begin{equation}
Y=\left[
\begin{matrix}
5&5&0&0&?\\\\
5&?&?&0&?\\\\
?&4&0&?&?\\\\
0&0&5&4&?\\\\
0&0&5&0&?
\end{matrix}
\right]
\end{equation}
$$

我们来看看协同过滤算法会对这个用户做什么。

假设我们的电影只有两个特征$n=2$，用户的参数向量也是2维的：$\theta^{(5)} \in R^2$。

![](/img/17_05_28/009.gif)

由于用户没有给任何电影评分，因此代价函数的前半部分没有任何电影满足$r(i,j)=1$的条件，所以可以忽略。所以，真正影响代价函数值变化的只有这一项：

$$
\frac{\lambda}{2}
\sum\_{j=1}^{n\_u}
\sum\_{k=1}^{n}
(\theta\_k^{(j)})^2
$$

对应到我们的具体的两个特征上之后，就是如下的形式：

$$
\frac{\lambda}{2}
[
(\theta_1^{(5)})^2+(\theta_2^{(5)})^2
]
$$

因此，为了最小化这一项，我们得到的结果就是：

$$
\begin{equation}
\theta^{(5)}=\left[
\begin{matrix}
0\\\\
0
\end{matrix}
\right]
\end{equation}
$$

所以，我们可以得出用户对电影评分的预测结果：

$$
(\theta^{(5)})^Tx^{(1)}=0
$$

都是0颗星。

这个结果看起来并没有什么用，因为其实有的电影是对于某些人来说是有较高的评分的，所以某些电影是值得被推荐的。但是我们得到的预测结果都是0颗星，这个结果在暗示我们不要推荐电影给他，这其实也不够好。

所以，我们需要引入**均值归一化**来解决我们这个问题。

### 均值归一化处理数据

对于已知的电影评分数据：

$$
\begin{equation}
Y=\left[
\begin{matrix}
5&5&0&0&?\\\\
5&?&?&0&?\\\\
?&4&0&?&?\\\\
0&0&5&4&?\\\\
0&0&5&0&?
\end{matrix}
\right]
\end{equation}
$$

我们要做的就是计算每个电影所得评分的均值，用向量$μ$表示：

$$
\begin{equation}
μ=\left[
\begin{matrix}
2.5\\\\
2.5\\\\
2\\\\
2.25\\\\
1.25
\end{matrix}
\right]
\end{equation}
$$

然后将所有评分减去平均评分，得到新的$Y$：

$$
\begin{equation}
Y=\left[
\begin{matrix}
2.5&2.5&-2.5&-2.5&?\\\\
2.5&?&?&-2.5&?\\\\
?&2&-2&?&?\\\\
-2.25&-2.25&2.75&1.75&?\\\\
-1.25&-1.25&3.75&-1.25&?
\end{matrix}
\right]
\end{equation}
$$

这样做的目的，**就是把每部电影的评分的平均数都调整为0**。

然后我们对经过归一化处理之后的矩阵使用协同过滤算法，即使用户没有对电影做出过评分，那么我们得到的参数向量是：

$$
\begin{equation}
\theta^{(5)}=\left[
\begin{matrix}
0\\\\
0
\end{matrix}
\right]
\end{equation}
$$

我们最终预测用户给出的分值是：

$$
(\theta^{(5)})^Tx^{(1)} + μ\_i
$$

在这个例子中，我们预测Eva对五部电影初始化的评分分别为：2.5，2.5，2，2.25，1.25。

因为：

$$
\begin{equation}
μ=\left[
\begin{matrix}
2.5\\\\
2.5\\\\
2\\\\
2.25\\\\
1.25
\end{matrix}
\right]
\end{equation}
$$