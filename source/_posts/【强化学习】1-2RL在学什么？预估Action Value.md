title: 【强化学习】1-2 RL在学什么？预估Action Value
tags:
  - 机器学习
  - 强化学习
categories:
  - 强化学习
comments: true
date: 2021-02-11 11:42:00
mathjax: true
---

我们来回顾一下Action-Value的定义：某个动作a的价值$q*$定义为采取该动作后收到的预期奖励：

$$
q*(a) \doteq \mathbb{E}[R_t|A_t=a] 
$$

由于$q*(a)$是未知的，所以我们需要预估它。

## 样本平均法

样本平均法通过计算动作奖励（reward）的均值来预估该动作的value：

$$
\begin{equation}
\begin{split}
 Q_t(a) &\doteq \frac{所有动作a的总奖励}{动作a出现的次数}  \\
         &= \frac{\sum_{i=1}^{t-1}R_i}{t-1} 
\end{split}
\end{equation}
$$

> 说明：我们使用$t-1$是因为我们计算value的时间t是基于t时刻之前产生的动作来预估的。
> 另外如果我们尚未执行动作A，那么我们需要对该动作的value设置为一个默认值（比如0）

回到我们的医学实验的例子中，医生需要决定使用3种药物中的哪一种，如果经过治疗后，病人变得更好，医生会记录奖励值为1，否则记录奖励值为0。

<img src="/img/rl_21_02_11/06.png" height=400 />

随着试验的进行，我们估计的q值会逐渐接近真正的action value。

## 贪婪策略

有了对q值合理的估计后，医生就不会随机的给患者分配治疗方案了，相反，医生会开始选择目前为止认为最好的治疗方案给到患者。我们称这种选择action的方式为**贪婪策略**。

贪婪策略选取动作的方案是选择当前具有最大估计值的动作。选择贪婪策略也就意味着agent正在利用其当前所学到的知识（agent正在努力获得最大的奖励）。

我们可以通过argmax来计算贪婪策略的行为。

<img src="/img/rl_21_02_11/07.png" height=200 />

## 探索开发困境

agent也可以选择非贪婪动作来做探索，此时，agent会牺牲掉暂时的奖励来换取更多的关于其他动作的信息。

agent不能同时选择探索（explore）和利用（exploit），这是强化学习的基本问题之一。这就是探索开发困境。

## 增量式预估Action Value

