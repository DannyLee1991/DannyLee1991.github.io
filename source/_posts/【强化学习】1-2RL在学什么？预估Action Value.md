title: 【强化学习】1-2 RL在学什么？预估Action Value
tags:
  - 机器学习
  - 强化学习
categories:
  - 强化学习
comments: true
date: 2021-02-11 11:42:00
mathjax: true
---

我们来回顾一下Action-Value的定义：某个动作a的价值$q*$定义为采取该动作后收到的预期奖励：

$$
q*(a) \doteq \mathbb{E}[R_t|A_t=a] 
$$

由于$q*(a)$是未知的，所以我们需要预估它。

## 样本平均法

样本平均法通过计算动作奖励（reward）的均值来预估该动作的value：

$$
\begin{equation}
\begin{split}
 Q_t(a) &\doteq \frac{所有动作a的总奖励}{动作a出现的次数}  \\
         &= \frac{\sum_{i=1}^{t-1}R_i}{t-1} 
\end{split}
\end{equation}
$$

> 说明：我们使用$t-1$是因为我们计算value的时间t是基于t时刻之前产生的动作来预估的。
> 另外如果我们尚未执行动作A，那么我们需要对该动作的value设置为一个默认值（比如0）

回到我们的医学实验的例子中，医生需要决定使用3种药物中的哪一种，如果经过治疗后，病人变得更好，医生会记录奖励值为1，否则记录奖励值为0。

<img src="/img/rl_21_02_11/06.png" height=400 />

随着试验的进行，我们估计的q值会逐渐接近真正的action value。

## 贪婪策略

有了对q值合理的估计后，医生就不会随机的给患者分配治疗方案了，相反，医生会开始选择目前为止认为最好的治疗方案给到患者。我们称这种选择action的方式为**贪婪策略**。

贪婪策略选取动作的方案是选择当前具有最大估计值的动作。选择贪婪策略也就意味着agent正在利用其当前所学到的知识（agent正在努力获得最大的奖励）。

我们可以通过argmax来计算贪婪策略的行为。

<img src="/img/rl_21_02_11/07.png" height=200 />

## 探索开发困境

agent也可以选择非贪婪动作来做探索，此时，agent会牺牲掉暂时的奖励来换取更多的关于其他动作的信息。

agent不能同时选择探索（explore）和利用（exploit），这是强化学习的基本问题之一。这就是探索开发困境。

## 增量式预估Action Value

假设我们现在维护了一个每日都有百万访问量的网站。在这个网站下投放的广告可以想象成是一个k臂老虎机问题。我们希望找出产生收益最多的广告。如果我们把所有历史数据存储下来，采用样本平均法来预估广告的value固然可行，但在如此庞大的访问量下，数据的存储和计算可能是一个比较明显的问题。如何在不存储数据的情况下保持最新的action value的估算值呢？如何以增量的方式来预估action value是我们需要考虑的问题。

在样本平均法中，我们的q值更新规则可以以递归的方式来编写（这样一来，我们就可以避免存储历史数据）：

$$
\begin{equation}
\begin{split}
Q_{n+1} 
&= \frac{1}{n}\sum_{i=1}^n R_i \\
&= \frac{1}{n}(R_n + \sum_{i=1}^{n-1}R_i) \\
&= \frac{1}{n}(R_n + (n - 1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_i)
\end{split}
\end{equation}
$$

由于

$$
Q_n=\frac{1}{n-1}\sum_{i=1}^{n-1}R_i
$$

所以上面的式子可以改写为：

$$
\begin{equation}
\begin{split}
Q_{n+1} 
&= \frac{1}{n}(R_n + (n - 1)Q_n) \\
&= \frac{1}{n}(R_n + nQ_n - Q_n) \\
&= Q_n + \frac{1}{n}(R_n - Q_n)
\end{split}
\end{equation}
$$

综上，我们可以得到:

$$
Q_{n+1} = Q_n + \frac{1}{n}(R_n - Q_n)
$$

这个表达式是一种形式，后面还将出现很多次。这种形式的抽象表述如下：

$$
NewEstimate \leftarrow OldEstimate + StepSize(Target - OldEstimate)
$$

我们有必要对这其中的一些术语进行说明。

- 估计值误差：指的是$(Target - OldEstimate)$，即旧估计值与新目标之间的差值。
- 目标值（Target）：指的是$R_n$，即新的奖励。
- 步长（StepSize）：指的是$\frac{1}{n}$的部分，即目标值朝着真实值的方向更新一步的步长大小。

我们用$α_n$来表示步长，其范围是$[0,1]$:

$$
Q_{n+1} = Q_n + α_n(R_n - Q_n)
$$

$$
α_n \rightarrow [0,1]
$$

在均值采样的场景下，我们的步长为：

$$
α_n = \frac{1}{n}
$$

我们再回到医疗试验的例子中。如果随着试验的进行，某种药物的效果发生了变化，我们该如何应对？

<img src="/img/rl_21_02_11/08.png" height=300 />

假设我们让B药物现在的治愈率提升到0.9:

<img src="/img/rl_21_02_11/09.png" height=300 />

此时，这个问题就变成了**非固定老虎机问题**了，奖惩的分配随着时间的变化而发生了变化，而这一变化医生并不知道，但是我们希望算法能适应于这种变化。

解决这一问题的一种方式是**使用固定步长大小**。

### 历史奖励衰减

假设$α_n=0.1$是一个恒定的值，那么最近的奖励对估计的影响将大于旧的奖励。

<img src="/img/rl_21_02_11/10.png" height=300 />

此图表显示了最近一次获得的奖励与T时间步长前收到的奖励的比例。加权随着时间的推移呈指数级淡化。

$$
\begin{equation}
\begin{split}
Q_{n+1} 
&= Q_n + α_n(R_n - Q_n) \\
&= Q_n + αR_n - αQ_n \\
&= αR_n + Q_n - αQ_n \\
&= αR_n + (1 - α)Q_n
\end{split}
\end{equation}
$$

由于

$$
Q_n = αR_{n-1} + (1 - α)Q_{n-1}
$$

所以

$$
\begin{equation}
\begin{split}
Q_{n+1} 
&= αR_n + (1 - α)Q_n \\
&= αR_n + (1 - α)[αR_{n-1} + (1 - α)Q_{n-1}] \\
&= αR_n + (1 - α)αR_{n-1} + (1 - α)^2Q_{n-1} \\
\end{split}
\end{equation}
$$

我们可以依次展开递归表达式，得到：

$$
\begin{equation}
\begin{split}
Q_{n+1} 
&= Q_n + α_n(R_n - Q_n) \\
&= αR_n + (1 - α)αR_{n-1} + (1 - α)^2 αR_{n-2} + ... + (1-α)^{n-1}αR_1 + (1-α)^nQ_1 \\
&= (1-α)^nQ_1 + \sum_{i=1}^nα(1-α)^{n-1}R_i
\end{split}
\end{equation}
$$

这里$Q_1$是初始的action value，$Q_{n+1}$最终等于初始的$Q_1$加上随着时间推移而变化的奖励加权总和。

这个等式告诉我们：
- $Q_1$的贡献随着时间的推移呈指数级减少
- 在过去的时间里，reward对总和的贡献是呈指数级变化的。

我们看到随着时间的推移，初始的Q值对结果的影响越来越小，最近的奖励对我们的下一次估计影响最大。