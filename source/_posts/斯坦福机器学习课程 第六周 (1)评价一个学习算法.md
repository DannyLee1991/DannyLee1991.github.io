title: 斯坦福机器学习课程 第六周 (1)评价一个学习算法
tags:
  - 机器学习
  - 斯坦福课程
categories:
  - 机器学习
comments: true
date: 2016-10-24 22:28:58
mathjax: true
---

## 如何少走弯路？

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/OVM4M/deciding-what-to-try-next)

### 这些坑可能会耽误你几个月

> 到目前为止，我们已经介绍了许多不同的学习算法。如果你一直跟着这些视频的进度学习，你会发现自己已经不知不觉地成为一个了解许多先进机器学习技术的专家了。
> 
> 然而，在懂机器学习的人当中，依然存在着很大的差距。一部分人确实掌握了怎样高效有力地运用这些学习算法，而另一些人他们可能对我马上要讲的东西就不那么熟悉了，他们可能没有完全理解怎样运用这些算法，因此总是把时间浪费在毫无意义的尝试上。
> 
> 我想做的是，确保你在设计机器学习的系统时，你能够明白怎样选择一条最合适、最正确的道路。
> 
> 因此，在这节视频和之后的几段视频中，我将向你介绍一些实用的建议和指导，帮助你明白怎样进行选择。
> 
> 具体来讲，我将重点关注的问题是：假如你在开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，你应如何决定接下来应该选择哪条道路？

以预测房价为例：

假如在预测房价的例子中，你已经完成了正则化线性回归，也就是最小化代价函数$J$的值。

![](/img/16_10_24/001.png)

假如在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在的问题是要想改进这个算法接下来该怎么办？

实际上你可以想出很多方法来改进这个算法的性能。

通常情况下我们会使用这几种方法：

- 通过使用更多的训练样本

> 但有的时候，更多的训练样本并不能解决问题。

- 尝试选用更少的特征集

> 你可以从众多的特征集中仔细挑选一小部分来防止**过拟合**。

- 尝试选用更多的特征集

> 也许目前的特征集对你来讲并不是很有帮助，你希望获取更多有用的特征数据

- 也可以尝试增加多项式特征的方法($x\_{1}^{2}$,$x\_{2}^{2}$,$x\_{1}x\_{2}$,etc.)
- 通过增大正则化参数$\lambda$
- 通过减小正则化参数$\lambda$

上面的这些方法，都可以扩展开来，都可能是一个花费6个月甚至更长时间的项目。

遗憾的是，大多数人用来选择这些方法的标准是凭感觉的。而且很多人都会错误的选择其中一种方法花费大量的时间和精力，走上了“不归路”。

幸运的是，有一系列简单的方法，能让你事半功倍，排除掉上面的那个优化清单上的至少一半的方法，留下真正有用的方法。同时也有一种很简单的方法，可以很轻松的排除掉很多选择从而为你节省大量不必要花费的时间。

### 机器学习诊断法引入

在接下来的视频中，我首先介绍**怎样评估机器学习算法的性能**，然后在之后的几段视频中，我们将开始讨论这些方法，它们也被称为**“机器学习诊断法(Machine learning diagnostic)”**。

> Diagnostic:A test that you can run to gain insight what is/isn't working with a learning algorithm, and gain guidance as to how best to improve its performance.
> 
> 诊断法：这是一种测试法，你通过执行这种测试能够深入了解某种算法到底是否有用，并且也可以告诉你如何改进算法的效果。

诊断法是一种很有用的方法，可以更有效率地利用好你的时间，但同时也会花费一些时间来实现。

## 评估假设函数

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/yfbJY/evaluating-a-hypothesis)

当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化。有人认为，得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差并不能说明它就一定是一个好的假设函数，而且我们也学习了过拟合假设函数的例子。所以这推广到新的训练集上是不适用的。

那么，你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数$h(x)$进行画图，然后观察图形趋势：

![](/img/16_10_24/002.png)

但对于特征变量不止一个的这种一般情况：

![](/img/16_10_24/003.png)

想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现的。

因此我们需要另一种方法来评估我们的假设函数。

### 评估假设函数的方法

如下给出了一种评估假设函数的标准方法：

假设我们有这样一组数据组：

![](/img/16_10_24/004.png)

虽然这里只有十组数据，但通常情况下我们都有成百上千组训练样本。

为了确保我们可以评估我们的假设函数，我们要做的是将这些数据进行三七分：

![](/img/16_10_24/005.png)

第一部分（70%）将成为我们的**训练集(Training Set)**

第二部分（30%）将成为我们的**测试集(Test Set)**

> 将所有数据按照7:3的比例划分，是一种常见的划分比例

现在我们有了一部分训练数据集：

$$
(x^{(1)},y^{(1)})\\\\
(x^{(2)},y^{(2)})\\\\
......\\\\
(x^{(m)},y^{(m)})
$$

> 这里的$m$依然表示训练样本的总数

剩下的部分数据将被用作测试数据：

$$
(x^{(1)}\_{test},y^{(1)}\_{test})\\\\
(x^{(2)}\_{test},y^{(2)}\_{test})\\\\
......\\\\
(x^{(m\_{test})}\_{test},y^{(m\_{test})}\_{test})
$$

> 这里$m\_{test}$表示测试样本的总数，$\_{test}$表示这些样本是来自测试集。

>**注意**：如果说我们的数据是有某种规律的话，那么我们按照7:3的比例取数据时，应该是随机选取的。

### 评估步骤详解

#### 线性回归中

在线性回归中，的训练/测试流程：

- 1.需要对训练集进行学习，得到参数$\theta$。

> 具体来讲就是最小化训练误差$J(\theta)$。这里的是使用那70%的数据训练得出的结果。

- 2.计算出测试误差

> 使用$J\_{test}(\theta)$来表示测试误差，我们要做的是取出之前从训练集中学习得到的参数$\theta$带入到$J\_{test}(\theta)$来计算我们的测试误差。可以写成如下形式：

$$
\begin{align\*}
J\_{test}(\theta)= \frac{1}{2m\_{test}}\sum\_{i=1}^{m\_{test}}(
 h\_{\theta}(x\_{test}^{(i)}) - y\_{test}^{(i)}
 )^{2}
\end{align\*}
$$

> 这实际上是测试集平方误差的平均值。

----

#### 分类问题中

当然，这是当我们使用线性回归和平方误差标准时测试误差的定义，那么如果是分类问题，比如说使用逻辑回归的时候呢？

训练和测试逻辑回归与之前所说的非常类似：

- 1.首先我们要从训练数据中（前70%）学习得到参数$\theta$
- 2.然后用下面的式子计算测试数据的误差值:

![](/img/16_10_24/006.png)

在分类问题中的测试误差$J\_{test}(\theta)$其实也被称作**误分类率**（也被称为**0/1错分率**）。表示你预测到的正确或错误样本的情况。

比如说可以这样定义一次预测的误差：

- 当$h\_{\theta}\ge0.5$时$y=0$

- 或者当$h\_{\theta}\lt0.5$时$y=1$

这两种情况下我们的假设都对样本进行另外误判，否则其他情况下假设值都能正确的对样本$y$进行分类。

$$
err(h\_{\theta}^{(x)},y)
=\\{
\begin{align\*}
&= 1 \ \ if\ h\_{\theta}(x)\ge0.5,\ y=0 
		\ or\ if\ h\_{\theta}(x)\le0.5,\ y=1 \\\\
&= 0 \ otherwise
\end{align\*}
$$

然后我们就能应用错分率误差来定义测试误差，也就是：

$$
\begin{align\*}
Test\ error = 
 \frac{1}{m\_{test}}\sum\_{i=1}^{m\_{test}}err(
 h\_{\theta}(x\_{test}^{(i)}), y\_{test}^{(i)}
 )
\end{align\*}
$$

> 以上我们介绍了一套标准技术来评价一个已经学习过的假设，在下一段视频中，我们要应用这些方法来帮助我们进行诸如特征选择一类的问题（比如多项式次数的选择，或者正则化参数的选择）。

## 多项式模型的选择以及训练集/验证集/测试集的划分

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/QGKbr/model-selection-and-train-validation-test-sets)

> 假如你想要确定对于某组数据最合适的多项式次数是几次，怎样选用正确的特征来构造学习算法，这些问题我们称之为**”模型选择问题“**。
> 
> 在我们对于这一问题的讨论中，我们还将提到如何将数据分为三组：也就是**训练集、验证集和测试集**，而不仅仅是前面提到的两组数据。
> 
> 这一节中，我们将会介绍这些内容的含义。

### 选择合适的模型解决过拟合问题

在过拟合的情况中，学习算法在适用于训练集时表现非常完美，但这并不代表此时的假设也很完美。

更一般的说，这也是为什么训练集误差通常不能正确预测出该假设是否能很好地拟合新样本的原因。

具体来讲，如果你把这些参数集，比如$\theta\_{0}$,$\theta\_{1}$,$\theta\_{2}$...调整到尽量拟合你的训练集，那么结果就是你的假设会在训练集上表现地很好，但这并不能确定当你的假设推广到训练集之外的新的样本上时，预测结果是怎样的。

而更为普遍的规律是，只要你的参数非常拟合某个数据组，比如说非常拟合训练集（当然也可以是其他数据集）：

![](/img/16_10_24/007.png)

那么你的假设对于相同数据组的预测误差（比如说训练误差）是不能够用来推广到一般情况的，或者说，是不能作为实际的泛化误差的（也就是说，不能说明你的假设对于新样本的效果）。

---

接下来具体来说说模型选择问题：

假如说你现在要选择能最好地拟合你数据的多项式次数，那么下面的式子你应该选择哪一个呢？

![](/img/16_10_24/008.png)

假设$d$代表我们应该选择的多项式的次数(上面的式子次数依次为从1到10)，我们除了需要确定参数$\theta$之外，还要考虑如何确定这个多项式的次数$d$。

因此，我们需要确定参数$d$最适当的取值。

具体地说，比如你想要选择一个模型，那就从这10个模型中，选择一个最适当的多项式次数，并且用这个模型进行估计，预测你的假设能否很好地推广到新的样本上。

那么你可以这样做：

- 你可以先选择第一个模型，然后求训练误差的最小值。这样你就会得到一个参数向量$\theta$。
- 然后你再选择第二个模型，进行同样的过程，这样你会得到另一个参数向量$\theta$。
- 拟合三次函数模型时，同理，也能得到一个参数向量$\theta$。
- ...
- 用相同的方式得到第10个模型的向量$\theta$。

![](/img/16_10_24/009.png)

如果一次模型训练后得到的参数向量为$\theta^{(1)}$,二次模型训练后得到的参数向量为$\theta^{(2)}$，依次类推，十次模型训练后得到的参数向量为$\theta^{(10)}$。

接下来，我们要做的是对所有这些模型求出**测试集误差**。因此，我们可以算出每一个模型的$Jtest(\theta^{(1)})$、$Jtest(\theta^{(2)})$、$Jtest(\theta^{(3)})$...$Jtest(\theta^{(10)})$。

接下来为了确定哪一个模型最好，我们可以找出**测试集误差最小的模型**。

假设我们最终选择了五次多项式模型:

$$
Choose \ \ \theta\_{0} + ... \theta\_{5}x^{5}
$$

### 正确的评价某个假设函数的预测能力

> 由于我们使用的是**测试结果集**来衡量的代价函数，从而得到多项式次数$d$这个参数的值为5，所以这样任然不能公平地说明这个假设可以推广到一般情况。
> 
> 也就是说我们选择了一个能最好地拟合测试集的参数$d$的值，因此我们的参数向量$\theta^{(5)}$在拟合测试集时的结果很可能导致一个比实际泛化误差更完美的预测结果。（因为我们找到的是一个最能拟合测试集的参数$d$）。因此我们再用测试集来评价我们的假设就显得不公平了。
> 
> 而我们其实更关心的是对新样本的拟合效果，所以我们之前说到的如果我们用训练集来拟合参数$\theta\_{0}$,$\theta\_{1}$等参数时，拟合后的模型在作用于训练集上的效果是不能预测出我们将这个假设推广到新样板上时效果如何的。这是因为这些参数能够很好地拟合训练集，因此它们很有可能在对训练集的预测中表现的很好，但对其他的新样本来说就不一定那么好了。
> 

我们要做的实际上是用测试集来拟合参数$d$，但是这同样也意味着这并不能较为公平地预测出假设函数在遇到新样本时的表现。

#### 方式

为了解决这一问题在模型选择中，如果我们要评价某个假设，我们通常采用以下的方法：

给定某个数据集，这里我们要将其分为三段：训练集、**交叉验证集（cross validation set）**、测试集。

![](/img/16_10_24/010.png)

一种典型的分割比例是：训练集60%，交叉验证集20%，测试集20%。（这个比例可稍作调整）

![](/img/16_10_24/011.png)

> $m$是训练集个数，$m\_{cv}$是交叉验证集个数，$m\_{test}$是测试集个数。

我们可以得到 训练集/交叉验证集/测试集 的误差：

![](/img/16_10_24/012.png)

现在我们的模型选择问题是这样的：我们现在要使用交叉验证集来选择合适的模型：

![](/img/16_10_24/013.png)

即通过使用**训练集**对每一个假设函数依次去求最小化的代价函数$minJ(\theta)$，并求得对应的参数向量$\theta^{(d)}$。

然后我们要做的是在**交叉验证集**中测试这些假设的表现，测出$J\_{cv}$来看看这些假设在交叉验证集中表现如何。

然后我们要选择**交叉验证集**误差最小的那个假设模型，假如这个模型是$J\_{cv}(\theta^{(4)})$对应的那个模型，因此我们就选择这个四次多项式模型：

$$
\theta\_{0} + \theta\_{1}x^{1} + ... + \theta\_{4}x^{4}
$$

我们得到了拟合出最好的系数$d=4$。

这个过程中，我们没有使用**测试集**进行拟合，这样我们就回避了**测试集**的嫌疑。这样我们就可以光明正大的使用**测试集**来估计所选模型的泛化误差了。

最后，我们可以使用**测试集**来评价模型的表现。

但最后还是要提醒大家的一点是，在如今的机器学习应用中，确实也有很多人是像我之前介绍的那样做的（用测试集来选择模型，然后用同样的测试集来评价模型的表现，报告测试误差，看起来好像还能得到比较不错的泛化误差），我说过这并不是一个好的方法，但不幸的是，现在还有很多人这样做。如果测试集足够多的话，这也许还能行得通，但大多数的机器学习开发人员是不会这么做的，因为最佳做法还是把数据分成 训练集、验证集、测试集。
