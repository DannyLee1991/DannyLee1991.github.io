title: 斯坦福机器学习课程 第七周 (3)使用SVM
tags:
  - 机器学习
  - 斯坦福课程
categories:
  - 机器学习
comments: true
date: 2017-04-11 22:47:58
mathjax: true
---

## 使用SVM

> 这段视频中，我们将介绍使用SVM时，我们实际上需要做些什么。

**支持向量机**是一个特定的优化问题，但是我不建议你自己去手动实现这一算法来求解参数$\theta$。就像如今只有很少的人，或者说根本没有人会考虑自己写代码，来实现对矩阵求逆，或求一个数的平方根等。我们只需要调用库函数来实现这些功能即可。

同样，可以用于解决SVM优化问题的软件很复杂，而且已经有专门研究数值优化很多年的学者在做这个，因此你需要使用好的软件库来做这个。我强烈建议使用一个高度优化的软件库，而不是尝试自己去实现它。

这里推荐两个我最常用到的库：liblinear和libsvm。

尽管你不需要自己去实现SVM，但你也需要做以下几件事：

- 选择参数$C$
- 选择**核函数**（相似度函数）

### 核函数的选择

#### 线性核函数（无核函数）

关于**核函数**其中一个选择是**不用任何核函数**（不用任何核函数也叫作**线性核函数**）:

即对于预测结果$y=1$，满足$\theta^Tx\ge0$。

> 例如这种情况下当$\theta\_0 + \theta\_1x\_1 + ... + \theta\_nx\_n \ge 0$时，预测$y=1$。

对**线性核函数**这个术语，你可以把它理解为这个版本的SVM。它只是给你一个标准的线性分类器，因此对某些问题来说，它是一个合理的选择：

具体来说，当你的特征数量$n$很大，但数据量$m$很小时，由于数据量不足，在这种情况下如果使用其他核函数，你可能会**过拟合**，因此，此时**线性核函数**是一个合理的选择。

#### 高斯核函数

$$
f\_i=exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}),
$$

这是我们之前见过的高斯核函数。如果你选择这个核函数，那么你需要选择一个$$\sigma^2$$。

我们之前讨论如何权衡偏差、方差的时候谈论过：

- 如果$\sigma^2$很大，那么你就有可能得到一个**较高偏差较低方差**的分类器。
- 如果$\sigma^2$很小，那么你就有可能得到一个**较低偏差较高方差**的分类器。

那么，什么时候选择高斯核函数呢？

如果你原来的特征变量$x$是$n$维的，而且**$n$很小，样本数量$m$很大时**，高斯核函数会是一个不错的选择。

##### 如何使用高斯核函数

在很多SVM的软件包中，如果你需要使用SVM时，你需要提供一个核函数。

具体地说，如果你决定使用高斯核函数，那么你需要做的就是根据你所用的SVM软件包，来提供一个用于计算核函数的特定特征的方法:

![](/img/17_04_11/001.png)

然后它将自动地生成所有特征变量。

> **注意**：如果你有大小很不一样的特征变量，在使用高斯核函数之前，对它们进行**归一化**是很重要的。

假设你在计算$x$和$l$之间的范数(就是高斯核函数的分子项)：

$||x-l||^2$

这个式子所表达的含义如下：

$$
v=x-l
$$

$$
\begin{align\*}
||v||^2 &= v\_1^2 + v\_2^2 + ... + v\_n^2 \\\\
&= (x\_1-l\_1)^2 + (x\_2-l\_2)^2 + ... + (x\_n-l\_n)^2
\end{align\*}
$$

其中$v$、$x$、$l$都是向量，由于$x$是$n$维的，所以$v$也是$n$维的。

现在如果你的特征变量取值范围很不一样。例如房价预测中，$x\_1$表示1000平方英尺，$x\_2$表示卧室数量为2，那么$(x\_1-l\_1)$可能相较于$(x\_2-l\_2)$大很多。

因此，为了让SVM更好的工作，我们需要对特征变量进行**归一化**处理。这将会保证SVM能够同等地关注到所有不同的特征变量。

#### 选择其他核函数

当你尝试使用SVM时，目前你能用到的核函数就是**线性核函数**和**高斯核函数**，这里有一个警告：

不是所有你可能提出来的相似度函数都是有效的核函数。线性核函数，高斯核函数，以及其他人有时会用到的其他的核函数，他们全部需要满足一个技术条件，这个条件叫做**摩赛尔定理(Mercer`s Theorem)**。

因为支持向量机算法的实现有许多巧妙的数值优化技巧，为了有效地求解参数$\theta$，在最初的设想里，有一个这样的决定，将我们的注意力仅仅限制在可以满足**摩赛尔定理**的核函数上，这个定义做的是：确保所有的SVM包能够使用大量的优化方法，并且快速地得到参数$\theta$。所以，大多数人最后要么使用线性核函数、要么使用高斯核函数。也有一些其他的核函数是满足**摩赛尔定理**的，而我个人是很少很少使用其他核函数的。

所以，我只是简单提及一下你可能会遇到的其他核函数，他们有：

- **多项式核函数（Polynomial kernel）**：
	- 将$x$和$l$之间的相似度，定义为$(x^Tl)^2$：

	$$
	k(x,l)=(x^Tl)^2
	$$
	
	这就是一个$x$和$l$相似度的估量，如果$x$和$l$每一项很接近，那么这个内积就会很大。
	
	这是一个有些不寻常的核函数，它并不常用，但你可能会见到有人使用它的变体形式，比如：
	
	$$
	k(x,l)=(x^Tl)^3
	$$
	
	$$
	k(x,l)=(x^Tl + 1)^3
	$$
	
	$$
	k(x,l)=(x^Tl + 5)^4
	$$
	
	这些都是多项式核函数的变形形式。
	
	多项式核函数实际上有两个参数，一个是加在后面的常数项，如上面最后式子中的5；另一个是多项式的次数，如上面最后式子中的4。
	
	因此，多项式核函数的更一般的形式是：
	
	$$
	k(x,l)=(x^Tl + constant)^{degree}
	$$
	
	这种核函数并不像高斯核函数那样频繁的使用，通常他只用在当$x$和$l$都是严格的非负数时。这样以确保内积值永远不会是负数。
	
	这捕捉到了这样一个直观感觉：如果$x$和$l$之间非常相似，也许它们之间的内积会很大。
	
	它们也有其他的一些性质，但是人们通常用得不多。
	
你也有可能会碰到其他一些更难懂的核函数，比如：
	
- **字符串核函数（String kernel）**:
	- 如果你的输入数据是文本字符串，或者其他类型的字符串，有时会用到这个核函数。
- **卡方核函数（chi-square kernel）**
- **直方图交叉核函数（histogram intersection kernel）**
- ...

你可以用它们来度量不同对象之间的相似性。

例如，你在做一些文本分类问题，在这个问题中，输入变量$x$是一个字符串，我们想要通过字符串核函数来找到两个字符串间的相似度（但是我个人很少用这些更加难懂的核函数，我想我平生可能用过一次卡方核函数，可能用过一次或两次直方图交叉核函数，我甚至没用过字符串核函数）。

---

### 两个细节

我想要在这个视频里讨论最后两个细节。

#### 多类分类

在多类分类中，你有K个类别：

$$
y \in ｛1,2,3,...,K｝
$$

对应下图：

![](/img/17_04_11/002.png)

很明显，这里$K=4$。

那么怎样让SVM输出下面这种各个类别间合适的判定边界呢？

![](/img/17_04_11/003.png)

大部分SVM软件包已经内置了多类分类的函数了，因此，如果你用的是这种软件包，你可以直接使用内置函数。

另一种方式就是使用**一对多(one-vs-all)方法**。这个我们在讲逻辑回归的时候讨论过，所以，你要做的就是要训练$K$个SVM，每一个SVM把一个类同其他类区分开。这种操作会给你$K$个参数向量：

$$
\theta^{(1)},\theta^{(2)},...,\theta^{(K)}
$$

最后，这就与我们在逻辑回归中用到的一对多方法一样，选取使得$(\theta^{(i)})^Tx$最大的类$i$即可。

> 其实大多数软件包都已经内置了多类分类的函数，因此你不必重新造轮子。

#### 逻辑回归 vs SVM

关于**逻辑回归**和**SVM**，我想讨论的是，你什么时候应该用哪个呢？

假设$n$是特征变量的个数，$m$是训练样本数：

- 如果$n$(相对于$m$)大很多时，使用**逻辑回归**，或者使用**无核函数的SVM（线性核函数）**。
	比如你有一个文本分类的问题，特征数量$n=10000$，而且如果你的训练集大小为$m=10$，在这个问题中，你有10000个特征变量，对应10000个词，但是你只有10个训练样本。这种情况下就比较适合使用逻辑回归或者线性核函数的SVM了。
- 如果$n$较小，$m$是中等大小，（例如$n$为1到1000之间的值，$m$为10到10000之间的值）那么使用**高斯核函数的SVM**效果好一些。
- 如果$n$很小，$m$很大时（例如$n=1000$,$m=100000+$），那么高斯核函数的SVM运行起来会很慢，这种情况下，需要**尝试手动地创建更多的特征变量，然后使用逻辑回归或者无核函数的SVM（线性核函数）**。

逻辑回归和不带核函数的SVM它们都是非常相似的算法，他们会做相似的事情，并且表现也相似，但是根据你实现的具体情况，其中一个可能会比另一个更加有效。

但是SVM的威力会随着你用不同的核函数而发挥出来。

#### 什么时候使用神经网络？

最后，神经网络应该在什么时候使用呢？

对于上面所有的情况，一个设计得很好的神经网络也很可能会非常有效，它的一个缺点（或者说不使用神经网络的原因）是：神经网络训练起来可能会很慢。但是如果你有一个非常好的SVM实现包，它会运行得比较快，比神经网络快很多。

> SVM的优化问题，实际上是一个**凸优化**问题。因此好的SVM优化软件包总是会找到全局最小值，或者接近它的值。
> 对于SVM，你不需要担心局部最优。在实际应用中，局部最优对神经网络来说不是非常大，但是也不小。所以使用SVM，你不用考虑这部分问题。

----

### 总结

最后，如果你觉得上面这些使用参考有一些模糊，在面临实际问题时，仍然不能完全确定具体使用哪个算法更好，这个其实很正常。

当我遇到机器学习问题时，有时确实不清楚具体使用哪个算法更好，但是正如你在之前的视频中看到的，算法确实很重要，但是通常更重要的是：**你有多少数据**，**你有多熟练**，**是否擅长做误差分析和调试学习算法**，**想出如何设计新的特征变量**，**想出如何设计新的特征变量**，以及**找出应该输入给学习算法的其它特征变量**等方面。通常这些方面会比你使用**逻辑回归**还是**SVM**更加重要。

但是**SVM**仍然被广泛认为是**最强大的学习算法之一**，最强大的学习算法之一，而且SVM在一个区间内是一个非常有效地学习复杂非线性函数的方法。因此，有了**逻辑回归**、**神经网络**、**SVM**这三个学习算法，我想你已经具备了在广泛的应用里构建最前沿的机器学习系统的能力。