title: 房价预测（2）-数据清洗与持久化
tags:
  - 算法
  - 机器学习
categories:
  - 算法
  - 机器学习
comments: true
date: 2016-12-08 23:05:58
---

> 在[房价预测（1）-搜房网数据爬取](/2016/11/30/房价预测（1）-搜房网数据爬取/)中，已经介绍了爬虫的核心代码，但是爬取到的数据是没有经过加工且带有html标签的，很不利于阅读和使用，并且我们没有把数据持久化到本地，那么这篇文章主要介绍的就是这两步工作。

## Item Pipeline介绍

Scrapy中有个很好用的工具[Item Pipeline](http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html)。

通过阅读文档我们可以看到它的作用是：

- 清理HTML数据
- 验证爬取的数据(检查item包含某些字段)
- 查重(并丢弃)
- 将爬取结果保存到数据库中

真是完全符合我们的要求。

> **Pipeline**的意思是**管道**,如果你玩过Linux的话，对这个名称一定不会陌生。管道是计算机中一个非常常见且重要的概念，大致的作用就是**将管道前一部分的输出结果作为它后一部分的输入**。类似这种设计都可以称为**管道**。

还记得我们之前写的蜘蛛吗？在`ESFListSpider`这个蜘蛛中，开始爬取数据时会走到这个方法中:

```
# 爬取房源列表信息
def parse(self, response):
        self.log('A response from %s just arrived!' % response.url)
        infos = response.xpath("//a/@href").extract()

        for i in infos:
            i_str = str(i).encode("utf-8")
            if "esf" in i_str:
                url = i_str.replace('\\', '').strip()
                yield scrapy.Request(url=url.replace("\"", ""), callback=self.parse_details)
```

在这个方法的内部的循环中会遍历链接地址，再次发起request，之后会自动调用回调函数`self.parse_details`，自动执行`parse_details`:

```
# 爬取房源详情信息
def parse_details(self, response):
	...
	# 顺便爬取一下详情页 url
	item = ESFItem()
    item['url'] = response.url
    
    # 详情页对应的 城市-一级区域-二级区域 信息
	bread_list = response.xpath("//body/div[@class='wrap']/div[@class='bread']/p[@class='floatl']/a").extract()
    for index,bread in enumerate(bread_list[1:]):
        if index == 0:
            item["bread_city"] = bread
        elif index == 1:
            item["bread_area"] = bread
        elif index == 2:
            item["bread_positon"] = bread
	...
	# 这句很重要，保证item可以传递到后面的Pipeline中
	yield item
```

以上是我们蜘蛛的部分核心代码，如果我们配置了Pipeline，蜘蛛会把塞满信息的item传递到我们定义的Pipeline中。

### 创建Pipeline

为了满足**清洗数据**和**存储数据**的功能，我们创建两个Pipeline来分别处理这两类逻辑：

在`pipelines.py`中添加：

```
class ExtractDataPipeline(object):
	def process_item(self, item, spider):
		...
		return item
```

和

```
class SaveDataPipline(object):
	def process_item(self, item, spider):
		...
		return item
```

并在`settings.py`中进行注册：

```
...
ITEM_PIPELINES = {
   'soufang.pipelines.ExtractDataPipeline': 300,
   'soufang.pipelines.SaveDataPipline':500,
}
...
```

后面的数字范围是0-1000之内的任意整数，代表的是优先级。可以看出来我们是优先清洗数据，然后保存数据。

## 清洗数据

清洗数据的代码如下，就是将每一个字段的信息的html标签进行剥离，并且截取掉冗余的字符串：

```
# 剔除html代码
def take_out_html(str):
    dr = re.compile(r'<[^>]+>', re.S)
    return dr.sub('', str)


class ExtractDataPipeline(object):
    def process_item(self, item, spider):
        print ">>> ExtractDataPipeline >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
        self.extract_data(item, "url")
        self.extract_data(item, "id", f=5)
        self.extract_data(item, "publish_time", f=-10)
        self.extract_data(item, "title")
        self.extract_data(item, "total_price")
        self.extract_data(item, "house_type", f=3)
        self.extract_data(item, "house_build_area", f=5)
        self.extract_data(item, "house_use_area", f=5)
        self.extract_data(item, "house_age", f=3)
        self.extract_data(item, "orientation", f=3)
        self.extract_data(item, "floor", f=3)
        self.extract_data(item, "structure", f=3)
        self.extract_data(item, "decoration", f=3)
        self.extract_data(item, "residential_category", f=5)
        self.extract_data(item, "building_class", f=5)
        self.extract_data(item, "property_right", f=5)
        self.extract_data(item, "property_name")
        self.extract_data(item, "school")
        self.extract_data(item, "supporting_facilities")
        self.extract_data(item, "bread_city", t=-3)
        self.extract_data(item, "bread_area", t=-3)
        self.extract_data(item, "bread_positon", t=-3)
        return item

    # 剔除数据中的多余部分
    def extract_data(self, item, key_bread_positon, f=None, t=None):
        if self.check_key_exist(item, key_bread_positon):
            item[key_bread_positon] = take_out_html(item[key_bread_positon]).strip()[f:t]
        else:
            item[key_bread_positon] = ""
        self.print_itme_value(item, key_bread_positon)

    # 检查key是否存在
    def check_key_exist(self, item, key):
        return key in item.keys()

    # 输出数据
    def print_itme_value(self, item, key):
        print key + " >>> " + item[key]
```

## 保存数据到MongoDB

保存数据我们使用[MongoDB](http://www.mongodb.org/)，这是一个很简单易用且功能强大的**非关系型数据库**，它可以把数据保存成一种类似Json的格式。

使用教程网上很容易搜到，推荐一个自学网站：[MongoDB教程](http://www.runoob.com/mongodb/mongodb-tutorial.html)

mongo的可视化工具也有很多，我使用的是[Robomongo](https://robomongo.org/)。

使用mongo我们需要在`settings.py`中增添下面的配置信息：

```
...
MONGO_URI = "mongodb://localhost:27017";
MONGO_DATABASE = "soufang";
...
```

下面是`SaveDataPipline`的代码：

```
class SaveDataPipline(object):
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        print ">>> SaveDataPipline >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>"
        collection_name = self.mongo_db
        self.db[collection_name].insert(dict(item))
        return item
```

### 去除重复项

为了保证id重复的房源信息不再重复爬取，我们可以对数据库建立**唯一索引**，这样既能够提高查询效率，又能够去除重复数据。

后台建立唯一索引：

```
db.soufang.ensureIndex({"id":1},{"unique":true},{"background":true})
```

## 开始爬取！

终于可以开心的爬取数据啦~

命令行输入`mongod`开启mongo服务后，进入我们的爬虫项目，开启我们的爬虫：

```
scrapy crawl esflist
```

![](/img/16_12_07/001.gif)

在Robomongo中我们执行一条查询语句`db.soufang.find()`就可以看到我们爬取到的全部数据：

![](/img/16_12_07/002.png)

执行`db.soufang.findOne()`可以查询到一条记录：

```
> db.soufang.findOne()
{
	"_id" : ObjectId("58497108f26cd3d1ac97004b"),
	"orientation" : "",
	"bread_area" : "普陀",
	"publish_time" : "2016-10-20",
	"id" : "268151561",
	"property_name" : "甘泉一村",
	"title" : "甘泉一村 边套全明户型 黄金3楼 得房率高 配套成熟 便利",
	"building_class" : "板楼",
	"bread_positon" : "甘泉",
	"house_build_area" : "55.27㎡",
	"bread_city" : "上海",
	"house_use_area" : "",
	"house_type" : "2室1厅1厨1卫",
	"structure" : "平层",
	"decoration" : "简装修",
	"school" : "",
	"total_price" : "275",
	"url" : "http://esf.sh.fang.com/chushou/3_268151561.htm",
	"house_age" : "1987年",
	"floor" : "中层(共6层)",
	"property_right" : "个人产权",
	"supporting_facilities" : "",
	"residential_category" : "普通住宅"
}
```

全上海目前在售的二手房源大概有60到70万套之间，我们可以把速度缩短到1秒中爬取一条，如果24小时爬取的话，预计6天可以爬完全上海。不过我们的目的是对数据进行分析，没必要盲目的采集大量的数据，只要数据够用即可。

如果数据量过大，后面进行训练机器性能也跟不上，所以我准备将总数据量控制在5万条（按照各区域房源数量占比进行组合）。

## what`s next?

接下来我们将构建一个可视化的界面来直观的观察我们爬取到的房源数据。
