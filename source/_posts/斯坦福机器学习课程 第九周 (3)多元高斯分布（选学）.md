title: 斯坦福机器学习课程 第九周 (3)多元高斯分布（选学）
tags:
  - 机器学习
  - 斯坦福课程
categories:
  - 机器学习
comments: true
date: 2017-05-26 21:00:00

mathjax: true
---

## 多元高斯分布

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/Cf8DF/multivariate-gaussian-distribution)

> 接下来我将介绍**多元高斯分布 (multivariate Gaussian distribution)**。它是我们目前所学的异常检测算法的延伸，它有一些优势也有一些劣势，它能捕捉到一些之前的算法检测不出来的异常。

### 异常检测算法无法解决的问题

还是现以一个例子来解释：

假设在数据中心监控机器的例子中，我们有如下的内存和CPU使用数据：

![](/img/17_05_26/001.png)

其中对于这两个维度的数据，都服从正态分布：

|$x\_1$|$x\_2$|
|:-:|:-:|
|![](/img/17_05_26/002.png)|![](/img/17_05_26/003.png)|

如果现在在我们的测试集中，有一个异常数据点出现在下图的位置中：

![](/img/17_05_26/004.png)

那么在这种情况下我们会发现，这一点对应的两个维度下的概率其实都不低，从$p(x)$的结果上，我们无法准确预测这个样本是否属于异常。

产生这个问题的实际原因其实是这样的，从$x\_1$和$x\_2$这两个维度来看，我们的正常数据及时大多数集中分布在这样一个范围内：

![](/img/17_05_26/005.png)

但我们使用之前的异常检测算法，其实是以中心区域向外以正圆的形式扩散的。也就是说距离中心区域距离相等的点，对应的$p(x)$都是一样的，所以我们可能无法检测到这一个异常样本，因为它也处在一个$p(x)$比较大的范围内：

![](/img/17_05_26/006.png)

所以，为了解决异常检测算法的这一问题，接下来我解释改良版的异常检测算法，要用到叫做**多元高斯分布（多元正态分布）**的东西。

### 通过多元高斯分布改良异常检测算法

在多元高斯分布中，对于n维特征$x \in R^n$，不要把模型$p(x\_1)$,$p(x\_2)$,...,$p(x\_n)$分开，而要建立$p(x)$整体的模型。

多元高斯分布的参数包括向量$µ$和一个$n×n$的矩阵$Σ$。

$$
µ \in R^n  \\\\
Σ \in R^{n×n}
$$

$Σ$被称为**协方差矩阵**，它类似于我们之前学习PCA的时候所见到的协方差矩阵。

带入之后计算$p(x)$：

$$
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))
$$

> 这个公式不用背，用的时候再去查。

注意，公式中的$|Σ|$这一项，代表矩阵$Σ$的**行列式**。在Octave中可以用下面的代码来计算：

```
det(Sigma)
```

其实这个公式是什么样并不重要，更重要的是$p(x)$到底是什么样。

### 多元高斯分布的样子

我们来对比一下不同的$µ$和不同的$Σ$组合后，对应的$p(x)$的形状。

|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&0\\\\0&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}0.6&0\\\\0&0.6\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}2&0\\\\0&2\end{matrix}\right]\end{equation}$|
|:-:|:-:|:-:|
|![](/img/17_05_26/007.png)|![](/img/17_05_26/009.png)|![](/img/17_05_26/011.png)|
|![](/img/17_05_26/008.png)|![](/img/17_05_26/010.png)|![](/img/17_05_26/012.png)|

表格从上到下依次是三种情况对应的参数、三维图像以及俯视图。

$µ$作为均值，象征着中心区域对应的坐标点。$Σ$是协方差矩阵，它衡量的是特征$x\_1$和$x\_2$的方差。

从图中可以看出来，当缩小协方差$Σ$时，中心区域的凸起就会变得更细长；当扩大协方差$Σ$时，中心区域的凸起就会变得更扁平。因为概率分布的积分必须等于1，所以如果你缩小方差，就会变得细长，反之就会变得扁平。

接下来我们尝试对协方差中使用不同的数值，来观测$p(x)$形状的变化：

|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&0\\\\0&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}0.6&0\\\\0&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}2&0\\\\0&1\end{matrix}\right]\end{equation}$|
|:-:|:-:|:-:|
|![](/img/17_05_26/007.png)|![](/img/17_05_26/013.png)|![](/img/17_05_26/015.png)|
|![](/img/17_05_26/008.png)|![](/img/17_05_26/014.png)|![](/img/17_05_26/016.png)|

在协方差$Σ$中，左上角元素对应的是$x\_1$特征，右下角元素对应的是$x\_2$特征。

在第二组图中，我们可以看到，当我们缩小$x\_1$到原先的0.6倍而$x\_2$保持原先的大小时，由于相当于是对特征$x\_1$的方差进行了缩小，所以图像在$x\_1$的方向上，会显得更细长。

在第三组图中，我们可以看到，当我们放大$x\_1$到原先的2倍而$x\_2$保持原先的大小时，由于相当于是对特征$x\_1$的方差进行了放大，所以图像在$x\_1$的方向上，会显得更扁平。

对于多元高斯分布来说，一个很棒的事情就是我们可以用它来**对数据的相关性建模**。也就是说，我们可以用它来给$x\_1$和$x\_2$高度相关的情况建立模型。具体来说，我们可以通过改变协方差$Σ$非对角线上的元素来得到不同的高斯分布：

|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&0\\\\0&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&0.5\\\\0.5&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&0.8\\\\0.8&1\end{matrix}\right]\end{equation}$|
|:-:|:-:|:-:|
|![](/img/17_05_26/007.png)|![](/img/17_05_26/017.png)|![](/img/17_05_26/019.png)|
|![](/img/17_05_26/008.png)|![](/img/17_05_26/018.png)|![](/img/17_05_26/020.png)|

可以看出来，当我改变了非对角线上元素的值时，$p(x)$的图像也变得倾斜了；当我增大了这些元素时，这个倾斜的分布图像变得更细长了。

上面是我们把这些非对角线上元素设置为正数时的样子，那么如果我们把它们设置为负数时，会是什么样呢？

它们的倾斜方向会发生改变：

|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&0\\\\0&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&-0.5\\\\-0.5&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&-0.8\\\\-0.8&1\end{matrix}\right]\end{equation}$|
|:-:|:-:|:-:|
|![](/img/17_05_26/007.png)|![](/img/17_05_26/021.png)|![](/img/17_05_26/023.png)|
|![](/img/17_05_26/008.png)|![](/img/17_05_26/022.png)|![](/img/17_05_26/024.png)|

如果我们改变$µ$，会对图像$p(x)$产生什么影响呢？

它们会发生平移：

|$\begin{equation}µ=\left[\begin{matrix}0\\\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&0\\\\0&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}0\\\\0.5\end{matrix}\right]Σ=\left[\begin{matrix}1&0\\\\0&1\end{matrix}\right]\end{equation}$|$\begin{equation}µ=\left[\begin{matrix}1.5\\\\-0.5\end{matrix}\right]Σ=\left[\begin{matrix}1&0\\\\0&1\end{matrix}\right]\end{equation}$|
|:-:|:-:|:-:|
|![](/img/17_05_26/007.png)|![](/img/17_05_26/025.png)|![](/img/17_05_26/027.png)|
|![](/img/17_05_26/008.png)|![](/img/17_05_26/026.png)|![](/img/17_05_26/028.png)|

$µ$有两个元素，第一个元素对应的是图像在$x\_1$方向上的位移，第二个元素对应的是图像在$x\_2$方向上的位移。当为正数时，是沿着增大的方向平移，反之是沿着缩小的方向平移。

### 总结

不同的图片，能够帮助你了解**多元高斯分布**所能描述的概率分布是什么样。它最重要的优势就是能够描述当两个特征变量之间可能存在正相关或者是负相关关系的情况。

## 通过多元高斯分布来处理异常检测问题

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/DnNr9/anomaly-detection-using-the-multivariate-gaussian-distribution)

> 在上一节中，我们谈到了多元高斯分布，而且也看到了一些例子通过改变参数$µ$和$Σ$来给不同的概率分布建模。
> 
> 在这节中，我们使用它们来开发另一种异常检测算法。

### 知识回顾

再回顾一下**多元高斯分布（或者叫多元正态分布）**：

有两个参数：$µ$和$Σ$。

$µ$是一个n维向量，协方差矩阵$Σ$是一个$n×n$的矩阵。

其对应的概率分布公式如下：

$$
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))
$$

随着你改变$µ$和$Σ$，你可以得到一系列不同的概率分布：

![](/img/17_05_26/029.png)

#### 参数拟合

接下来让我们谈一下**参数拟合问题（参数估计问题）**。

如果我有一组符合高斯分布的样本：

$$
｛x^{(1)},x^{(2)},...,x^{(m)}｝
$$

我们可以通过公式来得到参数$µ$和$Σ$：

$$
µ=\frac{1}{m}\sum^m\_{i=1}x^{(i)}
$$

$$
Σ=\frac{1}{m}\sum^m\_{i=1}(x^{(i)}-µ)(x^{(i)}-µ)^T
$$

#### 具体应用步骤

有了这两个参数值，我们就可以把他们应用到具体的异常检测算法中了。具体步骤是这样的：

假设我们有如下的训练样本：

![](/img/17_05_26/030.png)

- 首先，用我们的训练集来拟合模型$p(x)$，得到参数$µ$和$Σ$：

$$
µ=\frac{1}{m}\sum^m\_{i=1}x^{(i)}
$$

$$
Σ=\frac{1}{m}\sum^m\_{i=1}(x^{(i)}-µ)(x^{(i)}-µ)^T
$$

- 然后，当你得到一个新的测试样本时，我们用下面的公式来计算其$p(x)$：

$$
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))
$$

- 最后，如果$p(x)<ε$时，就把它标记为是一个异常样本，反之，如果$p(x)>=ε$则不标记为异常样本。

所以，如果使用多元高斯分布来解决异常检测问题，你可能会得到这样一个高斯分布的概率模型：

![](/img/17_05_26/031.png)

所以，他可以正常的识别出之前用普通的异常检测算法无法正确检测的那个异常样本。

### 多元高斯模型和原始模型的关系

最后说一下**多元高斯分布模型**和原来的模型它们之间的关系。

原先的模型是这样的：

$$
p(x)=p(x\_1;µ\_1,σ\_1^2)×p(x\_2;µ\_2,σ\_2^2)×...×p(x\_n;µ\_n,σ\_n^2)
$$

事实上，你可以证明我们原先的这种模型，是多元高斯模型的一种。它其实是一种等高线都沿着坐标轴方向的多元高斯分布，但这里我不给出证明过程：

![](/img/17_05_26/032.png)

所以这三个图像，全都是你可以用原来的模型来拟合的高斯分布的例子。

其实这个模型对应于一种多元高斯分布的特例，具体来说这个特例被定义为约束$p(x)$的分布(也就是多元高斯分布$p(x)$)，使得它的概率密度函数的等高线是沿着轴向的。也就是要求**协方差矩阵$Σ$的非对角线元素都为0**。

所以你可以得到多元高斯分布$p(x)$看起来是上图中这三种样式的。你会发现，在这3个例子中，它们的轴都是沿着$x\_1$，$x\_2$的轴的。

因此，在**协方差矩阵$Σ$的非对角线元素都为0**的情况下，这两者是相同的：

$$
p(x)=p(x\_1;µ\_1,σ\_1^2)×p(x\_2;µ\_2,σ\_2^2)×...×p(x\_n;µ\_n,σ\_n^2)
$$

$$
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))
$$

### 何时使用多元高斯模型？何时使用原始模型？

既然我们知道了原始的模型是多元高斯模型的一个特例，那么应该在什么时候用哪个模型呢？

事实情况是，原始模型比较常用，而多元高斯模型比较少用。

假设在你的样本中，$x\_1$和$x\_2$是线性相关的特征组合，下面是这两种算法在处理不正常的特征组合时的具体方式对比：

|原始模型|多元高斯模型|
|:-:|:-:|
|捕捉到这两个特征，建立一个新的特征$x\_3$(比如$x\_3=\frac{x\_1}{x\_2}$)，去尝试手工组合并改变这个新的特征变量，从而使得算法能很好的工作。|自动捕捉不同特征变量之间的相关性。|
|运算量小(更适用于特征变量个数$n$很大的情况)|计算更复杂（Σ是$n×n$的矩阵，这里会涉及两个$n×n$的矩阵相乘的逻辑，计算量很大）|
|即使训练样本数$m$很小的情况下，也能工作的很好|必须满足$m>n$，或者$Σ$不可逆（奇异矩阵）。这种情况下，还可以帮助你省去为了捕捉特征值组合而手动建立额外特征变量所花费的时间。|

#### 一种异常情况的应对

当你在拟合多元高斯模型时，如果你发现协方差矩阵$Σ$是**奇异的（不可逆的）**，一般只有两种情况：

- 第一种是它没有满足$m>n$的条件
- 第二种情况是，你有冗余特征变量 
	- 冗余特征变量的意思是出现了以下两种情况的任意一种：
		- 出现了两个完全一样的特征变量（你可能不小心把同一个特征变量复制了两份）
		- 如果你有$x\_3=x\_4+x\_5$，这里$x\_3$其实并没有包含额外的信息，相对于$x\_4$和$x\_5$来说，它就是冗余特征变量。

这是你调试算法时的一个小知识，可能很少会遇到，但是一旦你发现$Σ$不可逆，那么首先需要从这两个方面来考虑解决方案。