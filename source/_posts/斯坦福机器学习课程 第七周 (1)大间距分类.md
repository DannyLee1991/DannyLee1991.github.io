title: 斯坦福机器学习课程 第七周 (1)大间距分类
tags:
  - 机器学习
  - 斯坦福课程
categories:
  - 机器学习
comments: true
date: 2017-02-06 00:42:58
mathjax: true
---

## 优化目标

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/sHfVT/optimization-objective)

> 到目前为止，你已经见过一系列不同的学习算法。在监督学习中许多学习算法的性能都非常类似，因此重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是应用这些算法时所创建的大量数据。
> 
> 在应用这些算法时，表现情况通常依赖于你的水平。比如你为学习算法所设计的 特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法，广泛的应用于工业界和学术界，它被称为**支持向量机(Support Vector Machine)**。
> 
> 与逻辑回归和神经网络相比，**支持向量机**或者简称**SVM**在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。
> 
> 因此，在接下来的视频中我会探讨这一算法，在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于**支持向量机**，鉴于该算法的强大和受欢迎度，在本课中我会花许多时间来讲解它，它也是我们所介绍的最后一个监督学习算法。

### 支持向量机引入

为了描述**支持向量机**，我将会从逻辑回归开始，展示我们如何一点一点修改，来得到本质上的支持向量机。

在逻辑回归中，我们已经熟悉了它的假设函数形式：

$$
h\_{\theta}(x)=\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}
$$

和S型激励函数：

<img src="/img/17_02_06/001.png" width = "300" height = "200" align=center />

现在让我们一起来考虑下，我们想要逻辑回归做什么？

- 如果有一个样本为$y=1$，那么我们希望假设函数$h(x)≈1$，即$\theta^{T}>>0$。你不难发现，此时逻辑回归的输出将趋近于1。

- 如果有另一个样本为$y=0$，那么我们希望假设函数$h(x)≈0$，即$\theta^{T}<<0$。此时逻辑回归的输出将趋近于0。

---

如果你进一步观察逻辑回归的代价函数，你会发现每个样本(x, y)都会为总代价函数增加这样的一项：

$$
-(ylogh\_{\theta}(x) + (1-y)log(1-h\_{\theta}(x))) 
= -ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})
$$

在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。

现在一起来考虑**y=1**和**y=0**的两种情况：

- **y=1的情况下（即$\theta^{T}x>>0$）**：

对于

$$
-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})
$$

由于$(1-y)=0$，所以我们只需考虑前半部分：

$$
-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}
$$

如果画出代价函数关于$z$的图，你会看到下图：

<img src="/img/17_02_06/002.png" width = "300" height = "200" align=center />

我们可以看到，当$z$增大时(即$\theta^{T}x$增大时)，$z$对应的值会变得非常小，对整个代价函数而言，影响也非常小。

现在开始建立**支持向量机**，我们会从这个代价函数$-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}$开始，一点点的修改：

我们画出一个非常接近于逻辑回归函数的折线，这个折线经由$z=1$的一点的两条线段组成：

<img src="/img/17_02_06/003.png" width = "300" height = "200" align=center />

到这里已经非常接近逻辑回归中使用的代价函数了，只是这里是由两条线段组成。先不要考虑线段的斜率，这并不重要，重要的是我们将在$y=1$的前提下使用新的代价函数。

- **y=0的情况下（即$\theta^{T}x<<0$）**：

对于

$$
-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})
$$

由于$y=0$，所以我们只需考虑后半部分：

$$
(1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})
$$

如果画出代价函数关于$z$的图，你会看到下图：

<img src="/img/17_02_06/004.png" width = "300" height = "200" align=center />

用相似的方法，我们开始建立**支持向量机**：

<img src="/img/17_02_06/005.png" width = "300" height = "200" align=center />

我们将在$y=0$的前提下使用新的代价函数。

--- 

那么现在我们来给这两个方程命名：

对于这个函数：

<img src="/img/17_02_06/003.png" width = "300" height = "200" align=center />

我们命名为**$cost\_{1}(z)$**。

对于第二个函数：

<img src="/img/17_02_06/005.png" width = "300" height = "200" align=center />

我们命名为**$cost\_{0}(z)$**。

这里的下标指的是在函数中对应的$y=1$和$y=0$的情况。

---

### 构建支持向量机

拥有了这些定义之后，现在我们就开始构建**支持向量机**。

#### 1.替换逻辑回归函数

这就是我们在逻辑回归中使用的代价函数$J(\theta)$：

![](/img/17_02_06/006.png)

对于支持向量机而言，实际上，我们要将

- 上面式子中的这一项：$(-logh\_{\theta}(x^{(i)}))$替换为：$cost\_{1}(z)$，即:$cost\_{1}(\theta^{T}x^{(i)})$

- 同样，这一项：$((-log(1-h\_{\theta}(x^{(i)}))))$替换为：$cost\_{0}(z)$，即:$cost\_{0}(\theta^{T}x^{(i)})$

这里替换之后的$cost\_{1}(z)$和$cost\_{0}(z)$就是上面提到的那两条靠近逻辑回归函数的折线。

所以对于**支持向量机**的最小化代价函数问题，代价函数的形式如下：

$$
\mathop{min}\limits\_{θ} 
\frac{1}{m}[
\sum\_{i=1}^{m}
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{\lambda}{2m}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

#### 2.去除多余的常数项 $\frac{1}{m}$

现在按照**支持向量机**的惯例，我们去除$\frac{1}{m}$这一项，因为这一项是个常数项，即使去掉我们也可以得出相同的$\theta$最优值：

$$
\mathop{min}\limits\_{θ} 
\sum\_{i=1}^{m}
[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{\lambda}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

#### 3.正则化项系数的处理

在逻辑回归的目标函数中，我们有两项表达式：

- 来自于训练样本的代价函数:

$$
\frac{1}{m}[
\sum\_{i=1}^{m}
y^{(i)}
(-logh\_{\theta}(x^{(i)}))+
(1-y^{(i)})
((-log(1-h\_{\theta}(x^{(i)}))))
]
$$

- 正则化项：

$$
\frac{\lambda}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

我们不得不使用正则化项来平衡我们的代价函数。这就相当于：

$$
A + \lambda B
$$

其中A相当于上面的第一项，B相当于第二项。

我们通过修改不同的正则化参数$\lambda$来达到优化目的，这样我们就能够使得训练样本拟合的更好。

但对于**支持向量机**，按照惯例我们将使用一个不同的参数来替换这里使用的$\lambda$来实现权衡这两项的目的。这个参数我们称为**C**。同时将优化目标改为:

$$
CA + B
$$

因此，在逻辑回归中，如果给$\lambda$一个很大的值，那么就意味着给与$B$了一个很大的权重，而在**支持向量机**中，就相当于对$C$设定了一个非常小的值，这样一来就相当于对$B$给了比$A$更大的权重。

因此，这只是一种来控制这种权衡关系的不同的方式。当然你也可以把这里的$C$当做$farc{1}{\lambda}$来使用。

因此，这样就得到了在**支持向量机**中的我们的整个优化目标函数：

$$
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

---

最后有别于**逻辑回归**的一点，对于**支持向量机**假设函数的形式如下：

$$
h\_{\theta}(x) = 1 \ \ \ if \ \theta^Tx \ge 0
$$

$$
h\_{\theta}(x) = 0 \ \ \ if \ \theta^Tx \lt 0
$$

而不是**逻辑回归**中的S型曲线：

$$
h\_{\theta}(x)=\frac{1}{1+e^{-x}}
$$

## 大间距的直觉

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/wrjaS/large-margin-intuition)

> 人们有时将**支持向量机**看做是**大间距分类器**。在这一部分，我将介绍其中的含义，这有助于我们直观地理解SVM模型的假设是什么样的。

这是我的支持向量机模型的代价函数：

$$
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

- 如果你有一个正样本，即$y=1$时，那么代价函数$cost\_{1}(z)$的图像如下：

<img src="/img/17_02_06/007.png" width = "300" height = "200" align=center />

可以看出，**只有在$z\ge1$(即$\theta^{T}x\ge1$)时(不仅仅是$\ge0$)，代价函数$cost\_{1}(z)$的值才等于0**。

- 反之，如果你有一个负样本，即$y=0$时，那么代价函数$cost\_{0}(z)$的图像如下：

<img src="/img/17_02_06/008.png" width = "300" height = "200" align=center />

可以看出，**只有在$z\le-1$(即$\theta^{T}x\le-1$)时(不仅仅是$\lt0$)，代价函数$cost\_{0}(z)$的值才等于0**。

这是**支持向量机**的一个有趣的性质。

### 安全距离因子

事实上，在逻辑回归中：

- 如果你有一个正样本，即$y=1$的情况下，我们仅仅需要$\theta^{T}x\ge0$；

- 如果你有一个负样本，即$y=0$的情况下，我们仅仅需要$\theta^{T}x\lt0$；

就能将该样本恰当的分类了。

但是**支持向量机**的要求更高，不仅仅要求$\theta^{T}x\ge0$或$\theta^{T}x\lt0$，而且要求$\theta^{T}x$比0大很多，或小很多。比如这里要求$\theta^{T}x\ge1$以及$\theta^{T}x\le-1$。

这就相当于在**支持向量机**中嵌入了一个额外的安全因子（或者说是安全距离因子）。接下来让我们来看看这个因子会导致什么结果：

具体而言，我接下来会将代价函数中的常数项$C$设置成一个非常大的值，比如100000或者其他非常大的数，然后再来观察支持向量机会给出什么结果。

当代价函数中

$$
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

$C$的值非常大时，则最小化代价函数的时候，我们会很希望找到一个使第一项：

$$
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]
$$

为0的最优解。

可以看到当输入一个正样本$y^{(i)}=1$时，我们想令上面这一项为0，从图中可以得出

<img src="/img/17_02_06/007.png" width = "300" height = "200" align=center />

对于代价函数$cost\_{1}(z)$我们需要使得$\theta^{T}x^{(i)}\ge1$。

类似地，对于一个负训练样本$y^{(i)}=0$时，我们想令上面这一项为0，从图中可以得出

<img src="/img/17_02_06/008.png" width = "300" height = "200" align=center />

对于代价函数$cost\_{0}(z)$我们需要使得$\theta^{T}x^{(i)}\le-1$。

---

这样一来会产生下面这种优化问题：

因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是：

$$
\mathop{min}\limits\_{θ}
C0+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

我们知道是$C0$的结果是0，因此可以删掉，所以最终得到的结果是：

$$
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

其中：

- 若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$
- 若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$

这样我们就得到了一个非常有趣的决策边界。

### SVM决策边界：线性分割案例

具体而言，如果你仔细观察下面这个既有正样本又有负样本的数据集

<img src="/img/17_02_06/009.png" width = "300" height = "200" align=center />

不难看出，这个数据集是线性可分的（即存在一条直线把正负样本分开）。可以看出有很多直线都可以把正负样本区分开，比如下面这两条看起来不太自然的直线：

<img src="/img/17_02_06/010.png" width = "300" height = "200" align=center />

支持向量机会选择黑色的这一条直线：

<img src="/img/17_02_06/011.png" width = "300" height = "200" align=center />

这条直线看起来好很多，因为它看起来更加稳健。在数学上来讲就是这条直线拥有相对于训练数据更大的最短距离，这个所谓的距离就是指**间距(margin)**：

<img src="/img/17_02_06/012.png" width = "300" height = "200" align=center />

而之前两条粉线和蓝线距离训练样本非常近，在分离样本时就会表现的比黑线差。

这就是**支持向量机**拥有[鲁棒性](http://baike.baidu.com/link?url=My7Y1mL_9uj-XdR2DC2kyGLop-AaPdzSgdNmgRmaJVYV77puxNs-_A7ERwLv3uWih02JCu6esljRn90mc3EkMwKXBckm_6wSqU42EX06vC4ouxQhinIZco7crxr7HetC)的原因。因为它一直努力用一个最大间距来分离样本。因此支持向量机分类器有时又被称为**大间距分类器**。

也许你想知道支持向量机是如何做到产生这个大间距分类器的，目前我还没解释这一点，在下一节中我会直观的来解释这一点。目前这个例子只是用于理解**支持向量机模型**的做法，即努力将正负样本用最大的间距区分开。

### 大间距分类器中的异常值

最后要讲的一点是对于支持向量机中的异常数据的处理。在下面这组训练集中：

<img src="/img/17_02_06/013.png" width = "300" height = "200" align=center />

我们通过使用支持向量机来进行分类，会得到这条黑色的决策边界，从而最大间距的区分这两种数据：

<img src="/img/17_02_06/014.png" width = "300" height = "200" align=center />

当有一个异常值产生时：

<img src="/img/17_02_06/015.png" width = "300" height = "200" align=center />

我们的算法会受到异常值的影响。这时我们将支持向量机中的正则化因子$C$设置的非常大，那么我们会得到类似这样一条粉色的决策边界：

<img src="/img/17_02_06/016.png" width = "300" height = "200" align=center />

那么我们仅仅通过一个异常值，就将我们的决策边界旋转了这么大的角度，实在是不明智的。

<img src="/img/17_02_06/017.png" width = "300" height = "200" align=center />

当我们的正则化因子$C$的值非常大时，支持向量机确实会如此处理，但如果我们适当的减小$C$的值，你最终还是会得到那条黑色的决策边界的。

如果数据是线性不可分的话，像这样：

<img src="/img/17_02_06/018.png" width = "300" height = "200" align=center />

支持向量机也可以恰当的将它们分开。

值得提醒的是$C$的作用其实等同于$\frac{1}{\lambda}$，$\lambda$就是我们之前用到的正则化参数。在支持向量机中，$C$不是很大的时候，可以对包含异常数据、以及线性不可分的数据有比较好的处理效果。

稍后我们还会介绍支持向量机的偏差和方差，希望到那时候关于如何处理参数的这种平衡会变得更加清晰。

## 大间距分类器背后的数学原理(选学)

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/3eNnh/mathematics-behind-large-margin-classification)

> 这一节将介绍大间距分类背后的数学原理。
> 
> 本节作为选学内容，你完全可以跳过，但是听听这节课可能让你对支持向量机中的优化问题，以及如何得到大间距分类器产生更好的直观理解。

### 向量内积

首先，带大家复习一下**向量内积**的知识。

假设我们有两个二维向量：

$$
u=
 \begin{bmatrix}
   u\_1 \\\
	u\_2
 \end{bmatrix}
 \\
 v=
 \begin{bmatrix}
   v\_1 \\\
   v\_2
 \end{bmatrix}
$$

我们把

$$
u^{T}v
$$

的计算结果称作向量$u$和$v$之间的**内积**。

由于这里我们用的是二维向量，因此我们可以把这两个向量绘制在同一坐标系内，向量$u$和$v$如下：

<img src="/img/17_02_06/019.png" width = "300" height = "200" align=center />

其中我们用$||u||$来表示$u$的**范数**（即$u$的长度），因此$||u||$的计算公式如下：

$$
||u||=\sqrt{u\_1^2+u\_2^2}
$$

下面我们来看看**向量内积**具体是如何计算的：

将向量$v$投影到向量$u$上，如下图我们对向量$v$做一个相对于向量$u$的直角投影：

<img src="/img/17_02_06/020.png" width = "300" height = "200" align=center />

投影之后的长度就是图中红线$p$的长度：

$$
p = 向量v投影到向量u上的长度
$$

同时也有另外一种计算内积的方式：

$$
u^Tv=p·||u||
$$

通过这种方式计算出来的内积，答案和之前也是一样的。

事实上，如果你想要使用将$u$投影到$v$上来用这种方式来计算内积，得到的答案也是相同的。

要注意的一点是，这里的$p$是有符号的，如果两个向量的夹角大于90°，像下图中这种情形，如果将$v$投影到$u$上会得到这样一种投影：

<img src="/img/17_02_06/021.png" width = "300" height = "200" align=center />

此时的$p$就是一个负数。

> 在向量的内积问题中，如果两个向量的夹角小于90°，那么$p$的符号就是为正；如果两个向量的夹角大于90°，那么$p$的符号就为负。

这就是**向量内积**的知识，接下来我们尝试使用它来理解支持向量机中的目标函数。

### SVM决策边界

下面是我们在支持向量机中的目标函数:

$$
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

其中

- 若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$
- 若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$

接下来为了让目标函数更容易被分析，我们来忽略掉截距的影响，令$\theta\_0=0$，这样更容易绘制示意图。并且我们将特征数$n$设置为2，因此我们仅有两个特征$x\_1$和$x\_2$：

$$
\begin{align\*}
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
&=
\frac{1}{2}
(\theta\_1^2+\theta\_2^2)
\\\\&=
\frac{1}{2}
(\sqrt{\theta\_1^2+\theta\_2^2})^2
\end{align\*}
$$

其中

$$
\sqrt{\theta\_1^2+\theta\_2^2} = ||\theta||
$$

因此可以得出：

$$
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
=
\frac{1}{2}||\theta||^2
$$

可见，**支持向量机所做的事情，其实就是在极小化参数向量$\theta$范数的平方（或者说是长度的平方）**。

---

现在让我们来看看这两行的含义：

- 若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$
- 若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$

想一想$\theta^{T}x^{(i)}$这一项等于什么呢？

在前面我们画出了$u^Tv$的示意图，这里$\theta^T$就相当于$u^T$、$x^{(i)}$就相当于$v$。让我们来看一下示意图：

我们考虑一个单一的样本$x^{(i)}$，其坐标为$(x^{(i)}\_1,x^{(i)}\_2)$

<img src="/img/17_02_06/022.png" width = "300" height = "200" align=center />

这个训练样本点其实可以表示为一个训练样本向量：

<img src="/img/17_02_06/023.png" width = "300" height = "200" align=center />

现在，我们有一个参数向量：

<img src="/img/17_02_06/024.png" width = "300" height = "200" align=center />

那么我们向量内积的计算方式，通过使用之前的方法可以得出。训练样本向量投影到参数向量上的长度$p^{(i)}$，表示第i个训练样本在参数向量$\theta$上的投影：

<img src="/img/17_02_06/025.png" width = "300" height = "200" align=center />

根据之前我们所学到的，我们可以知道：

$$
\begin{align\*}
\theta^Tx^{(i)}
&=p^{(i)}·||\theta||
\\\\
&=\theta\_1x\_1^{(i)}+\theta\_2x\_2^{(i)}
\end{align\*}
$$

那么，这告诉我们了什么呢？这说明：

- 若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$
- 若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$

这里的约束项是可以用$p^{(i)}·||\theta||$来替代的：

- 若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$
- 若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$

因此，将其写入我们的优化目标后，**完整的目标函数**为：

$$
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
=
\frac{1}{2}||\theta||^2
$$

其中

- 若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$
- 若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$

---

#### 实例

现在让我们考虑下面这里的训练样本：

<img src="/img/17_02_06/026.png" width = "300" height = "200" align=center />

其中假设截距依然为0，即$\theta\_0=0$，我们来看一下支持向量机会选择什么样的决策边界。

假设有这样一条决策边界：

<img src="/img/17_02_06/027.png" width = "300" height = "200" align=center />

很明显，这不是一个好的决策边界，因为这个决策边界离训练样本很近，我们来看一下为什么支持向量机不会选择它。

由于**决策边界和参数向量是正交的(斜率相乘结果为-1)**([为什么决策边界和参数向量是正交的](https://zhidao.baidu.com/question/1992397864989257747.html))，我们可以绘制出对应的参数向量$\theta$：

<img src="/img/17_02_06/028.png" width = "300" height = "200" align=center />

> 这里由于我们指定了$\theta\_0=0$，也就意味着决策边界是过原点的。

假设我们以这一点为第一个训练样本：

<img src="/img/17_02_06/029.png" width = "300" height = "200" align=center />

我们可以画出这个样本向量到$\theta$的投影$p^{(1)}$：

<img src="/img/17_02_06/030.png" width = "300" height = "200" align=center />

类似的，我们也可以画出第二个样本向量到$\theta$的投影$p^{(2)}$：

<img src="/img/17_02_06/031.png" width = "300" height = "200" align=center />

我们会发现，这些$p^{(i)}$将会是一些非常小的数。因此当我们考察优化目标函数的时候：

- 对于**正样本($y^{(i)}=1$，即图中的"x"样本)**而言，我们需要$p^{(i)}·||\theta||\ge1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。

- 对于**负样本($y^{(i)}=-1$，即图中的"o"样本)**而言，我们需要$p^{(i)}·||\theta||\le-1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。

但我们的实际目标是希望找到一个参数$\theta$，使得它的范数$||\theta||$是尽可能小的，因此这并不是一个好的决策边界，因为我们的$||\theta||$比较大。

---

对于下面这个决策边界来说，情况就会有很大的不同：

<img src="/img/17_02_06/032.png" width = "300" height = "200" align=center />

这里，我们以纵坐标作为决策边界，那么我们的参数向量的方向就是垂直于它的方向：

<img src="/img/17_02_06/033.png" width = "300" height = "200" align=center />

如果我们现在再来绘制出样本向量在参数向量上的投影$p^{(1)}$和$p^{(2)}$的话，你会发现这些投影的长度比之前长多了：

<img src="/img/17_02_06/034.png" width = "300" height = "200" align=center />

因为投影$p$的长度变大了，随之$\theta$的范数$||\theta||$也相应的变小了。这就意味着通过选择第二种远离样本的决策边界，支持向量机可以使参数$\theta$的范数$||\theta||$变小很多。

这就是**为什么支持向量机可以产生大间距分类的原因**。

---

最后一点，我们的推导自始至终都使用了**截距为0（即$\theta\_0=0$）**这个简化假设。这样做的作用就是可以使得决策边界始终是通过原点的，如果你的决策边界不过原点，那么$\theta\_0\ne0$，但支持向量机会产生大间距分类器的结论依然成立（具体推导过程不再叙述，和这里很类似）。但是可以说明的是，即便$\theta\_0\ne0$，支持向量机仍然会找到正样本和负样本之间的大间距分隔。总之 我们解释了为什么支持向量机是一个大间距分类器。


