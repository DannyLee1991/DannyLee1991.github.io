title: 数学之美 技术详解(1)
date: 2016-05-03 10:06:55
tags: 
- 读书笔记
categories: 
- 读书笔记 
comments: true
---

## 统计语言模型（关键人物：贾里尼克）

计算机需要知道一个文字序列是否能构成一个大家理解而且有意义的句子，然后显示或者打印给使用者。

比如：

>美联储主席本·伯南克昨天告诉媒体7000亿美元的救助资金将借给上百家银行、保险公司和汽车公司。


如果改变一些词的顺序或者换掉一些词，将这句话变成：

>本·伯南克美联储主席昨天7000亿美元的救助资金告诉媒体将借给银行、保险公司和汽车公司上百家。

意思就含混了，虽然多少还能猜测到一点。

但是如果换成：

>联主美储本·伯诉体南将借天的救克告媒昨助资金70元亿00美给上百百百家银保行、汽车险公司公司和。


基本上读者就不知所云了。

**贾里尼克**的出发点很简单：

**一个句子是否合理，就看看它的可能性大小如何。至于可能性就用概率来衡量。**

第一个句子出现的概率大致是10^-20，第二个句子出现的概率是10^-25,第三个句子出现的概率是10^70。因此，第一个最有可能，它的可能是第二个句子的10万倍，是第三个句子的一百亿亿亿亿亿亿倍。

更通俗的描述：
假定S表示一个有意义的句子，由一连串特定顺序排列的词w1，w2，……，wn组成，这里n是句子的长度。既然S=w1,w2,...,wn,那么不妨把P(S)展开表示：

$$
\begin{cases}
	P(S)=P(w1,w2,...,wn)
\end{cases} 
$$

利用条件概率公式，S这个序列出现的概率等于每一个词出现的条件概率相乘，于是$P(w1,w2,...,wn)$可展开为：

$$
\begin{cases}
	P(w1,w2,...,wn) = P(w1)·P(w2|w1)·P(w3|w1,w2)...P(wn|w1,w2,...,wn-1)
\end{cases}
$$

其中$P(w1)$表示第一个词$w1$出现的概率；$P(w2|w1)$是在已知第一个词的前提下，第二个词出现的概率；以此类推。不难看出，到了词$wn$,
它的出现概率取决于它前面的所有词。

从计算上来看，前两个词的概率还是可以算的，到了第三个词的条件概率$P(w3|w1,w2)$已经非常难算了，因为它涉及到三个变量，每个变量到可能性都是一种语言字典的大小。

19世纪到20世纪初，俄罗斯有个数学家叫马尔可夫(Andrey Markov)，他给了个偷懒但还颇为有效的方法，也就是每当遇到这种情况时，
就假设任意一个词wi出现的概率只同它前面的词wi-1有关，于是问题就变得很简单了。这种假设在数学上称为马尔科夫假设。现在,S出现
的概率就变得简单了：
	
	P(S) = P(w1)·P(w2|w1)·P(w3|w2)...P(wi|wi-1)...P(wn|wn-1)	(3.3)

公式(3.3)对应的统计语言模型是二元模型。
当然，也可以假设一个词由前面的N-1个词决定，对应的模型稍微复杂些，被称为N元模型。

接下来的问题就是如何估计条件概率P(wi|wi-1):

	P(wi|wi-1) = P(wi-1,wi)/P(wi-1)

而估计联合概率P(wi-1,wi)和边缘概率P(wi-1),现在变得很简单。因为有了大量机读文本，也就是专业人士讲的语料库（Corpus），
只要数一数wi-1，wi这对词在统计的文本中前后相邻出现了多少次#(wi-1,wi),以及wi-1本身在同样的文本中出现了多少次#(wi-1),然后用两个
数分别除以语料库的大小#,即可得到这些词或者二元组的相对频度：

	f(wi-1,w1) = #(wi-1,wi)/#
	f(wi-1) = #(wi-1)/#

根据大数定理，只要统计量足够，相对频度就等于概率，即：

	P(wi-1,wi) ≈ #(wi-1,wi)/#
	P(wi-1) ≈ #(wi-1)/#

而P(wi|wi-1)就是这两个数的比值，再考虑到上面的两个概率有相同的坟墓，可以约掉，因此：

	P(wi|wi-1) ≈ #(wi-1,wi)/#(wi-1)

现在，读者也许已经开始能感受到数学的美妙之处了，它把一些复杂的问题变得如此简单。这似乎有点让人难以置信，用这么简单的
数学模型能解决复杂的语音识别、机器翻译等问题，而用很复杂的文法规则和人工智能却做不到。

在Google语音搜索Google Voice和中英文自动翻译（罗塞塔）中，发挥了最重要作用的就是这个统计语言模型。值得一提的是罗赛塔使用的是四元模型，
该模型存储于500台以上的google服务器中。

－－－－－－

（思考：是否可以通过词的信息量来动态计算N的值呢？）

－－－－－－
数据的平滑处理：

(wi-1,wi)在语料库中出现的次数#(wi-1,wi)为0怎么办？是否意味着P(wi|wi-1)＝0？
如果#(wi-1,wi)和#(wi-1)都只出现了一次，那么是否敢得出P(wi|wi-1)=1这样的绝对结论呢？

这样是不对的，因为我们敢于用对采样数据的观察结果来预测概率，是因为有大数定理在背后做支持，它的要求是有足够的观测值。

正确训练一个语言模型的办法之一就是：增加数据量，但即便如此，也亦然会遇到0概率或者统计量不足的问题。

1953年古德在他的老板图灵的指导下，提出了：在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法，
同时将折扣出来的那一小部分概率给予未看见的事件。他们给出了一个很漂亮的重新估算概率的公式，这个公式后来别成为：古德－图灵估计。

其原理如下：
对于没有看见的事件，我们不能认为它发生的概率就是0，因此我们从概率总量中，分配一个很小的比例给予这些没有看见的事件，
这样一来，看见的那些事件的概率总和就要小于1了，因此，需要将所有看见的事件概率调小一点。至于小多少，要根据“越是不可信的统计折扣越多”
的方法进行。

以统计字典中每个词的概率为例，来说明古德-图灵估计公式：

假定在语料库中出现r次的词有Nr个，特别地，未出现的词数量为N0。语料库的大小为N。那么，很显然

(3.11 公式)

出现r次的词在整个语料库中的相对频度则是r/N，如果不做任何优化处理，就以这个相对频度作为这些词的概率估计。

现在假定当r比较小时，它的统计可能不可靠，因此出现r次的那些词在计算它们的概率时要使用一个更小一点的次数，是dr（而不直接使用r），古德－图灵估计按照下面的公式计算dr：

（3.12 公式）
显然

（3.13 公式）

一般来说，出现的次数越多，单词数量越少。这种规律称为Zipf定律。

(见图3.2 )

可以看出r越大，词的数量Nr越小，即Nr+1<Nr。因此，一般情况下dr<r，而d0>0。这样就给未出现的词赋于了一个很小的非零值，从而解决了0概率的问题。同时下调了出现频率很低的词的概率。当然，在实际的自然语言处理中，一般对出现次数超过某个阈值对词，频率不下调，只对出现次数低于这个阈值的词，频率才下调，下调得到的频率总和给未出现的词。





















