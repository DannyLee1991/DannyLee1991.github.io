title: 斯坦福机器学习课程 第二周 (3)参数的计算分析
tags:
  - 算法
categories:
  - 算法
  - 机器学习
comments: true
date: 2016-08-12 10:58:00
mathjax: true
---

##  正规方程(Normal Equation)

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/2DKxQ/normal-equation)

这一节我们要讲**正规方程(Normal Equation)**。对于某些线性回归问题，用正规方程法求解参数$θ$的最优值更好。具体而言，到目前为止，我们一直在使用的线性回归的算法是**梯度下降法**，就是说，为了最小化代价函数$J(θ)$，我们通过多次迭代来计算梯度下降，来收敛到全局最小值。相反地，**正规方程法提供了一种可以直接一次性求解$θ$的最优值的解法**。

首先让我们先对这个算法有一个直观的理解，我们举一个例子来解释这个问题：

![](/img/16_08_12/001.png)

我们假设有一个非常简单的代价函数$J(θ)$，它就是一个实数$θ$的函数，所以现在假设$θ$只是一个标量($θ$只有一行，它是一个数字，不是向量)。假设我们的代价函数$J$是这个实参数$θ$的二次函数，所以$J(θ)$看起来是这样的:

![](/img/16_08_12/002.png)

那么如何最小化一个二次函数呢? 对于那些了解一点微积分的同学来说，你可能知道最小化的一个函数的方法是对它求导，并且将导数置零。这样你就可以求得使得$J(θ)$最小的$θ$值。

在实际情况中，我们感兴趣的是$θ$为$n+1$维参数向量的情况，如下：

![](/img/16_08_12/003.png)

我们如何最小化这个代价函数$J$?实际上微积分告诉我们一种方法: 对每个参数$θ$求$J$的偏导数，然后把它们全部置零。如果你这样做 并且求出$θ\_{0}$，$θ\_{1}$，一直到$θ\_{n}$的值，这样就能得到能够最小化代价函数$J$的$θ$值。

![](/img/16_08_12/004.png)


### 正规方程法 实例

举个实现正规方程法的例子：

假如说我有$m=4$个训练样本，训练集如下：

|**Size($feet^{2}$)**|**Number of bedrooms**|**Number of floors**|**Age of home(years)**|**Price($1000)**|
|:--:|:--:|:--:|:--:|:--:|
| $x\_{1}$ | $x\_{2}$ | $x\_{3}$ | $x\_{4}$ | $y$ |
| 2104 | 5 | 1 | 45 | 460 |
| 1416 | 3 | 2 | 40 | 232 |
| 1534 | 3 | 2 | 30 | 315 |
| 852 | 2 | 1 | 36 | 178 |

我们假设这4个训练样本就是我的所有数据，另外我所要做的是在我的训练集中加上一列对应额外特征变量的$x\_{0}$(就是那个取值永远是1的)。

||**Size($feet^{2}$)**|**Number of bedrooms**|**Number of floors**|**Age of home(years)**|**Price($1000)**|
|:--:|:--:|:--:|:--:|:--:|:--:|
|$x\_{0}$| $x\_{1}$ | $x\_{2}$ | $x\_{3}$ | $x\_{4}$ |$y$ |
| 1 | 2104 | 5 | 1 | 45 | 460 |
| 1 | 1416 | 3 | 2 | 40 | 232 |
| 1 | 1534 | 3 | 2 | 30 | 315 |
| 1 | 852 | 2 | 1 | 36 | 178 |

接下来我要做的是构建一个矩阵$X$，这个矩阵基本包含了训练样本的所有特征变量：

$$
X = 
\begin{bmatrix}
1 & 2104 & 5 & 1 & 45 \\\\
1 & 1416 & 3 & 2 & 40 \\\\
1 & 1534 & 3 & 2 & 30 \\\\
1 & 852 & 2 & 1 & 36 
\end{bmatrix}
$$

我要对$y$做类似的事情:

$$
y = 
\begin{bmatrix}
460 \\\\
232 \\\\
315 \\\\
178 
\end{bmatrix}
$$

最后，你可以通过下面这个方程来计算使得代价函数最小化的$θ$:

$$
θ = (X^{T}X)^{-1}X^{T}y
$$


这样就得到能够使得代价函数最小化的$θ$。

让我把这个写成更加通用的形式，在之后的视频中我会仔细介绍这个方程，以防你不完全清楚要如何做。

在一般情况下，假如我们有$m$个训练样本:

$$
(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})
$$

$n$个特征变量:

$$
x^{(i)} = 
\begin{bmatrix}
x\_{0}^{(i)} \\\\
x\_{1}^{(i)} \\\\
x\_{2}^{(i)} \\\\
... \\\\
x\_{n}^{(i)} 
\end{bmatrix}
\in
\mathbb{R}^{n+1}
$$

每一个训练样本$x^{(i)}$看起来像一个$n+1$维的特征向量。

我要构建矩阵$X$，也被称为**设计矩阵(design matrix)**。如下所示：

$$
X = 
\begin{bmatrix}
(x^{(1)})^{T} \\\\
(x^{(2)})^{T} \\\\
(x^{(3)})^{T} \\\\
... \\\\
(x^{(m)})^{T} \\\\
\end{bmatrix}
=
\begin{bmatrix}
x\_{0}^{(1)} & x\_{1}^{(1)} & x\_{2}^{(1)} & ... &x\_{n}^{(1)} \\\\
x\_{0}^{(2)} & x\_{1}^{(2)} & x\_{2}^{(2)} & ... &x\_{n}^{(2)} \\\\
x\_{0}^{(3)} & x\_{1}^{(3)} & x\_{2}^{(3)} & ... &x\_{n}^{(3)} \\\\
... \\\\
x\_{0}^{(m)} & x\_{1}^{(m)} & x\_{2}^{(m)} & ... &x\_{n}^{(m)} \\\\
\end{bmatrix}
$$

矩阵$X$是一个$m * (n+1)$维矩阵。

举个具体的例子：假如我只有一个特征变量(就是说除了$x\_{0}$之外只有一个特征变量，而$x\_{0}$始终为1)，比如说房屋的大小：

$$
x^{(i)} = 
\begin{bmatrix}
1 \\\\
x^{(i)}\_{1} 
\end{bmatrix}
$$

那么我的设计矩阵$X$会是这样：

$$
X = 
\begin{bmatrix}
1 & x^{(1)}\_{1}\\\\
1 & x^{(1)}\_{2}\\\\
...
1 & x^{(1)}\_{mysql}\\\\
\end{bmatrix}
$$

向量$y$代表所有训练集中正确的房屋价格，是这样的：

$$
y = 
\begin{bmatrix}
y^{1} \\\\
y^{2} \\\\
...
y^{m} \\\\
\end{bmatrix}
$$

最后，构建完矩阵$X$和向量$y$，我们就可以通过计算：

$$
θ = (X^{T}X)^{-1}X^{T}y
$$

来得到$θ$。

> 这就是通过正规方程来求解最优的$θ$值的过程，在数学上可以证明这个式子会给出最优的$θ$值，可以最小化代价函数$J(θ)$，但在这里我并不打算给出证明过程。

### 使用Octave来执行正规方程

$$
θ = (X^{T}X)^{-1}X^{T}y
$$

这个方程在Octave中的表达方式如下：

```
pinv(X`*X)*X`*y
```

其中X`代表X的转置，pinv是用来计算矩阵的逆的函数。

### 正规方程不需要对特征变量归一化

在之前视频中我提到**特征变量归一化**和让特征变量在相似的范围内的想法将所有的值归一化在类似范围内。**如果你使用正规方程法 那么就不需要归一化特征变量，实际上这是没问题的**。

例如：$x\_{1}$在0到1的区间，$x\_{2}$在0到1000的区间，$x\_{3}$在0到$10^{-5}$的区间，如果使用正规方程法，不需要做特征变量归一化也是没有问题的，**但如果你使用梯度下降法，特征变量归一化就很重要**。

### 梯度下降法VS正规方程

最后你何时应该使用梯度下降法，而何时应该使用正规方程法呢？这里列举了一些它们的优点和缺点：

假如你有$m$个训练样本和$n$个特征变量 :

|梯度下降算法|正规方程法|
|:-:|:-:|
| 需要选择一个学习速率$α$ | 不需要选择一个学习速率$α$|
| 需要多次迭代 | 不需要多次迭代 |
| 当$n$很大时也能良好运行 | 需要计算$(X^{T}X)^{-1}$ |
| | 当$n$很大时，运行会很慢(因为复杂度为:$O(n^3)$) |

> 那么在什么情况下会考虑使用梯度下降算法呢？ 通常$n<10000$的情况下，建议使用正规方程法，当$n>10000$时，建议使用梯度下降法。

随着我们要讲的学习算法越来越复杂，例如当我们讲到分类算法：像逻辑回归算法，我们会看到实际上对于那些问题，并不能使用正规方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题，因为标准方程法不适合用在它们上。但对于特征变量比较少的情况下，正规方程法是一个比梯度下降法更快的替代算法。所以这两算法都是值得学习的。

## *正规方程法的不可逆性

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/zSiE6/normal-equation-noninvertibility)

在这段视频中我想谈谈正规方程(normal equation)以及它们的不可逆性。由于这是一种较为深入的概念，并且总有人问我有关这方面的问题，因此我想在这里来讨论它。由于概念较为深入，所以对这段可选材料大家放轻松吧，也许你可能会深入地探索下去并且会觉得理解以后会非常有用，但即使你没有理解正规方程和线性回归的关系也没有关系。

首先，我们知道，并不是所有的矩阵都是可逆的，不可逆的矩阵被称为**奇异矩阵**或**退化矩阵**

那么当计算

$$
θ = (X^{T}X)^{-1}X^{T}y
$$

时，当$X^{T}X$不可逆时，怎么办呢？

问题的关键在于实际上$X^{T}X$不可逆的情况很少发生。

在Octave里，如果你用它来实现θ的计算 你将会得到一个正常的解 

> 在Octave里有两个函数可以求解矩阵的逆，一个是pinv()另一个是inv()。这两者之间的差异是些许计算过程上的，一个是所谓的伪逆，另一个被称为逆。使用pinv()函数可以展现数学上的过程，它可以在即便矩阵$X^{T}X$是不可逆的的情况下，计算出$θ$的值。另外在计算法方面，inv()引入了先进的数值计算的概念。

下面来看看$X^{T}X$不可逆的情况的处理：

关于矩阵$X^{T}X$的不可逆的问题，如果你懂一点线性代数或许你会感兴趣。我不会从数学的角度来证明它，但如果矩阵$X^{T}X$结果是不可逆的通常有两种最常见的原因：

- 第一个原因是，多余的特征（线性依赖）：

	例如：
	
	$$
	x\_{1} = size\ in\ feet^{2}
	$$
	
	$$
	x\_{2} = size\ in\ m^{2}
	$$
	
	在预测住房价格时,如果$x\_{1}$是以英尺为尺寸规格计算的房子，$x\_{2}$是以平方米为尺寸规格计算的房子，同时你也知道1米等于3.28英尺(四舍五入到两位小数)。这样你的这两个特征值将始终满足约束$x\_{1}=(3.28)^{2}$x\_{2}$，这种矩阵$X^{T}X$是不可逆的。
	
	
- 第二个原因是，过多的特征($m\leq{n}$)：

	在你想用大量的特征值尝试实践你的学习算法的时候，可能会导致矩阵$X^{T}X$的结果是不可逆的。具体地说，在$m\leq{n}$的时候不可逆。
	
	例如有$m=10$的训练样本，也有$n=100$的特征数量，仅仅通过10各训练样本来适应这100个特征向量，数据还是有些少。稍后我们将看到，如何使用小数据样本以得到这100或101个参数。通常我们会使用一种叫做**正则化**的线性代数方法通过删除某些特征，或者是使用某些技术来解决当$m\leq{n}$小的时候的问题，这也是在本节课后面要讲到的内容。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。
	
	有关正规化的内容将是本节之后课程的话题，总之当你发现的矩阵$X^{T}X$的结果是奇异矩阵(不可逆)，建议你：
	
	- 首先看特征值里是否有一些多余的特征，例如$x\_{1}$和$x\_{2}$是否线性相关，同时当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留,直到他们不再是多余的为止。
	- 其次如果特征里没有多余的，我会检查是否有过多的特征，如果特征数量实在太多，我会删除些用较少的特征来反映尽可能多内容
	
	否则我会考虑使用正规化方法，这也是我们将要谈论的话题。

总之出现不可逆矩阵的情况极少发生，所以在大多数实现线性回归中，出现不可逆的问题不应该过多的关注。