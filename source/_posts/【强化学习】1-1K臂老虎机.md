title: 【强化学习】1-1 K臂老虎机
tags:
  - 机器学习
  - 强化学习
categories:
  - 强化学习
comments: true
date: 2021-02-11 09:57:00
mathjax: true
---

在强化学习中，代理人通过与世界互动来生成自己的训练数据。 代理人必须通过试验和错误了解自己行为的后果，而不是被告知正确的行动。 

我们使用K臂老虎机（K-Armed Bandit）来描述强化学习中的基本概念，比如rewards、timesteps和values。

想象一下，有一名医生想要测量3种药物的效果：

<img src="/img/rl_21_02_11/01.png" height=200 />

医生会随机开始一次治疗，然后观测患者的反映情况。

过了一段时间后，医生发现其中某种药物似乎比其他药物效果要更好：

<img src="/img/rl_21_02_11/02.png" height=200 />

医生现在必须决定**是否坚持最佳治疗**或**继续进行随机研究**。如果医生只使用一种药物治疗， 那么他们就不能再收集另外两种药物治疗的数据（也许其他治疗方法之一实际上是更好的， 只是由于偶然而变得更糟）。 但如果其他两种治疗方法更糟， 那么继续研究会危及其他患者的健康。 

这种医疗试验的本质就是一个K臂老虎机的案例。

## K臂老虎机

在K臂老虎机问题中，我们有一个决策者（代理）负责在k个不同的行动之间进行选择，并根据他选择的行动获得奖励。

在**医疗试验**的例子中，代理人就是医生。医生必须在3种不同的治疗行动之间进行选择，选择不同的治疗会产生一些未知的回报，最后患者是否得到治疗就是医生获得的奖励。

<img src="/img/rl_21_02_11/03.png" height=200 />

### Action-Values

为了让医生决定哪个动作是最好的，我们必须定义每个动作的价值（Action-Values）。

我们对动作价值的定义是**奖励的期望**：

$$
q*(a) \doteq \mathbb{E}[R_t|A_t=a] 
\ \ \ \  \forall a \in \{1, ..., k\}
$$

> 说明：`$\doteq$` 符号的意思是 **定义为**
> 上面公式的含义是：$q*(a)$被定义为$R_t$的期望，$R_t$是我们在选中某个动作的时候的奖惩值。

$$
q*(a) = \sum_r p(r|a) r
$$

这个条件期望被定义为所有可能的奖励的总和。在该总和中，我们将可能的奖励（$r$）乘以观察到该奖励的概率（$p(r|a)$）。

代理（agent）的目标就是**最大限度的提高预期的回报**。

代理选择最大的value的操作的过程称为argmax:

<img src="/img/rl_21_02_11/04.png" height=40 />

### 连续型的reward

在上面的例子中，我们把患者是否治愈作为reward，下面我们用一个更容易测量的指标来衡量患者的状态：**接受治疗后血压的变化**。

在每种不同的药物治疗之后，患者的血压会呈现出不同的概率分布，也许是伯努利分布（结果1）、二项式分布（结果2）、均值分布中的某一种（结果3）：

<img src="/img/rl_21_02_11/05.png" height=300 />

$q*(a)$是a动作的reward的均值

## 为什么要研究老虎机问题

现实生活中，我们随时都需要做出各种各样的决策，除了医生对患者选择最佳药物的例子外，我们决定去看什么电影、听什么歌曲、即便是在餐厅点菜 我们都在做着类似的选择。

老虎机是对未知环境下动作选择问题的简单抽象，在这种简单的问题下去思考算法的设计会使问题更加清晰。