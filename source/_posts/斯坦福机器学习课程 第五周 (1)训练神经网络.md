title: 斯坦福机器学习课程 第五周 (1)训练神经网络
tags:
  - 机器学习
  - 斯坦福课程
  - 神经网络
categories:
  - 机器学习
comments: true
date: 2016-09-14 01:14:00
mathjax: true
---

## 代价函数

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/na28E/cost-function)

> 在本节课中和后面的几节课中，我将开始讲述一种在给定训练集下为神经网络拟合参数的学习算法。正如我们讨论大多数学习算法一样，我们准备从拟合神经网络参数的代价函数开始讲起。

以神经网络在分类问题中的应用为例：

![](/img/16_09_18/001.png) 

假设我们有一个如上图所示的神经网络结构，然后假设我们有一个这样的训练集：

$$
\begin{aligned}
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})
\end{aligned}
$$

一共有m个训练样本$(x^{(i)},y^{(i)})$。

用大写字母$L$表示这个神经网络结构的总层数：

$$
L = total\ no.\ of\ layers\ in\ network(神经网络总层数)
$$

所以，对于左边的网络结构，我们得到$L=4$。

然后，我们准备用$S\_{l}$表示第$l$层的神经元的数量，**这其中不包括L层的偏置单元**：

$$
S\_{l}= no.\ of\ units(not\ counting\ bias\ unit)\ in\ layer\ l
$$

比如说：

- $S\_{1}=3$(也就是输入层)
- $S\_{2}=5$
- $S\_{3}=5$
- $S\_{4}=S\_{L}=4$ (因为$L=4$)

我们接下来讨论两种分类问题，分别是**二元分类**和**多类别分类**。

### 二元分类(Binary classification)

在二元分类中$y$只能是0或者1：

$$
y= 0\ or\ 1
$$

在这个例子中，我们有一个输出单元（不同于上面的神经网络有4个输出单元）。神经网络的输出会是一个实数：

$$
h\_{Θ}(x)\in \mathbb{R}
$$

输出单元的个数:

$$
S\_{L} = 1 
$$

> 在这类问题里，为了简化记法，我会把$K$设为1，这样**你可以把$K$看作输出层的单元数目**。

### 多类别分类（Multi-class classification）

在多类别分类问题中，会有$K$个不同的类，比如说如果我们有四类的话，我们就用下面这种表达形式来代表$y$。在这类问题里，我们就会有$K$个输出单元。

![](/img/16_09_18/002.png) 

我们的输出假设就是一个$K$维向量：

$$
h\_{Θ}(x)\in \mathbb{R}^{K}
$$

输出单元的个数就是$K$：

$$
S\_{L} = K
$$

> 通常这类问题中，我们都有$K\ge3$，因为如果我们只有两类的话，我们直接使用二元分类法就可以了。因此只有在$K\ge3$的情况下，我们才会使用这种**多类别分类**。

### 定义代价函数

我们在神经网络里，使用的代价函数，应该是逻辑回归里使用的代价函数的一般形式。

**逻辑回归的代价函数:**

$$
\begin{equation\*}
J(\theta)=-\frac{1}{m}
[
\sum\_{i=1}^m
y^{(i)}logh\_{\theta}(x^{(i)})
+(1-y^{(i)})log(1-h\_{\theta}(x^{(i)}))
]+
\frac{\lambda}{2m}
\sum\_{j=1}^n
\theta^{2}\_{j}
\end{equation*}
$$

其中$\begin{equation\*}
\frac{\lambda}{2m}
\sum\_{j=1}^n
\theta^{2}\_{j}
\end{equation*}$这一项是个额外的正则化项，是一个$j$从1到$n$的求和形式。因为我们并没有把偏置项0正则化。

对于神经网络，我们使用的代价函数是这个式子的一般化形式。

$$
\begin{equation\*}
J(Θ)=-\frac{1}{m}
[
\sum\_{i=1}^m
\sum\_{k=1}^K
y^{(i)}\_{k}log(h\_{Θ}(x^{(i)}))\_{k}
+(1-y^{(i)})\_{k})log(1-(h\_{Θ}(x^{(i)}))\_{k})
]+
\frac{\lambda}{2m}
\sum\_{l=1}^{L-1}
\sum\_{i=1}^{S\_{l}}
\sum\_{j=1}^{S\_{l+1}}
(Θ^{(l)}\_{ji})^{2}
\end{equation*}
$$

神经网络现在输出了在$K$维的向量$h\_{Θ}(x)$：

$$
h\_{Θ}(x)\in \mathbb{R}^{K}
$$

用$(h\_{Θ}(x))\_{i}$来表示第$i$个输出。

其中$\begin{equation\*}\sum\_{k=1}^K\end{equation*}$这个求和项是$K$个输出单元的求和，比如你有4个输出单元在神经网络的最后一层，那么这个求和项就是$k$从1到4所对应的每一个逻辑回归算法的代价函数之和。

最后式子中的这一项$\begin{equation\*}
\frac{\lambda}{2m}
\sum\_{l=1}^{L-1}
\sum\_{i=1}^{S\_{l}}
\sum\_{j=1}^{S\_{l+1}}
(Θ^{(l)}\_{ji})^{2}
\end{equation*}$类似于我们在逻辑回归里所用的正则化项，这个求和项看起来确实非常复杂，它所做的就是把这些项全部加起来，也就是对所有的$Θ\_{ji}^{(l)}$的值都相加。正如我们在逻辑回归里的一样，这里要除去那些对应于偏差值的项。具体来说，我们不把$Θ\_{j0}^{(l)}$这些项加进去，这是因为当我们计算神经元的激励值时，我们会有这些项。这些带0的项，类似于偏置单元的项。类比于我们在做逻辑回归的时候，我们就不应该把这些项加入到正规化项里去，因为我们并不想正规化这些项，并把这些项设定为0。 （这里我表示没看懂）

即使我们真的把他们加进去了，也就是说$i$从0加到$S\_{l}$依然成立，并且不会有大的差异，但是这个“不把偏差项正规化”的规定可能只是会更常见一些。

## 反向传播(B-P)

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm)

> 这个视频中，我们来讨论一下让代价函数最小化的算法，具体来说，我们将主要讲解**反向传播算法（BP算法）**

下面是我们上一节写好的代价函数：

![](/img/16_09_18/003.png) 

我们要做的就是试图找到使得代价函数$J(Θ)$最小的$Θ$值：

![](/img/16_09_18/004.png) 

为了使用梯度下降法，我们需要做的就是写好一个通过输入参数$Θ$，然后计算：

![](/img/16_09_18/005.png) 

这一节的大部分内容也都是在讲解如何计算真两项的。

---

**梯度下降计算**

![](/img/16_09_18/006.png) 

首先，我们从只有一个训练样本的情况说起，假设我们整个训练集只包含一个训练样本：

$$
(x,y)
$$

让我们粗看一下，使用这样一个训练样本来计算的顺序。

首先我们用向前传播方法来计算一下在给定输入的时候，假设函数的输出结果：

$$
a^{(1)} = x
$$

> $a^{(1)}$就是第一层的激励值，也就是输入层

$$
z^{(2)} = Θ^{(1)}a^{(1)}
$$

$$
a^{(2)} = g(z^{(2)})
$$

> 这里记得添加偏差项$a\_{0}^{(2)}$

$$
z^{(3)} = Θ^{(2)}a^{(2)}
$$

$$
a^{(3)} = g(z^{(3)})
$$

> 这里记得添加偏差项$a\_{0}^{(3)}$

$$
z^{(4)} = Θ^{(3)}a^{(3)}
$$

$$
a^{(4)} = h\_{Θ}(x) = g(z^{(4)})
$$

通过上面的步骤计算，我们就可以得出假设函数的输出结果了：

![](/img/16_09_18/007.png) 

接下来，为了计算导数项，我们将采用一种叫做**反向传播(Backpropagation)**的算法。

反向传播算法从直观上说就是对每一个节点求下面这一个误差项:

![](/img/16_09_18/008.png) 

> $δ\_{j}^{(l)}$这种形式代表了第$l$层的第$j$个结点的**误差**
> 
> 我们还记得我们使用$a\_{j}^{(l)}$来表示第$l$层的第$j$个结点的**激励值**，所以这个$δ$项，在某种程度上就捕捉到了我们在这个神经结点的激励值的误差。所以我们可能希望这个结点的激励值稍微不一样。

具体来讲，我们用上面的那个四层的神经网络结构做例子：

每一项的输出单元(layer L = 4)

$$
δ\_{j}^{(4)} = a\_{j}^{(4)} - y\_j
$$

> 对于每一个输出单元，我们准备计算$δ$项，所以第四层的第$j$个单元的$δ$就等于**这个单元的激励值减去训练样本里的真实值**。所以$a\_{j}^{(4)}$这一项同样可以写成$h\_{Θ}(x)\_{j}$：

$$
δ\_{j}^{(4)} = h\_{Θ}(x)\_{j} - y\_j
$$

顺便说一下，如果你把$δ$、$a$和$y$这三项都看作向量的话，那么上面的式子你也可以写出向量化的实现：

$$
δ^{(4)} = a^{(4)} - y
$$

这里的$δ^{(4)}$、$a^{(4)}$和$y$都是一个向量，并且向量维数等于输出单元的数目。

所以现在我们计算出网络结构的误差项$δ^{(4)}$，我们下一步就是计算网络中前面几层的误差项$δ$。

这就是$δ^{(3)}$的计算公式：

$$
δ^{(3)} = (Θ^{(3)})^{T}δ^{(4)}.*g'(z^{(3)})
$$

> 这里的点乘$.*$是我们从MATLAB里知道的对y元素的乘法操作，指的是两个向量中元素间对应相乘。

其中$g'(z^{(3)})$这一项其实是对激励函数$g$在输入值为$z(3)$的时候所求的导数。

如果你稍微会一些微积分的知识，你可以很容易的求得$g'(z^{(3)})$这一项的值是：

$$
a^{(3)}.*(1-a^{(3)})
$$

这里的$1$是元素都为1的向量。

接下来，你可以应用一个相似的公式来求得$δ^{(2)}$:

$$
δ^{(2)} = (Θ^{(2)})^{T}δ^{(3)}.*g'(z^{(2)})
$$

值得注意的是，这里我们没有$δ^{(1)}$项，因为第一层是输入层，不存在误差。所以这个例子中，我们的$δ$项就只有第2层和第3层。

---

反向传播法这个名字源于我们从输出层开始计算$δ$项，然后我们返回到上一层计算第三隐藏层的$δ$项，接着我们再往前一步来计算$δ^{(2)}$。

所以说我们是类似于把输出层的误差反向传播给了第3层，然后再传到第二层。这就是反向传播的意思。

最后，这个推导过程是出奇的复杂，但是如果你按照这样几个步骤来计算，就有可能简单直接地完成复杂的数学证明。

如果你忽略标准化所产生的项，我们可以证明我们想要的偏导项，恰好就是下面这个表达式：

$$
\frac{\partial}{\partial Θ\_{ij}^{(l)}}J(Θ)=
a\_{j}^{(l)}δ\_{i}^{(l+1)}
$$

$$
(ignore\ λ; if\ λ = 0)
$$

> 这里我们忽略了$λ$，我们将在之后完善这一个关于正则化项。

所以到现在，我们通过反向传播计算这些$δ$项，可以非常快速的计算出所有参数的偏导数项。

----

好，现在让我们把上面所讲的所有内容整合在一起，然后说说如何实现反向传播算法：

当我们有一个很大的训练样本的时候，而不是像我们例子里这样的一个训练样本。我们是这样做的：

假设我们有m个样本的训练集：

$Training\ set\ {(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})}$

我们要做的第一件事就是设置这些值：

![](/img/16_09_18/009.png) 

> 这里的$△$其实是大写的$δ$，实际上这些$△\_{ij}^{(l)}$将被用来计算偏导数项$\frac{\partial}{\partial Θ\_{ij}^{(l)}}J(Θ)$
>
> 所以，正如我们接下来看到的，这些$△\_{ij}^{(l)}$将被作为累加项，慢慢地增加，以算出这些偏导数。

下图是我们接下来要执行的一些操作：

![](/img/16_09_18/010.png) 

在这里我们将遍历我们的训练集。

我们要做的第一件事就是设定$a^{(1)}$，也就是输入层的激励函数：$a^{(1)} = x^{(i)}$

接下来我们运用正向传播，来计算第2，3，4，...，L层的激励值$a^{l}$。

接下来，我们将用$y^{(i)}$来计算$δ^{(L)}=a^{(L)}-y^{(i)}$

接下来，我们使用反向传播算法来计算$δ^{(L-1)},δ^{(L-2)},...,δ^{(2)}$

最终，我们将用$△\{ij}^{(l)}$来积累我们在前面写好的偏导数项：

$$
△\{ij}^{(l)}:=△\{ij}^{(l)} + a\_{j}^{(l)}δ\_{i}^{(l+1)}
$$

如果你再看一下上面这个表达式，你可以把它写成向量形式：

具体来说，如果你把$△$看做一个矩阵，$ij$代表矩阵中的位置，那么上面的式子我们就可以写成：

$$
△^{(l)}:=△^{(l)} + δ^{(l+1)}(a^{(l)})^{T}
$$

最后，执行这个for循环体之后我们挑出这个for循环，然后计算下面这些式子：

$$
D\_{ij}^{(l)} := \frac{1}{m}△\{ij}^{(l)} + λΘ\_{ij}^{(l)}	
\ \ \ \ if\ j ≠ 0
$$

$$
D\_{ij}^{(l)} := \frac{1}{m}△\{ij}^{(l)}
\ \ \ \ if\ j = 0
$$

> 这里我们对$j ≠ 0$ 和 $j = 0$分两种情况来讨论，在$j=0$的情况下对应的是偏差项，所以这也是为什么在$j=0$的情况下没有写额外的标准化项的原因。

最后，尽管严格的证明对于你来说太复杂，你现在可以说明的是一旦你计算出来了这些，这就正好是代价函数关于每一个参数的偏导数：

$$
\frac{\partial}{\partial Θ\_{ij}^{(l)}}J(Θ)
= D\_{ij}^{(l)}
$$

所以，你可以把它用在梯度下降算法，或者其他更高级的算法中。

## 反向传播算法的直观介绍

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/du981/backpropagation-intuition)

> 这一节中，将要更加深入的讨论一下反向传播算法的这些复杂的步骤，并且希望给你一个更加直观的感受，理解这些步骤究竟是在做什么。也希望通过这一节，你能理解它至少还是一个合理的算法。
>
> 但可能你即使看了这段视频你还是觉得反向传播依然很复杂，这也没关系，其实即使是我（吴恩达）接触了反向传播这么多年了，有时候任然觉得这是一个难以理解的算法，但还是希望这段视频能有些许帮助。

### 距离说明神经网络计算过程

为了更好地理解反向传播算法，我们再来仔细研究一下向前传播的原理。

![](/img/16_09_18/011.png)

这里有一个包含两个输入单元（不包括偏差单元）的神经网络，在第二层有两个隐藏单元（不包括偏差单元），第三层也有两个隐藏单元（不包括偏差单元），最后的输出层有一个输出单元。

### 向前传播

为了更清楚的展示向前传播，下图展示了这个神经网络的**向前传播**的运算过程：

![](/img/16_09_18/012.png)

事实上，**反向传播**算法的运算过程非常类似于此，只有计算的方向不同而已。

### 代价函数

为了更好的理解反向传播算法的原理，我们把目光转向代价函数：

![](/img/16_09_18/013.png)

这个代价函数对应的情况是只有一个输出单元，如果我们有不止一个输出单元的话，只需要对所有的输出单元进行一次求和运算。

请注意这组训练样本$x^{(i)}$,$y^{(i)}$，注意这种只有一个输出单元的情况，如果不考虑正则化即$λ=0$，因此最后的正则化项就没有了。

![](/img/16_09_18/014.png)

这个求和运算括号里面与第i个训练样本对应的代价项，也就是说$(x^{(i)},y^{(i)})$对应的代价项，将有下面这个式子决定：

$$
cost(i) = y^{(i)}log h\_{Θ}(x^{(i)}) + (1 - y^{(i)})logh\_{Θ}(x^{(i)})
$$

而这个代价函数所扮演的角色可以看做是平方误差，当然，如果你愿意，你可以把$cost(i)$想象成：

$$
cost(i)≈(h\_{Θ}(x^{(i)})-y^{(i)})^{2}
$$

因此，这里的$cost(i)$表征了该神经网络是否能准确地预测样本i的值，也就是输出值，和实际观测值$y^{(i)}$的接近程度。

### 反向传播

现在我们来看看反向传播是怎么做的。

一种直观的理解是反向传播算法就是在计算所有这些$δ$项：

$$
δ\_{j}^{(l)} = "error" \ of \ cost \ for \ a\_{j}^{(l)} \ (unit \ j \ in \ layer \ l).
$$

并且我们可以把它们看作是这些激励值的“**误差**”(注意这些激励值是第l层中的第j项)。

更正式一点的说法是$δ$项实际上是关于$z\_{j}^{(l)}$的偏微分，也就是cost函数关于我们计算出的输入项的加权和，也就是$z$项的偏微分:

$$
δ\_{j}^{(l)} = \frac{\partial}{\partial z\_{j}^{(l)}}cost(i) \ \ \ \ (for \ j \ge 0 )
$$

其中：

$$
cost(i) = y^{(i)}log h\_{Θ}(x^{(i)}) + (1 - y^{(i)})logh\_{Θ}(x^{(i)})
$$

如果我们观察该神经网络内部的话，把这些$z\_{j}^{(l)}$项稍微改一点点，那就将影响到神经网络的输出，并且最终会改变代价函数的值。

因此，它们度量着我们对神经网络的权值做多少的改变，对中间的计算量影响是多少，进一步对整个神经网络的输出$h(x)$影响多少，以及对整个的代价影响多少。

可能刚才讲的偏微分的这种理解不太容易理解，没关系，不用偏微分的思想，我们同样也可以理解。

我们再深入一点，研究一下反向传播的过程，对于输入层，如果我们设置$δ$项，假设我们进行第i个训练样本，那么：

$$
δ\_{1}^{(4)}=y^{(i)}-a^{(4)}\_{1}
$$

接下来我们要对这些值进行反向传播，算出$δ\_{1}^{(3)}$、$δ\_{2}^{(3)}$，然后同样的再进行下一层的反向传播，算出$δ\_{1}^{(2)}$、$δ\_{2}^{(2)}$。

举个例子:

接下来，我们来看看如何计算$δ\_{2}^{(2)}$。

我要对一些权值进行标记:

![](/img/16_09_18/016.png) 

实际上，我们要做的是我们要用下一层的$δ$值和权值相乘，然后加上另一个$δ$值和权值相乘的结果。也就是说，它其实是$δ$值的加权和。权值是这些对应边的强度。

![](/img/16_09_18/017.png) 

计算过程是：

$$
δ\_{2}^{(2)}=Θ\_{12}^{(2)}δ\_{1}^{(3)} + Θ\_{22}^{(2)}δ\_{2}^{(3)}
$$

再看看另一个例子：

如果想要计算$δ\_{2}^{(3)}$的值，计算过程也是类似的：

$$
δ\_{2}^{(3)}=Θ\_{12}^{(3)}δ\_{1}^{(4)}
$$

另外顺便提一下，目前为止我写的$δ$值仅仅是隐藏层中的没有包括偏差单元:"+1"的。包不包括偏差单元取决于你如何实现这个反向传播算法，你也可以对这些偏差单元计算$δ$的值，这些偏差单元总是取为"+1"的值。

通常来说，我在执行反向传播的时候，我是算出了这些偏差单元的$δ$值，但我通常忽略掉它们，而不是把它们带入计算，因为它们其实并不是计算那些微积分的必要部分，