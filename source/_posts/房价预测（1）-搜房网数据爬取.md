title: 房价预测（1）-搜房网数据爬取
tags:
  - 机器学习
categories:
  - 机器学习
comments: true
date: 2016-11-30 22:21:58
---

## 搜房网数据爬取

> 如今**房事**牵动着每家每户的心，所以以搜房网数据为例，用程序员的角度去客观分析一下房价走势，并对房价数据进行预测，也顺便对学到的**机器学习**相关知识练练手。

### 爬虫Scrapy

数据分析的第一步，当然是数据的采集。这里我使用的是[Scrpy](http://scrapy-chs.readthedocs.io/zh_CN/latest/)。这是一个开源的python爬虫框架，可以很方便的处理很多爬虫方面的工作，而且中文的文档写的也比较明确。

安装好python环境后，通过pip即可安装：

```
pip install Scrapy
```

具体使用方式，可以看一下这个[Scrapy入门教程](http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html)，也不难，这里就不做过多的介绍了。

### fang.com从何处入手呢？

大致了解了Scrapy的用法之后，我们就可以实际操作一把了。那么我们面对搜房网这个庞然大物到底要从何处开始爬呢？

首先，先声明一下我的目的：目的是分析房价数据，所以要爬取的数据主要是每个房源信息的数据咯。

想要拿到每条房源信息的url，通常我们是拿到每个列表页的数据。

那么我们第一个要爬取的数据就是房源列表信息了。

#### 列表爬取

所谓列表页，就是这个页面：[上海二手房](http://esf.sh.fang.com/housing/)，下面“全部小区”部分的列表信息就是我们想要的。

通过观察搜房网，我发现还有另外一个列表页入口，看起来似乎更容易爬取一些，就是：[地图页](http://esf.sh.fang.com/map/)。

在这个页面左侧有个列表：

![](/img/16_11_30/001.png)

进一步点击这个列表，最后我们可以得到这个页面：

![](/img/16_11_30/002.png)

通过抓包工具，我们可以看到这个列表通过Ajax发送get请求，得到一段json数据：

![](/img/16_11_30/003.png)

我们真正想要的数据在这段json中最后的`item`中：

![](/img/16_11_30/004.png)

这里藏的就是这个列表的html代码。

回过头来，再来看看我们请求的地址：

```
http://esf.sh.fang.com/map/?mapmode=y&district=996&subwayline=&subwaystation=&price=&room=&area=&towards=&floor=&hage=&equipment=&keyword=&comarea=21929&orderby=30&isyouhui=&x1=120.798&y1=30.926054&x2=122.177798&y2=31.571157&newCode=&houseNum=&schoolDist=&schoolid=&ecshop=ecshophouse&PageNo=1&zoom=16&a=ajaxSearch&city=sh&searchtype=loupan
```

这里需要凭借直觉来剔除掉一些多余的参数的，至少满足请求到的数据不是局限于**崇明区**的，而是全上海的。

经过一些尝试之后，我发现下面这个url可以请求到全部的列表数据，其中`PageNo`参数是页数：

```
http://esf.sh.fang.com/map/?mapmode=y&orderby=30&ecshop=ecshophouse&PageNo=2&a=ajaxSearch&city=sh&searchtype=loupan
```

我们可以先通过

```
scrapy shell "http://esf.sh.fang.com/map/?mapmode=y&orderby=30&ecshop=ecshophouse&PageNo=2&a=ajaxSearch&city=sh&searchtype=loupan"

```

来找出房源详情页的url。

最终，我发现通过`response.xpath("//a/@href").extract()`可以提取出所有的链接地址，其中如果url中包含`"esf"`字符串，是二手房房源的url地址。所以`parse()`的代码如下：

```
def parse(self, response):

        infos = response.xpath("//a/@href").extract()

        for i in infos:
            i_str = str(i).encode("utf-8")
            if "esf" in i_str:
                url = i_str.replace('\\', '').strip()

```

这样我们就得到了所有二手房房源详情页面的url了。

> 这里有一个小坑：
> 
> 这一行代码`i_str = str(i).encode("utf-8")`中，由于i默认没有解码成utf-8的格式，所以不能直接和字符串`"esf"`进行运算，所以需要一部encode操作。

#### 详情页爬取

对于详情页，我主要想爬取的数据如下：

![](/img/16_11_30/005.png)

这个页面似乎没有什么取巧的方式了，只能硬着头皮去看他的源码，一层层的抽取了。下面是详情页解析的代码：

```
def parse_details(self, response):

        # path
        xpath = "//body/div[@class='wrap']/div[@class='main clearfix']/div[@class='mainBoxL']"
        div_title = "/div[@class='title']"
        p_gray9 = "/p[@class='gray9']"
        h1 = "/h1"
        span_mr10 = "/span[@class='mr10']"
        div_houseInfor_clearfix = "/div[@class='houseInfor clearfix']"
        div_inforTxt = "/div[@class='inforTxt']"
        dl = "/dl"
        dt_gray6_zongjia1 = "/dt[@class='gray6 zongjia1']"
        span_red20b = "/span[@class='red20b']"
        dd_gray6 = "/dd[@class='gray6']"
        dd = "/dd"
        dt = "/dt"

        item = ESFItem()
        item['id'] = response.xpath(xpath +
                                    div_title +
                                    p_gray9 +
                                    span_mr10).extract_first().strip()

        item['publish_time'] = response.xpath(xpath +
                                              div_title +
                                              p_gray9).extract_first().strip()

        item['title'] = response.xpath(xpath +
                                       div_title +
                                       h1).extract_first().strip()

        item['total_price'] = response.xpath(xpath +
                                             div_houseInfor_clearfix +
                                             div_inforTxt +
                                             dl +
                                             dt_gray6_zongjia1 +
                                             span_red20b).extract_first().strip()

        dd_infos = response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dd).extract()
        huxing_str = "<span class=\"gray6\">户<span class=\"padl27\"></span>型："
        jzmj_str = "<dd class=\"gray6\">建筑面积：<span class=\"black \">"
        symj_str = "<dd class=\"gray6\">使用面积：<span class=\"black \">"
        nd_str = "<span class=\"gray6\">年<span class=\"padl27\"></span>代：</span>"
        cx_str = "<span class=\"gray6\">朝<span class=\"padl27\"></span>向：</span>"
        lc_str = "<span class=\"gray6\">楼<span class=\"padl27\"></span>层：</span>"
        jg_str = "<span class=\"gray6 \">结<span class=\"padl27\"></span>构：</span>"
        zx_str = "<span class=\"gray6\">装<span class=\"padl27\"></span>修：</span>"
        zzlb_str = "<span class=\"gray6\">住宅类别：</span>"
        jzlb_str = "<span class=\"gray6\">建筑类别：</span>"
        cqxz_str = "<span class=\"gray6 \">产权性质：</span>"

        for i in dd_infos:
            if huxing_str in str(i).encode("utf-8"):
                item['house_type'] = i.strip()

            elif jzmj_str in str(i).encode("utf-8"):
                item['house_build_area'] = i.strip()

            elif symj_str in str(i).encode("utf-8"):
                item['house_use_area'] = i.strip()

            elif nd_str in str(i).encode("utf-8"):
                item['house_age'] = i.strip()
               
            elif cx_str in str(i).encode("utf-8"):
                item['orientation'] = i.strip()
                
            elif lc_str in str(i).encode("utf-8"):
                item['floor'] = i.strip()
                
            elif jg_str in str(i).encode("utf-8"):
                item['structure'] = i.strip()
                
            elif zx_str in str(i).encode("utf-8"):
                item['decoration'] = i
                
            elif zzlb_str in str(i).encode("utf-8"):
                item['residential_category'] = i.strip()
                
            elif jzlb_str in str(i).encode("utf-8"):
                item['building_class'] = i.strip()
                
            elif cqxz_str in str(i).encode("utf-8"):
                item['property_right'] = i.strip()
               

        dt_infos = response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dt + "/a").extract()
        for i in dt_infos:
            if "查看此楼盘的更多二手房房源" in str(i).encode("utf-8"):
                item['property_name'] = i.strip()
                
            elif "<span class=\"gray6 floatl\">学<span class=\"padl27\"></span>校：</span>" in str(i).encode("utf-8"):
                item['school'] = i.strip()

```

这样我们就可以得到详情页的信息了。

接下来，把上面两个爬取的操作链接在一起：

```
 def parse(self, response):
        infos = response.xpath("//a/@href").extract()

        for i in infos:
            i_str = str(i).encode("utf-8")
            if "esf" in i_str:
                url = i_str.replace('\\', '').strip()
                
                # 执行下一个request，回去自动调用callback函数，去解析详情页
                yield scrapy.Request(url=url.replace("\"", ""), callback=self.parse_details)
```

#### 设置起始url

由于要爬取的不仅仅是一页数据，所以我们的`start_urls`列表不能只有一个url，我们需要有一个`PageNo`自增长的url的list。

直接上代码：

```
base_url = "http://esf.sh.fang.com/map/?mapmode=y&orderby=30&ecshop=ecshophouse&PageNo=$&a=ajaxSearch&city=sh&searchtype=loupan"
    start_urls = []
    # 可以把10设置成一个比较大的数字，由于测试使用，所以写死一个较小的数
    for i in range(2, 10):
        start_urls.append(str(base_url).replace("$", str(i)))
```

#### 可以开始了吗？

列表爬取和详情爬取，以及起始url都设置ok了，那么我们是不是可以让我们的蜘蛛行动起来了呢？

且慢，这里也有坑。

这里我将页面数设置了200页，开始爬取，结果悲剧就发生了，由于短时间内访问量过大，搜房网的二手房页面似乎把我的ip屏蔽了：

![](/img/16_11_30/006.png)

这里我参考了[这篇文章](http://www.tuicool.com/articles/VRfQR3U)，通过

- 设置随机UserAgent
- 添加代理IP
- 禁用cookies
- 设置加载延迟

这些方式来防止被屏蔽。

设置完毕后，就可以愉快的开始爬取了~

#### 蜘蛛代码

最终蜘蛛的代码如下：

```
# coding=utf-8
import random

import scrapy
from ..items import ESFItem

import sys

reload(sys)
sys.setdefaultencoding('utf-8')


class ESFListSpider(scrapy.Spider):
    name = "esflist"

    # allowed_domains = ["fang.com"]

    base_url = "http://esf.sh.fang.com/map/?mapmode=y&orderby=30&ecshop=ecshophouse&PageNo=$&a=ajaxSearch&city=sh&searchtype=loupan"
    start_urls = []
    for i in range(2, 4):
        start_urls.append(str(base_url).replace("$", str(i)))

    def __init__(self, user_agent=''):
        self.user_agent = user_agent

    def process_request(self, request, spider):
        # 这句话用于随机选择user-agent
        ua = random.choice(self.user_agent_list)
        if ua:
            request.headers.setdefault('User-Agent', ua)

    user_agent_list = [
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1" \
        "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11", \
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6", \
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6", \
        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1", \
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5", \
        "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5", \
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3", \
        "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3", \
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3", \
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3", \
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3", \
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3", \
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3", \
        "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3", \
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3", \
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24", \
        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
    ]

    def parse(self, response):

        infos = response.xpath("//a/@href").extract()

        for i in infos:
            i_str = str(i).encode("utf-8")
            if "esf" in i_str:
                url = i_str.replace('\\', '').strip()

                yield scrapy.Request(url=url.replace("\"", ""), callback=self.parse_details)

    def parse_details(self, response):
        print "+++++++++++++被执行了+++++++++++++++++++"

        # path
        xpath = "//body/div[@class='wrap']/div[@class='main clearfix']/div[@class='mainBoxL']"
        div_title = "/div[@class='title']"
        p_gray9 = "/p[@class='gray9']"
        h1 = "/h1"
        span_mr10 = "/span[@class='mr10']"
        div_houseInfor_clearfix = "/div[@class='houseInfor clearfix']"
        div_inforTxt = "/div[@class='inforTxt']"
        dl = "/dl"
        dt_gray6_zongjia1 = "/dt[@class='gray6 zongjia1']"
        span_red20b = "/span[@class='red20b']"
        dd_gray6 = "/dd[@class='gray6']"
        dd = "/dd"
        dt = "/dt"

        item = ESFItem()
        item['id'] = response.xpath(xpath +
                                    div_title +
                                    p_gray9 +
                                    span_mr10).extract_first().strip()

        item['publish_time'] = response.xpath(xpath +
                                              div_title +
                                              p_gray9).extract_first().strip()

        item['title'] = response.xpath(xpath +
                                       div_title +
                                       h1).extract_first().strip()

        item['total_price'] = response.xpath(xpath +
                                             div_houseInfor_clearfix +
                                             div_inforTxt +
                                             dl +
                                             dt_gray6_zongjia1 +
                                             span_red20b).extract_first().strip()

        dd_infos = response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dd).extract()
        huxing_str = "<span class=\"gray6\">户<span class=\"padl27\"></span>型："
        jzmj_str = "<dd class=\"gray6\">建筑面积：<span class=\"black \">"
        symj_str = "<dd class=\"gray6\">使用面积：<span class=\"black \">"
        nd_str = "<span class=\"gray6\">年<span class=\"padl27\"></span>代：</span>"
        cx_str = "<span class=\"gray6\">朝<span class=\"padl27\"></span>向：</span>"
        lc_str = "<span class=\"gray6\">楼<span class=\"padl27\"></span>层：</span>"
        jg_str = "<span class=\"gray6 \">结<span class=\"padl27\"></span>构：</span>"
        zx_str = "<span class=\"gray6\">装<span class=\"padl27\"></span>修：</span>"
        zzlb_str = "<span class=\"gray6\">住宅类别：</span>"
        jzlb_str = "<span class=\"gray6\">建筑类别：</span>"
        cqxz_str = "<span class=\"gray6 \">产权性质：</span>"

        for i in dd_infos:
            if huxing_str in str(i).encode("utf-8"):
                item['house_type'] = i.strip()
            elif jzmj_str in str(i).encode("utf-8"):
                item['house_build_area'] = i.strip()
            elif symj_str in str(i).encode("utf-8"):
                item['house_use_area'] = i.strip()
            elif nd_str in str(i).encode("utf-8"):
                item['house_age'] = i.strip()
            elif cx_str in str(i).encode("utf-8"):
                item['orientation'] = i.strip()
            elif lc_str in str(i).encode("utf-8"):
                item['floor'] = i.strip()
            elif jg_str in str(i).encode("utf-8"):
                item['structure'] = i.strip()
            elif zx_str in str(i).encode("utf-8"):
                item['decoration'] = i
            elif zzlb_str in str(i).encode("utf-8"):
                item['residential_category'] = i.strip()
            elif jzlb_str in str(i).encode("utf-8"):
                item['building_class'] = i.strip()
            elif cqxz_str in str(i).encode("utf-8"):
                item['property_right'] = i.strip()

        dt_infos = response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dt + "/a").extract()
        for i in dt_infos:
            if "查看此楼盘的更多二手房房源" in str(i).encode("utf-8"):
                item['property_name'] = i.strip()
            elif "<span class=\"gray6 floatl\">学<span class=\"padl27\"></span>校：</span>" in str(i).encode("utf-8"):
                item['school'] = i.strip()

```

### what`s next?

接下来我会将爬取到的数据进行加工后持久化到本地，不过今天就先到这里吧~
