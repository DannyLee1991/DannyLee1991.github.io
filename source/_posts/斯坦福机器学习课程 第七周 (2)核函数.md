title: 斯坦福机器学习课程 第七周 (2)核函数
tags:
  - 机器学习
  - 斯坦福课程
categories:
  - 机器学习
comments: true
date: 2017-02-09 21:07:58
mathjax: true
---

## 核函数 I

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/YOMHn/kernels-i)

在本节，我将对支持向量机算法做一些改变，以构造复杂的非线性分类器。我们用**"kernels(核函数)"**来达到此目的。

我们来看看**核函数**是什么，以及如何使用。

如果你有一个像这个样的训练集：

![](/img/17_02_09/001.png)

然后你希望拟合一个非线性的判别边界来区别正负样本，那么你的判别边界可能是这样的：

![](/img/17_02_09/002.png)

当我们这么做的时候，其实这个决策边界是由类似于下面这种多项式构成的：

- 如果$
\theta\_0 + \theta\_1x\_1 + \theta\_2x\_2 + \theta\_3x\_1x\_2+\theta\_4x\_1^2 + \theta\_5x\_2^2 + ... \ge 0
$，则预测$h\_{\theta}(x)=1$；

- 如果$
\theta\_0 + \theta\_1x\_1 + \theta\_2x\_2 + \theta\_3x\_1x\_2+\theta\_4x\_1^2 + \theta\_5x\_2^2 + ... \lt 0
$，则预测$h\_{\theta}(x)=0$；

如果我们把假设函数改写成下面这种形式：

$$
\theta\_0+\theta\_1f\_1+\theta\_2f\_2+\theta\_3f\_3+\theta\_4f\_4+\theta\_5f\_5+...
$$

因此有：

$$
f\_1=x\_1
$$

$$
f\_2=x\_2
$$

$$
f\_3=x\_1x\_2
$$

$$
f\_4=x\_1^2
$$

$$
f\_5=x\_2^2
$$

$$
...
$$

以此类推，可以依次加入这些高阶项，但我们其实并不知道这些高阶项是不是我们真正需要的。我们之前谈到计算机视觉的时候，提到过在这里的输入是一个有很多像素的图像，我们看到如果用高阶项作为特征变量，运算量将是非常大的，因为有太多的高阶项需要被计算。

因此，我们是否有不同的选择，或者是更好的选择来构造特征变量，以用来嵌入到假设函数中呢？

### 用核函数构造新特征

事实上，这里有一个可以构造新特征$f\_1$、$f\_2$、$f\_3$的方法。

首先我们定义三个特征变量(但是对于实际问题而言，我们可以定义非常多的特征变量）：

<img src="/img/17_02_09/003.png" width = "300" height = "200" align=center />

将这三个点标记为$l^{(1)}$、$l^{(2)}$、$l^{(3)}$，接下来我要做的是定义新的特征变量：

$$
f\_1=similarity(x,l^{(1)})
$$

这里$similarite(x,l^{(1)})$是一种相似度的度量，度量样本$x$与第一个标记$l^{(1)}$的相似度。

这个度量相似度的公司是这样的：

$$
\begin{align\*}
f\_1&=similarity(x,l^{(1)})
\\\\
&=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}
$$

> $exp$是自然常数$e$为底的指数函数。

不知道你之前是否看了上一个选修课程的视频，$||w||$是表示向量$w$的长度。因此这里的$||x-l^{(1)}||$的意思就是就是向量的欧式距离。

因此，我们可以依次写出$f\_1$、$f\_2$、$f\_3$:

$$
\begin{align\*}
f\_1&=similarity(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}
$$

$$
\begin{align\*}
f\_2&=similarity(x,l^{(2)})
=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})
\end{align\*}
$$

$$
\begin{align\*}
f\_3&=similarity(x,l^{(3)})
=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})
\end{align\*}
$$

这里的$similarite(x,l)$函数，就被称为**核函数(Kernels)**。在这里，我们的例子中所说的**核函数**，实际上是**高斯核函数**，在后面我们还会见到不同的**核函数**。

核函数我们通常不写作$similarity(x,l^{(i)})$，而是写作：

$$
k(x,l^{(i)})
$$

### 核函数可以做什么？

我们来看看核函数到底可以做什么？

首先让我们来看看第一个标记：

$$
\begin{align\*}
f\_1&=similarity(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
=exp(-\frac{\sum\_{j=1}^{n}(x\_j-l\_j^{(1)})^2}{2σ^2})
\end{align\*}
$$

$l^{(1)}$是我之前在图中选取的几个点之中的一个，上面是$x$和$l^{(1)}$之间的核函数。

其中$||x-l^{(1)}||^2$这一项可以表示成各个$x$向量到$l$向量的距离求和的形式：$\sum\_{j=1}^{n}(x\_j-l\_j^{(1)})^2$。（这里我们依然忽略了截距的影响，即令$x\_0=1$）。

假设，如果$x\approx l^{(1)}$，即$x$与其中一个标记点非常接近，那么这个欧氏距离$||x-l^{(1)}||$就会接近0，因此：

$$
f\_1
\approx
exp(-\frac{0^2}{2σ^2})
\approx1
$$

相反的，如果$x$离$l^{(1)}$很远，那么会有：

$$
f\_1
\approx
exp(-\frac{(large\ number)^2}{2σ^2})
\approx0
$$

**这些特征变量的作用是度量$x$到标记$l^{(1)}$的相似度的，并且如果$x$离$l$非常接近，那么特征变量$f$就接近1；如果$x$离标记$l^{(1)}$非常远，那么特征变量$f$就接近于0。**

之前我绘制的三个标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$每一个标记点会定义一个新的特征变量：

$$
\begin{align\*}
f\_1&=k(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}
$$

$$
\begin{align\*}
f\_2&=k(x,l^{(2)})
=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})
\end{align\*}
$$

$$
\begin{align\*}
f\_3&=k(x,l^{(3)})
=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})
\end{align\*}
$$

**也就是说，给出一个训练样本$x$，我们就能基于我们之前给出的标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$来计算出三个新的特征变量$f\_1$、$f\_2$、$f\_3$。**

### 深入理解核函数

接下来让我们通过画一些图来更好地理解**核函数**是什么样的。

#### $x$对$f$的值的影响

看下面这个例子，假设我们有两个特征$x\_1$和$x\_2$，假设我们第一个标记点是$l^{(1)}$：

$$
l^{(1)}=
\begin{bmatrix}
   3 \\\
	5
 \end{bmatrix}
$$

假设:

$$
σ^2=1
$$

如果我画出:

$$
\begin{align\*}
f\_1&=k(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}
$$

结果就是这样的：

|3D曲面图|等高线图|
|:--:|:--:|
|<img src="/img/17_02_09/004.png" width = "300" height = "200" align=center />|<img src="/img/17_02_09/005.png" width = "300" height = "200" align=center />|

其中左侧的图纵轴是$f\_1$，水平方向的两个轴分别是$x\_1$和$x\_2$。右侧的图是左侧图的等高线图。

你会发现，当$x=(3,5)$的时候，$f\_1=1$，因为它在最大值的位置上。所以如果$x$往旁边移动，离这个点越远，那么从图中可以看到$f\_1$的值就越接近0。

#### $σ^2$对$f$的值的影响

在这里，要提到的另一点就是$σ^2$对结果的影响。$σ^2$是**高斯核函数**的参数，改变它会得到略微不同的结果。

可以对比$σ^2=1$、$σ^2=0.5$、$σ^2=3$的情况：

|$σ^2=1$|$σ^2=0.5$|$σ^2=3$|
|:--:|:--:|:--:|
|<img src="/img/17_02_09/004.png" width = "300" height = "200" align=center />|<img src="/img/17_02_09/006.png" width = "300" height = "200" align=center />|<img src="/img/17_02_09/008.png" width = "300" height = "200" align=center />|
|<img src="/img/17_02_09/005.png" width = "300" height = "200" align=center />|<img src="/img/17_02_09/007.png" width = "300" height = "200" align=center />|<img src="/img/17_02_09/009.png" width = "300" height = "200" align=center />|

你会发现，函数的形状还是相似的，只是$σ^2=0.5$相较于$σ^2=1$凸起的宽度变窄了，等值线图也收缩了一些；$σ^2=3$相较于$σ^2=1$凸起的宽度变宽了，等值线也扩张了一些。

所以，如果我们将$σ^2$设为0.5时，特征变量$f\_1$下降到0的速度也会相应变快；如果我们将$σ^2$设为3时，特征变量$f\_1$下降到0的速度也会相应变慢。

### 获取预测函数

讲完了特征变量的定义，我们来看看我们能得到什么样的预测函数。

给定一个训练样本$x$，我们要计算出三个特征变量$f\_1$ $f\_2$ $f\_3$

![](/img/17_02_09/010.png)

并且如果$\theta\_0 + \theta\_1f\_1 + \theta\_2f\_2 +  \theta\_3f\_3 \ge 0$，则预测函数的预测值为1，即$y=1$。

对于这个特定的例子而言，假设我们已经找到了一个学习算法，并且假设我已经得到了这些参数的值，比如如果：

$$
\theta\_0=-0.5 \\\\
\theta\_1=1 \\\\
\theta\_2=1 \\\\
\theta\_3=0
$$

如果我们现在有一个训练样本$x$：

![](/img/17_02_09/011.png)

我想知道预测函数会给出什么结果。

看看这个公式：

$$\theta\_0 + \theta\_1f\_1 + \theta\_2f\_2 +  \theta\_3f\_3 \ge 0$$

因为我的训练样本$x$接近于$l^{(1)}$，那么$f\_1$就接近于1：

$$
f\_1\approx1
$$

又因为训练样本$x$离$l^{(2)}$ $l^{(3)}$都很远，所以$f\_2$就接近于0，$f\_3$也接近于0：

$$
f\_2\approx0 \\\\
f\_3\approx0
$$

所以带入上面的公式可以得到：

$$
\begin{align\*}
\theta\_0 + \theta\_1f\_1 + \theta\_2f\_2 +  \theta\_3f\_3
&=\theta\_0 + \theta\_1·1 + \theta\_2·0 +  \theta\_3·0
\\\\
&=-0.5+1
\\\\
&=0.5\\\\
\ge0
\end{align\*}
$$

因此对于这一点，我们预测的结果是$y=1$。

---

现在我们选择另一个不同的点，假设我选择了另一个点：

![](/img/17_02_09/012.png)

如果将这个训练样本$x$带入之前相同的计算，你发现$f\_1$ $f\_2$ $f\_3$都接近于0。

因此，我们得到$\theta\_0 + \theta\_1f\_1 + \theta\_2f\_2 +  \theta\_3f\_3=-0.5$，因为$θ\_0=-0.5$，并且$f\_1$ $f\_2$ $f\_3$都为0，因此最后结果是-0.5，小于0。因此这个点，我们预测的y值是0。

---

类似的，如果你自己来对大量的点进行这样相应的处理，你应该可以确定如果你有一个非常接近于$l^{(2)}$的训练样本，那么通过这个点预测的y值也是1。

实际上，你最后得到的结果是：**对于接近$l^{(1)}$和$l^{(2)}$的点，我们的预测值是1，对于远离$l^{(1)}$和$l^{(2)}$的点，我们最后预测的结果是等于0的**。

我们最后会得到这个预测函数的判别边界看起来是类似这样的结果：

<img src="/img/17_02_09/013.png" width = "300" height = "200" align=center />

在这个红色的判别边界里面，预测的y值等于1；在这外面预测的y值等于0。

因此这就是一个我们如何通过标记点，以及核函数，来训练出非常复杂的非线性判别边界的方法。

这就是核函数这部分的概念，以及我们如何在支持向量机中使用它们。我们通过标记点和相似性函数来定义新的特征变量从而训练复杂的非线性分类器。

目前还有一些问题我们并没有做出回答，其中一个是**我们如何得到这些标记点**；另一个是**其他的相似度方程（核函数）是什么样的**，如果有其他的话，我们能够用其他的相似度方程来代替我们所讲的这个高斯核函数吗？在下一个视频中我们会回答这些问题，然后把所有东西都整合到一起来看看支持向量机如何通过核函数的定义 有效地学习复杂非线性函数。

## 核函数 II

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/hxdcH/kernels-ii)

> 在上一节视频里，我们讨论了**核函数**这个想法以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。例如，**怎么处理支持向量机中的偏差方差折中**。

### 如何选取标记点(landmark)

在上一节课中，我谈到过选择标记点，例如$l^{(1)}$ $l^{(2)}$ $l^{(3)}$ 这些点使我们能够定义**相似度函数**，也称之为**核函数**。在这个例子里，我们的相似度函数为**高斯核函数**。

![](/img/17_02_09/014.png)

但是，我们从哪里得到这些标记点呢？我们从哪里得到$l^{(1)}$ $l^{(2)}$ $l^{(3)}$？ 而且在一些复杂的学习问题中，也许我们需要更多的标记点，而不是我们手选的这三个。

因此，在实际应用时，怎么选取标记点，是机器学习中必须解决的问题。

这是我们的数据集：

![](/img/17_02_09/015.png)

有一些正样本和一些负样本。我们的想法是：我们直接将训练样本作为标记点。

![](/img/17_02_09/016.png)

如果我们有一个训练样本$x^{(1)}$，那么我将把第一个标记点就放在和第一个训练样本点完全重合的地方：

![](/img/17_02_09/017.png)

同样，第二个标记点也是使用同样的方式来标注：

![](/img/17_02_09/018.png)

在右边的这幅图上，我们使用红点和蓝点来阐述这幅图以及这些点的颜色，可能并不显眼，但是利用这个方法最终能得到m个标记点：

$$
l^{(1)} ，l^{(2)}，...，l^{(m)}
$$

即每一个标记点的位置，都与每一个样本点的位置精确对应。

这说明，特征函数基本上是在描述**每一个样本距离样本集中其他样本的距离**。

我们具体的列出这个过程的大纲：

给定m个训练样本：

$$
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}),
$$

我将选取与m个训练样本精确一致的位置作为我的标记点：

$$
l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},...,l^{(m)}=x^{(m)}.
$$

当输入样本$x$（样本$x$可以属于训练集，也可以属于交叉验证集，也可以属于测试集），我们可以计算这些特征，即：

$$
f\_1=similarity(x,l^{(1)}) \\\\
f\_2=similarity(x,l^{(2)}) \\\\
...
$$

> 这里的$l^{(1)}=x^{(1)}$，$l^{(2)}=x^{(2)}$...。

最终我们能得到一个特征向量，我们将特征向量记为$f$：

$$
f=
\left[
\begin{matrix}
 f\_1  \\\
 f\_2  \\\
 ...	\\\
 f\_m
\end{matrix}
\right]
$$

此外，按照惯例，如果我们需要的话，可以添加额外的特征$f\_0$，$f\_0$的值始终为1：

$$
f=
\left[
\begin{matrix}
 f\_0  \\\
 f\_1  \\\
 f\_2  \\\
 ...	\\\
 f\_m
\end{matrix}
\right]
$$

它与我们之前讨论过的截距$x^0$的作用相似。

---

举个例子，假设我们有训练样本$(x^{(i)},y^{(i)})$，这个样本对应的特征向量可以这样计算：

给定$x^{(i)}$，我们可以通过相似度函数
$$
f\_1^{(i)}=similarity(x^{(i)},l^{(1)}) \\\
f\_2^{(i)}=similarity(x^{(i)},l^{(2)}) \\\
... \\\
f\_m^{(i)}=similarity(x^{(i)},l^{(m)}) \\\
$$

在这一列中的某个位置，即第$i$个元素，有一个特征：

$$
f\_i^{(i)}=similarity(x^{(i)},l^{(i)})
$$

> 这里的$l^{(i)}$就等于$x^{(i)}$。

所以$f\_i^{(i)}$衡量的是$x^{(i)}$与其自身的相似度，如果你使用高斯核函数的话，这一项为：

$$
f\_i^{(i)}=similarity(x^{(i)},l^{(i)})=exp(-\frac{0}{2σ^2})=1
$$

所以，对于这个样本来说，其中的某一个特征等于1。接下来，类似于我们之前的过程，我将这m个特征合并为一个特征向量。于是，相比之前用$x^{(i)}$来描述样本，$x^{(i)}$为n维或者n+1维空间。我们现在可以使用这个特征向量$f^{(i)}$来描述我的特征向量：

$$
f^{(i)}=
\left[
\begin{matrix}
 f\_0^{(i)}  \\\
 f\_1^{(i)}  \\\
 f\_2^{(i)}  \\\
 ...	\\\
 f\_m^{(i)}
\end{matrix}
\right]
$$

其中 $f\_0^{(i)}=1$。

那么这个向量就是我们用于描述训练样本的特征向量。

当给定核函数和相似度函数后，我们按照这个方法来使用**支持向量机**。

---

如果你已经得到参数$\theta$并且想对样本x做出预测，我们先要计算特征向量$f$，$f$是$m+1$维的特征向量（这里有m是因为我们有m个训练样本，因此就有m个标记点）。

我们在$\theta^Tf\ge0$时，预测$y=1$

> 这里$\theta^Tf=\theta\_0f\_0 + \theta\_1f\_1 + ... + \theta\_mf\_m$

以上就是**当已知参数$\theta$时，怎么做出预测的过程**。

那么，怎样得到参数$\theta$呢？

你在使用SVM学习算法的时候，具体来说就是要求解这个最小化问题：


$$
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}f^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}f^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
$$

你需要求出能使这个式子取最小值的参数$\theta$。注意，这里我们把之前的$x^{(i)}$换成了$f^{(i)}$。

通过解决这个最小化问题，我们就能得到支持向量机的参数。

最后一个对于这个优化问题的细节是：我们有$n=m$个特征。有效的特征数量应该等于$f$的维数，所以$n$其实就等于$m$。

![](/img/17_02_09/019.png)

以上就是支持向量机的学习算法。

我在这里还要讲到一个数学细节，在支持向量机实现的过程中，最后一项与上面式子中的最后一项$\frac{1}{2}\sum\_{j=1}^{n}\theta\_{j}^2$有细微差别。其实在实现支持向量机时，你并不需要知道这个细节，事实上这个式子已经给你提供了全部需要的原理。但是在支持向量机实现的过程中，这一项$\sum\_{j=1}^{n}\theta\_{j}^2$可以被重写为$\theta^T\theta$(如果我们忽略$\theta\_0$的话)，即：

$$
\sum\_{j=1}^{n}
\theta\_{j}^2
=
\theta^T\theta
$$

其中：

$$
\theta=
\left[
\begin{matrix}
 \theta\_1  \\\
 \theta\_2  \\\
 ...	\\\
 \theta\_m  \\\
\end{matrix}
\right] \ \ (ignore \ \theta\_0)
$$

大多数支持向量机在实现的时候其实是替换掉$\theta^T\theta$的，用$\theta^TM\theta$来代替，其中$M$是某矩阵，具体是什么矩阵取决于你采用的核函数。这其实是另一种略有区别的距离度量方法。我们用这种略有变化的度量距离的形式来取代对$||\theta||^2$（即$\theta^T\theta$，或者$\sum\_{j=1}^{n}\theta\_{j}^2$）进行最小化的形式，这是参数向量$\theta$的**变尺度形式**，这种变化和核函数相关。这个数学细节使得支持向量机能够更有效率的运行。

支持向量机做这种修改的理由是：这么做可以适应超大的训练集。

例如：当你的训练集有10000个样本时

$$
M=10000
$$

根据我们之前定义标记点的方法，我们最终有10000个标记点，$\theta$也随之是10000维的向量。或许这时这么做还可行，但是，当$m$变得非常非常大时，那么求解这么多参数时，此时利用支持向量机软件包来解决这里的最小化问题时，求解这些参数的成本会非常高。

这些都是数学细节，事实上你没有必要了解这些，这里$\theta^TM\theta$实际上细微的修改了最后一项，使得最终的优化目标与直接最小化$||\theta||^2$略有区别。

如果你愿意的话，你可以直接认为这个具体的实现细节尽管略微的改变了优化目标，但是它主要是为了计算效率，所以你不必要对此有太多担心。

顺便说一下，你可能会想，为什么我们不将核函数这个想法应用到其他算法，比如逻辑回归上。事实证明，如果你愿意的话，确实可以将核函数这个想法用于定义特征向量，将标记点之类的技术用于逻辑回归算法。但是用于支持向量机的计算技巧不能较好的推广到其他算法，诸如逻辑回归上。所以，将核函数用于逻辑回归上时，会变得非常的慢。相比之下，这些计算技巧，比如这一步：$\theta^TM\theta$，这些细节的修改，以及支持向量机软件的实现细节，使得支持向量机可以和核函数相得益彰，而逻辑回归和核函数则会运行的十分缓慢，更何况它们还不能使用那些高级优化技巧，因为这些技巧是人们专门为使用核函数的支持向量机开发的。但是这些问题只有在你亲自实现最小化函数时才会遇到。

我将在下一节视频中进一步讨论这些问题，但是你并不需要知道怎么去写一个软件，来最小化代价函数。你能找到很好的成熟的软件来做这些，就像我一直不建议自己写矩阵求逆函数，或者平方根函数的道理一样。这些软件包已经包含了那些数值优化技巧，所以你不必担心这些东西。

---

### SVM参数：

#### $C(=\frac{1}{\lambda})$

但是另外一个值得说明的问题是，在你使用支持向量机时，怎么选择支持向量机中的参数？在本节视频的末尾，我想稍微说明一下，在使用支持向量机时的“偏差-方差折中”其中一个要选择的事情是，目标函数中的参数$C$。回忆一下，$C$的作用与$\frac{1}{\lambda}$相似。

![](/img/17_02_09/020.png)

$\lambda$是逻辑回归算法中的正则化参数，所以$C$对应着我们之前在逻辑回归问题中的$\lambda$，这意味着:

- 较小的$\lambda$对应较大的$C$，这就意味着有可能得到一个低偏差但高方差的模型。
- 较大的$\lambda$对应较小的$C$，这就意味着有可能得到一个高偏差但低方差的模型。

所以使用较大的$C$值模型，为高方差，更倾向于过拟合；而使用较小的$C$值的模型，为高偏差，更倾向于欠拟合。

#### $σ^2$

另一个要处理的参数是**高斯核函数**中的$σ^2$。

当高斯核函数中的$σ^2$偏大时，那么对应的相似度函数为：

$$
exp(-\frac{||x-l^\{(i)}||^2}{2σ^2})
$$

在下面这个例子中，如果我们只有一个特征$x\_1$，有一个标记点$l$：

![](/img/17_02_09/021.png)

**如果$σ^2$越大**，那么高斯核函数倾向于变得**越平滑**，由于函数平滑且变化的比较平缓，这会给你的模型带来**较高的偏差和较低的方差**，由于高斯核函数变得平缓，就更倾向于得到一个随着输入$x$变化得缓慢的模型：

![](/img/17_02_09/022.png)

反之**如果$σ^2$越小**，那么高斯核函数会变化的**很剧烈**，在这种情况下，最终得到的模型会是**低偏差和高方差**：

![](/img/17_02_09/023.png)

----

这就是利用核函数的支持向量机算法。

希望这些关于方差和偏差的讨论能给你一些对于算法结果预期的直观印象。