title: 斯坦福机器学习课程 第三周 (1)分类和表达式
tags:
  - 算法
categories:
  - 算法
  - 机器学习
comments: true
date: 2016-08-21 18:00:00
mathjax: true
---

## 分类

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/wlPeP/classification)

在接下来的几节课中，将介绍分类问题。我们将学习一种叫做**逻辑回归(Logistic Regression)**的算法，这也是目前最流行，使用最广泛的一种学习算法。

下面是几个使用分类算法的例子：

- Email的垃圾邮件分类问题：区分邮件是否为垃圾邮件
- 网上交易分类问题：是否为欺诈交易
- 肿瘤分类问题：区分肿瘤是恶性还是良性

在所有的这些问题中，我们想要预测的变量是$y$，我们可以认为它可以取两个值：`0`或者`1`。

$$
y\in {0, 1}
$$

> 0：“负类”（例如：良性肿瘤）
> 1：“正类”（例如：恶性肿瘤）
 
现在我们要开始研究只有两类（0和1）的分类问题。之后我们也会讨论更多类别的问题：$y\in {0,1,2,3,...}$，这就是所谓的**多类分类问题**。但接下来，我们将开始介绍只有两种类别的分类问题，或者叫做**二元分类**问题。

### 二元分类

那么我们要怎样开发一个分类算法呢？下面是一个用来给肿瘤分类的训练集的例子(恶性或良性)。

![](/img/16_08_22/001.png)

> 注意这里的恶性值(Malignant)只取两个值：0也就是良性，1也就是恶性。

所以拿到这样的数据，我们通常的做法是用线性回归算法尝试使用一条直线来拟合这些数据：

$$
h\_{\theta}(x)=\theta^{T}x
$$

如果你试着用一条直线去拟合这些数据，你也许会得到这样一条直线：

![](/img/16_08_22/002.png)

如果你想要尝试分类，可以试着将分类器的阈值设为0.5：

- 当$h\_{\theta}(x)\ge0.5$时，预测$y=1$。
- 当$h\_{\theta}(x)\lt0.5$时，预测$y=0$。

因此，如图这里就是阈值的所在点：

![](/img/16_08_22/003.png)

这个点右侧的点，我们会将它们全部预测为正，因为它们的输出值在纵轴上都是大于0.5的。同理，在这一点的左侧的点，我们会将它们全部预测为负。

在这个例子中，看起来好像线性回归所做的实际上是合理的，现在让我们稍做一些改动，延长一下横轴，假设在很远的右侧有一个训练样本。

![](/img/16_08_22/004.png)

此时，得到的拟合数据的直线就会是上图那样，而且阈值所在的位置也会向右侧偏移。此时我们预测阈值点右侧的值都为正，左侧都为负。这看起来线性回归的效果并不好。因为我们可以很明显的看出来，新增的这个点其实并没有给我们带来什么新的信息，这里的意思是说，按照原本的预测，这一个位置出现的点就应该是正。而且我们根据这个新增的点得到的阈值所在点并没有很好的把数据进行划分。

因此**使用线性函数进行分类并不是一个好办法**。在第一个例子中，线性函数能够正常的预测数据只不过是巧合罢了。

另外一个不使用线性函数进行分类的原因是：对于分类问题，我们通常只需要取得$y=0$或$y=1$。如果使用线性回归，那么假设输出函数很有可能远大于1或者远小于0。

![](/img/16_08_22/005.png)

因此我们接下来的视频中将要介**逻辑回归算法(Logistic Regression)**，这个算法的性质就是适用于$y$值为离散值的情况，并且它的输出值保证在0到1之间:
$$
0\le h\_{\theta}(x)\le1
$$

> 顺便提一句，**逻辑回归算法**是一个**分类算法**，即使它的名字里有**回归**，但我们也把它当成分类算法来使用。这里只是因为历史原因，才被这样称呼。

## 假设函数表达式

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/RJXfB/hypothesis-representation) 

之前我们说过，我们希望分类器的输出值的范围能控制在0和1之间：

$$
0\le h\_{\theta}(x)\le1
$$

因此我们渴望找到一个满足这种预测值性质的函数。

当我们使用线性回归时，我们用下面的式子来作为假设函数表达式：

$$
h\_{\theta}(x) = \theta^{T}x
$$

对于逻辑回归来说，我们需要稍微改动一下：

$$
h\_{\theta}(x) = g(\theta^{T}x)
$$

其中$g$函数的表达式为：

$$
g(z)=\frac{1}{1+e^{-z}}
$$

这个函数被称为**S型函数(Sigmoid function)**，或者**逻辑函数(Logistic function)**。

> **S型函数(Sigmoid function)**和**逻辑函数(Logistic function)**基本上是同义词。因此这两个术语是可以互换的。

**S型函数(Sigmoid function)**的图形是这样的，顶部是随着$z$的增大而逐渐趋近于1，底部随着$z$的减小而逐渐趋近于0：

![](/img/16_08_22/006.png)


因此我们可以得到：

$$
\begin{align\*}
h\_{\theta}(x) &= g(\theta^{T}x) \\\\
&= \frac{1}{1+e^{-\theta^{T}x}}
\end{align*}
$$ 

接下来我们要做的就是用参数$\theta$来拟合我们的数据。

### 对假设函数的进一步理解

**假设函数的解释**：

$h\_{\theta}(x)$是对于新注入样本$x$的满足$y=1$的概率估计。

举个肿瘤分类的例子来说：

如果：

$$
x=
\begin{bmatrix}
x\_{0} \\\\
x\_{1}
\end{bmatrix}
=
\begin{bmatrix}
1 \\\\
tumorSizs
\end{bmatrix}
$$

$$
h\_{\theta}(x) = 0.7
$$

和之前一样，特征向量$x\_{0}=1$，特征向量$x\_{1}=tumorSize$代表肿瘤大小。

假设有一个病人，我们已知他肿瘤的大小，将他的特征向量$x$带入到$h\_{\theta}(x)$中，得到的结果是$0.7$，那么我的解释就是$y=1$的概率是$0.7$，即这个患者的肿瘤是恶性的概率是$70\%$。

用更加正式的式子来表示就是：

$$
h\_{\theta}(x) = P(y=1|x;\theta)
$$

> 熟悉概率的同学可能对这个式子不陌生，这个式子代表着**在给定的特征$x$的情况下，$y=1$的概率，其中参数为$\theta$**。
> 在这里给定的$x$就是代表我的病人的肿瘤大小的特征$x$。

由于我们知道，$y$的取值不是0就是1，因此当我们得到了$y=1$的概率时，$y=0$的概率就很容易得出：

$$
P(y=0|x;\theta) + P(y=1|x;\theta) = 1 \\\\
P(y=0|x;\theta) = 1 - P(y=1|x;\theta) 
$$

## 决策边界

[视频地址](https://www.coursera.org/learn/machine-learning/discussions/weeks/3)

这一节向大家介绍一个**决策边界(Decision Boundary)**的概念，这个概念能更好的帮助大家理解逻辑回归中假设函数在计算什么。

在上一节中，我们说假设函数可以表示为如下形式：

$$
h\_{\theta}(x) = g(\theta^{T}x)
$$

$$
g(z)=\frac{1}{1+e^{-z}}
$$

对应的图像是这样的：

![](/img/16_08_22/007.png)

更进一步来理解：

- 如果$h\_{\theta}(x)\ge0.5$ 我们认为$y=1$
- 如果$h\_{\theta}(x)\lt0.5$ 我们认为$y=0$

根据图片，我们可以得出:

- 当$z\ge0$时，$h\_{\theta}(x)\ge0.5$，满足$y=1$；
- 当$z\lt0$时，$h\_{\theta}(x)\lt0.5$，满足$y=0$。

即：

- 当$\theta^{T}x\ge0$时，$y=1$；
- 当$\theta^{T}x\lt0$时，$y=0$；

通过上面这些，我们可以更好的理解如何利用逻辑回归的假设函数来进行预测。

接下来我们看一个例子：

![](/img/16_08_22/008.png)

假设我们有一个训练集，我们的假设函数为：

$$
h\_{\theta}(x)=g(\theta\_{0} + \theta\_{1}x\_{1} + \theta\_{2}x\_{2})
$$

> 目前我们还没有讲解如何拟合此模型中的参数，这个知识点将在下一节中讲解。这里我们假设数据已经是拟合好的了。

假设我们已经拟合好了数据，并且得到了$\theta\_{0}=-3$、$\theta\_{1}=1$、$\theta\_{2}=1$。这意味着：

$$
\theta=
\begin{bmatrix}
-3 \\\\
1 \\\\
1
\end{bmatrix}
$$

根据上面我们的讨论，我们可以得到：

- 当$-3 + x\_{1} + x\_{2} \ge 0$时，满足$y=1$；
- 当$-3 + x\_{1} + x\_{2} \lt 0$时，满足$y=0$。

因此，当$x\_{1} + x\_{2} = 3$时，我们可以得到一条直线：

![](/img/16_08_22/009.png)

在这条直线右上方的点，代表$y=1$的点，这片区域被称为$y=1$区域。于此相对的是，在这条直线左下方的点，代表$y=0$的点，这片区域被称为$y=0$区域。

上面我们绘制出来的$x\_{1} + x\_{2} = 3$这条直线被称为**决策边界(decision boundary)**。

> 值得说明的是，决策边界其实是假设函数$h\_{\theta}(x)$的属性，它取决于其参数$\theta$。目前我们还没有说明如何对这些参数$\theta$进行取值，后面我们会学习如何使用训练集来确定这些参数的取值。但是，我们一旦能确定$\theta$的值时，我们就能确定出决策边界了。

接下来我们来看一个更复杂的例子：

![](/img/16_08_22/010.png)

和之前一样，圆圈表示负样本，红色的叉号表示正样本。那么我们怎样使用逻辑回归来拟合这些数据呢？

之前我们讲过多项式回归时，我们谈论到可以通过添加额外的高阶多项式项，这里我们同样可以对逻辑回归使用相同的方法。

具体的说，假设我的假设函数如下：

$$
h\_{\theta}(x) = 
g(\theta\_{0}+\theta\_{1}x\_{1}+\theta\_{2}x\_{2}+\theta\_{3}x\_{1}^{2}+\theta\_{4}x\_{2}^{2})
$$

之前我们说到，我们将在后面的学习讨论如何得到正确的$\theta$，这里假设我们已经得到了正确的$\theta$，值如下：

$$
\begin{align\*}
\theta\_{0} &= -1 \\\\
\theta\_{1} &= 0 \\\\
\theta\_{2} &= 0 \\\\
\theta\_{3} &= 1 \\\\
\theta\_{4} &= 1 
\end{align*}
$$

在这个参数的选择下，我们的参数向量将是：

$$
\theta=
\begin{bmatrix}
-1 \\\\
0 \\\\
0 \\\\
1 \\\\
1
\end{bmatrix}
$$

这也说明，若我们期望$y$满足$y=1$，那么$x\_{1}$,$x\_{2}$需满足：

$$
-1 + x\_{1}^{2} + x\_{2}^{2} \ge 0
$$

对应的决策边界就是这样：

![](/img/16_08_22/011.png)

-----

其实对于更复杂的假设函数：

$$
h\_{\theta}(x) = 
g(\theta\_{0}+\theta\_{1}x\_{1}+\theta\_{2}x\_{2}+\theta\_{3}x\_{1}^{2}+\theta\_{4}x\_{1}^{2}x\_{2}+\theta\_{5}x\_{1}^{2}x\_{2}^{2}+\theta\_{6}x\_{1}^{3}x\_{2}+...)
$$

它的决策边界可能会是一些有趣的形状：

![](/img/16_08_22/012.png)
