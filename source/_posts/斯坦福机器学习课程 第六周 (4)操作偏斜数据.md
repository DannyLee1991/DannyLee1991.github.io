title: 斯坦福机器学习课程 第六周 (4)操作偏斜数据
tags:
  - 机器学习
  - 斯坦福课程
categories:
  - 机器学习
comments: true
date: 2017-01-19 23:29:58
mathjax: true
---

## 偏移类的错误度量

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/tKMWX/error-metrics-for-skewed-classes)

> 在前面的课程中,我提到了**误差分析**以及设定误差度量值的重要性。那就是设定某个实数来评估你的学习算法，并衡量它的表现。有了算法的评估和误差度量值，有一件重要的事情要注意，就是使用一个合适的误差度量值。这有时会对于你的学习算法造成非常微妙的影响。
> 
> 这件重要的事情就是**偏斜类（skewed classes）**的问题。

想一想之前的癌症分类问题，我们拥有内科病人的特征变量，我们希望知道他们是否患有癌症，这就像我们之前讲过的恶性与良性肿瘤的分类问题。

**癌症分类实例**

- 训练逻辑回归模型$h\_{\theta}(x)$。(如果是癌症$y=1$，否则$y=0$)
- 发现你在测试集上有1%的错误率。(99%的诊断是正确的)
- 只有0.50%的病人患有癌症

我们训练逻辑回归模型，假设我们用测试集检验了这个分类模型，并且发现它只有1%的错误，因此我们99%会做出正确诊断。看起来是非常不错的结果，我们99%的情况都是正确的。

但是，假如我们发现，在测试集中只有0.5%的患者真正得了癌症，那么在这个例子中 1%的错误率就不再显得那么好了。

举个具体的例子，这里有一行代码：

```
function y = predictCancer(x)
	y = 0; %ignore x!
return
```

不是机器学习代码，它忽略了输入值`X`，它让`y`总是等于0，因此它总是预测没有人得癌症，那么这个算法实际上只有`0.5%`的错误率。因此这甚至比我们之前得到的`1%`的错误率更好。这是一个非机器学习算法，因为它只是预测`y`总是等于`0`。

这种情况发生在正例和负例的比率非常接近于一个极端值，在这个例子中，正样本的数量与负样本的数量相比，非常非常少。因为`y=1`非常少，我们把这种情况叫做**偏斜类**。

一个类中的样本数与另一个类的数据相比多很多，通过总是预测`y=0`或者总是预测`y=1`算法可能表现非常好。因此使用分类误差或者分类精确度来作为评估度量可能会产生如下问题：

假如说你有一个算法，它的精确度是`99.2%`，因此它只有`0.8%`的误差。假设你对你的算法做出了一点改动，现在你得到了`99.5%`的精确度，只有`0.5%`的误差，这到底是不是算法的一个提升呢？

用某个实数来作为评估度量值的一个好处就是，它可以帮助我们**迅速决定我们是否需要对算法做出一些改进**。

将精确度从`99.2%`提高到`99.5%`，但是我们的改进到底是有用的，还是说我们只是把代码替换成了像`y=0`这样的东西？

因此如果你有一个偏斜类，用分类精确度并不能很好地衡量算法，因为你可能会获得一个很高的精确度，非常低的错误率。但是我们并不知道我们是否真的提升了分类模型的质量，因为`y=0`并不是一个好的分类模型。但是`y=0`会将你的误差降低至`0.5%`。

### 查准率（precision）和召回率（recall）

当我们遇到这样一个偏斜类时，我们希望有一个不同的误差度量值，或者不同的评估度量值。其中一种评估度量值叫做**查准率（precision）**和**召回率（recall）**。

让我来解释一下：

假设我们正在用测试集来评估一个分类模型，对于测试集中的样本，每个测试集中的样本都会等于0或者1(假设这是一个二分问题)我们的学习算法要做的是：做出值的预测，并且学习算法会为每一个测试集中的实例做出预测，预测值也是等于0或1。

让我画一个2x2的表格：

![](/img/17_01_19/001.png)

基于实际的类与预测的类：

- 如果有一个样本它实际所属的类是1，预测的类也是1，那么我们把这个样本叫做**真阳性（true positive）**，意思是说我们的学习算法 预测这个值为阳性，实际上这个样本也确实是阳性。
- 如果我们的学习算法预测某个值是阴性(等于0)，实际的类也确实属于0，那么我们把这个叫做**真阴性（true negative）**，我们预测为0的值实际上也等于0。
- 如果我们的学习算法预测某个值等于1，但是实际上它等于0，这个叫做**假阳性（false positive）**。
> 比如我们的算法预测某些病人患有癌症，但是事实上他们并没有得癌症。
- 最后如果我们的学习算法预测某个值等于0，但是实际上它等于1，这个叫做**假阴性（false negative）**。因为我们的算法预测值为0，但是实际值是1。

这样我们有了一个另一种方式来评估算法的表现。

我们要计算两个数字：

#### 查准率（Precision）

第一个叫做**查准率**这个意思是，对于所有我们预测他们患有癌症的病人，有多大比率的病人是真正患有癌症的，让我把这个写下来：

$$
\frac{真阳性的数量} {预测值为阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阳性的数量}
$$

这个就叫做**查准率**，**查准率**越高就越好。这是说，对于那些病人，我们告诉他们："非常抱歉，我们认为你得了癌症"，高查准率说明对于这类病人我们对预测他们得了癌症有很高的准确率。


#### 召回率（Recall）

另一个数字我们要计算的，叫做**召回率**，**召回率**是如果所有的在数据集中的病人（假设测试集中的病人，或者交叉验证集中的病人）确实得了癌症，有多大比率 我们正确预测他们得了癌症。召回率被定义为：

$$
\frac{真阳性的数量} {实际阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阴性的数量}
$$

同样地，召回率越高越好。

通过计算**查准率**和**召回率**我们能更好的知道分类模型到底好不好。

具体地说，如果我们有一个算法，总是预测`y=0`，即它总是预测没有人患癌症，那么这个分类模型的召回率等于0，因为它不会有**真阳性**。因此我们能会快发现这个分类模型不是一个好的模型。

总的来说，即使我们有一个非常偏斜的类，算法也不能够"欺骗"我们。我们能够更肯定的是：拥有高查准率或者高召回率的模型是一个好的分类模型。这给予了我们一个更好的评估值，给予我们一种更直接的方法来评估模型的好与坏。

最后一件需要记住的事，在查准率和召回率的定义中，我们总是习惯性地用`y=1`，因此如果我们试图检测某种很稀少的情况（比如癌症，我希望它是个很稀少的情况），查准率和召回率会被定义为`y=1`而不是`y=0`作为某种我们希望检测的出现较少的类。通过使用查准率和召回率，我们发现即使我们拥有非常偏斜的类，算法不能够通过总是预测`y=1`或`y=0`来"欺骗"我们。因为它不能够获得高查准率和召回率。具体地说，如果一个分类模型拥有高查准率和召回率，那么我们可以确信地说这个算法表现很好，即便它是一个很偏斜的类。

因此对于偏斜类的问题，查准率和召回率给予了我们更好的方法来检测学习算法表现如何，这是一种更好地评估学习算法的标准。当出现偏斜类时，比仅仅只用分类误差或者分类精度好。

## 查准率和召回率练习

[视频地址](https://www.coursera.org/learn/machine-learning/lecture/CuONQ/trading-off-precision-and-recall)

> 在之前的课程中，我们谈到**查准率**和**召回率**作为遇到偏斜类问题的评估度量值。在很多应用中，我们希望能够保证**查准率**和**召回率**的相对平衡。在这节课中，我将告诉你应该如何处理此类问题，同时也向你展示一些查准率和召回率 作为算法评估度量值的更有效的方式。

回忆一下，这是我们在上一节中讲到的**查准率**和**召回率**的定义：

$$
查准率 = \frac{真阳性的数量} {预测值为阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阳性的数量}
$$

$$
召回率 = \frac{真阳性的数量} {实际阳性的数量} = \frac{真阳性的数量} {真阳性的数量 + 假阴性的数量}
$$

让我们继续用癌症分类的例子。

如果病人患癌症，则`y=1`反之则`y=0`。

假设我们用**逻辑回归模型**训练了数据，输出概率是在`0-1`之间的值：

$$
0\le h\_{\theta}(x)\le1
$$

因此可以得到：

- 如果$h\_{\theta}(x)\ge0.5$，预测值为1 
- 如果$h\_{\theta}(x)\lt0.5$，预测值为0 

这个回归模型能够计算**查准率**和**召回率**。但是现在，假如我们希望在我们非常确信地情况下，才预测一个病人得了癌症。

> 因为你知道如果你告诉一个病人，说他得了癌症，他会非常震惊，因为这是一个非常坏的消息，而且他们会经历一段非常痛苦的治疗过程，因此我们希望只有在我们非常确信的情况下，才告诉这个人他得了癌症。

这样做的一种方法是修改算法。

### 高查准率低召回率情况

我们不再将临界值设为`0.5`，也许我们只在$h\_{\theta}(x)\ge0.7$的情况下，才预测$y=1$。因此我们会在我们认为他有$\ge70\%$得癌症的概率情况下，告诉一个人他得了癌症。

如果你这么做，那么你只在非常确信地情况下才去预测癌症，那么你的回归模型会有较高的**查准率**，因为在所有你准备告诉他们患有癌症的病人中，他们有较高的可能性真的患有癌症。

你预测患有癌症的病人中有较大比率的人，他们确实患有癌症。因为这是我们在非常确信的情况下做出的预测。

与之相反，这个回归模型会有较低的**召回率**，因为当我们做预测的时候，我们只给很小一部分的病人预测$y=1$，现在我们把这个情况夸大一下，我们不再把临界值 设在$0.7$，我们把它设为$0.9$，我们只在至少$90\%$肯定这个病人患有癌症的情况下才预测$y=1$。那么这些病人当中，有非常大的比率，真正患有癌症。因此这是一个**高查准率**的模型。但是**召回率**会变低，因为我们希望能够正确检测患有癌症的病人。

### 高召回率低查准率情况

现在考虑一个不同的例子。

假设我们希望避免遗漏掉患有癌症的人，即我们希望避免**假阴性**。具体地说，如果一个病人实际患有癌症，但是我们并没有告诉他患有癌症，那这可能会造成严重后果。因为如果我们告诉病人他们没有患癌症，那么他们就不会接受治疗。但是如果他们真的患有癌症，我们又没有告诉他们，那么他们就根本不会接受治疗，这可能会造成严重后果，甚至使病人丧失生命，因为我们没有告诉他患有癌症，他没有接受治疗。这种情况下，我们希望预测病人患有癌症，即$y=1$。这样他们会做进一步的检测，然后接受治疗以避免他们真的患有癌症。

在这个例子中，我们不再设置高的**临界值**，我们会设置另一个值，将**临界值**设得较低，比如$0.3$。这样以来，我们认为这些病人有$\gt30\%$的概率患有癌症，我们以更加保守的方式来告诉他们患有癌症，因此他们能够接受治疗。但是在这种情况下，我们会有一个较**高召回率**的模型。因为在患有癌症的病人中，有很大一部分被我们正确标记出来了，但是我们会得到较低的查准率，因为我们预测患有癌症的病人比例越大，那么就有较大比例的人其实没有患癌症。

---

顺带一提，当我在给别的学生讲这个的时候，令人惊讶的是有的学生问，怎么可以从两面来看这个问题？为什么我总是只想要高查准率或高召回率，但是这看起来可以使两边都提高。更普遍的一个原则是：这取决于你想要什么。你想要**高查准率低召回率**，还是**高召回率低查准率**？你可以预测$y=1$当$h\_\theta(x)\ge临界值$。因此总的来说，对于大多数的回归模型，你得权衡查准率和召回率。

### 临界值的选取

当你改变临界值的值时，你可以画出曲线来权衡查准率和召回率：

<img src="/img/17_01_19/002.png" width = "300" height = "200" align=center />

这里的一个值，反应出一个较高的临界值，这个临界值可能等于$0.99$，我们假设 只在有大于$99\%$的确信度的情况下，才预测$y=1$。至少，有$99\%$的可能性。因此这个点反应**高查准率低召回率**：

<img src="/img/17_01_19/003.png" width = "300" height = "200" align=center />

然而这里的一个点，反映一个较低的临界值，比如说$0.01$，毫无疑问，在这里预测$y=1$，如果你这么做，你最后会得到**低查准率高的召回率**的预测结果：

<img src="/img/17_01_19/004.png" width = "300" height = "200" align=center />

当你改变临界值时，如果你愿意，你可以画出回归模型的所有曲线，来看看你能得到的查准率和召回率的范围。

顺带一提，查准率-召回率曲线可以是各种不同的形状，有时它看起来是这样：

<img src="/img/17_01_19/005.png" width = "300" height = "200" align=center />

有时是那样：

<img src="/img/17_01_19/006.png" width = "300" height = "200" align=center />

查准率-召回率曲线的形状有很多可能性，这取决于回归模型的具体算法。因此这又产生了另一个有趣的问题，那就是**有没有办法自动选取临界值**，或者更广泛地说，如果我们有不同的算法，我们如何比较不同的查准率和召回率呢？

#### 选取临界值的方式：$F\_{1}Score$(F score) 

具体来说，假设我们有三个不同的学习算法；或者这三个不同的学习曲线是同样的算法，但是临界值不同。我们怎样决定哪一个算法是最好的？

||查准率(P)|召回率(R)|
|:-:|:--:|:--:|
|算法1|0.5|0.4|
|算法2|0.7|0.1|
|算法3|0.02|1.0|

我们之前讲到的其中一件事就是评估度量值的重要性。这个概念是通过一个具体的数字来反映你的回归模型到底如何。但是查准率和召回率的问题我们却不能这样做，因为在这里我们有两个可以判断的数字。

因此我们经常会不得不面对这样的情况：如果我们正在试图比较**算法1**和**算法2**，我们最后问自己，到底是`0.5`的查准率与`0.4`的召回率好；还是说`0.7`的查准率与`0.1`的召回率好？如果你最后这样坐下来思考，这回降低你的决策速度。思考到底哪些改变是有用的，应该被融入到你的算法中。

与此相反的是，如果我们有一个评估度量值，一个数字，能够告诉我们到底是算法1好还是算法2好，这能够帮助我们更快地决定哪一个算法更好，同时也能够更快地帮助我们评估不同的改动，哪些应该被融入进算法里面。那么我们怎样才能得到这个评估度量值呢？

你可能会去尝试的一件事情是计算一下查准率和召回率的平均值:

$$
平均值：\frac{P+R} {2}
$$

用**P**和**R**来表示查准率和召回率，你可以做的是计算它们的平均值，看一看哪个模型有最高的均值:

||查准率(P)|召回率(R)|平均值|
|:-:|:--:|:--:|:--:|
|算法1|0.5|0.4|0.45|
|算法2|0.7|0.1|0.4|
|算法3|0.02|1.0|0.51|

但是这可能并不是一个很好的解决办法，因为像我们之前的例子一样,如果我们的回归模型总是预测$y=1$，这么做你可能得到非常高的召回率得到非常低的查准率；相反地，如果你的模型总是预测$y=0$，就是说如果很少预测$y=1$，对应的设置了一个高临界值，最后你会得到非常高的查准率和非常低的召回率。

这两个极端情况一个有**非常高的临界值**，一个有**非常低的临界值**，它们中的任何一个都不是一个好的模型，我们可以通过非常低的查准率，或者非常低的召回率来判断这不是一个好模型。

在这里如果使用平均值的方式来计算，那么**算法3**的计算结果是最高的，但这并不是一个好模型，因为你总是预测$y=1$，这并不是一个有用的模型，因为它只能输出$y=1$。

所以，我们认为通过计算查准率和召回率平均值的方式来评估算法的好坏，不是一个好方法。

---

相反的，有一种结合查准率和召回率的另一种方式，叫做**F score**，公式如下：

$$
F\_{1}Score : 2\frac{PR} {P+R}
$$

通过**F score**方法我们可以得出：

||查准率(P)|召回率(R)|平均值|$F\_{1}Score$|
|:-:|:--:|:--:|:--:|:--:|
|算法1|0.5|0.4|0.45|0.444|
|算法2|0.7|0.1|0.4|0.175|
|算法3|0.02|1.0|0.51|0.0392|

可以看出**算法1**的**F score**最高，所以我们会选择使用**算法1**。

> **F score**也叫作$F\_{1}Score$，但通常人们叫它**F score**。

**F score**的定义会考虑一部分查准率和召回率的平均值，但是它会给查准率和召回率中较低的值更高的权重，因此你可以看到**F score**的分子是查准率和召回率的乘积，如果查准率等于0或者召回率等于0，**F score**也会等于0。因此它结合了查准率和召回率，对于一个较大的**F score**，查准率和召回率都必须较大。

我必须说明的是，有较多的公式可以结合查准率和召回率，**F score**公式只是 其中一个，但是出于历史原因和习惯问题，人们在机器学习中普遍使用**F score**。**F score**这个术语没有什么特别的意义，所以不要担心它到底为什么叫做**F score**或者$F\_{1}Score$。

**F score**给出了你所需要的有效方法，因为无论是查准率等于0，还是召回率等于0，它都会得到一个很低的**F score**。因此，如果要得到一个很高的**F score**，你的算法的查准率和召回率都要接近于1。具体地说，如果$P=0$或者$R=0$，你的**F score**也会等于0。

> 在这次的视频中，我们讲到了如何权衡查准率和召回率，以及我们如何变动临界值 来决定我们希望预测$y=1$还是$y=0$。比如我们需要一个$70\%$还是$90\%$置信度的临界值，或者别的，来预测$y=1$。
> 
> 通过变动临界值，你可以控制权衡查准率和召回率。
> 
> 之后我们讲到了**F score**，它权衡查准率和召回率，给了你一个评估度量值。当然，如果你的目标是自动选择临界值来决定你希望预测$y=1$还是$y=0$，那么一个比较理想的办法是试一试不同的临界值，然后评估这些不同的临界值在交叉检验集上进行测试，然后选择哪一个临界值能够在交叉检验集上得到最高的**F score**，这是自动选择临界值的较好办法。



