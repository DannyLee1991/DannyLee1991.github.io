<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>斯坦福机器学习课程 第二周 (2)多元线性回归分析 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="多特征量视频地址 在这段视频中，我们将开始介绍一种新的更为有效的线性回归形式，这种形式适用于多个变量，或者多特征量的情况。 比如说：在之前我们学习过的线性回归中，我们只有一个单一特征量：‘房屋面积x’，我们希望用这个特征量来预测房子的价格，这就是我们的假设 。  但是想象一下，如果我们不仅有房屋面积作为预测房屋价格的特征量或者变量，我们还知道卧室的数量楼层的数量以及房子的使用年限，这样就给了我们更">
<meta property="og:type" content="article">
<meta property="og:title" content="斯坦福机器学习课程 第二周 (2)多元线性回归分析">
<meta property="og:url" content="http://example.com/2016/07/29/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%8C%E5%91%A8%20(2)%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="多特征量视频地址 在这段视频中，我们将开始介绍一种新的更为有效的线性回归形式，这种形式适用于多个变量，或者多特征量的情况。 比如说：在之前我们学习过的线性回归中，我们只有一个单一特征量：‘房屋面积x’，我们希望用这个特征量来预测房子的价格，这就是我们的假设 。  但是想象一下，如果我们不仅有房屋面积作为预测房屋价格的特征量或者变量，我们还知道卧室的数量楼层的数量以及房子的使用年限，这样就给了我们更">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/16_07_29/001.png">
<meta property="og:image" content="http://example.com/img/16_07_29/002.png">
<meta property="og:image" content="http://example.com/img/16_07_29/003.png">
<meta property="og:image" content="http://example.com/img/16_07_29/004.png">
<meta property="og:image" content="http://example.com/img/16_07_29/005.png">
<meta property="og:image" content="http://example.com/img/16_07_29/006.png">
<meta property="og:image" content="http://example.com/img/16_07_29/007.png">
<meta property="og:image" content="http://example.com/img/16_07_29/008.png">
<meta property="og:image" content="http://example.com/img/16_07_29/009.png">
<meta property="og:image" content="http://example.com/img/16_07_29/010.gif">
<meta property="og:image" content="http://example.com/img/16_07_29/011.png">
<meta property="og:image" content="http://example.com/img/16_07_29/012.png">
<meta property="og:image" content="http://example.com/img/16_07_29/013.png">
<meta property="og:image" content="http://example.com/img/16_07_29/014.png">
<meta property="og:image" content="http://example.com/img/16_07_29/015.png">
<meta property="og:image" content="http://example.com/img/16_07_29/016.png">
<meta property="og:image" content="http://example.com/img/16_07_29/017.png">
<meta property="og:image" content="http://example.com/img/16_07_29/018.png">
<meta property="og:image" content="http://example.com/img/16_07_29/019.png">
<meta property="og:image" content="http://example.com/img/16_07_29/020.png">
<meta property="og:image" content="http://example.com/img/16_07_29/021.png">
<meta property="og:image" content="http://example.com/img/16_07_29/022.png">
<meta property="og:image" content="http://example.com/img/16_07_29/023.png">
<meta property="article:published_time" content="2016-07-29T01:26:00.000Z">
<meta property="article:modified_time" content="2021-02-08T08:48:41.861Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="斯坦福课程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/16_07_29/001.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-斯坦福机器学习课程 第二周 (2)多元线性回归分析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2016/07/29/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%8C%E5%91%A8%20(2)%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/" class="article-date">
  <time class="dt-published" datetime="2016-07-29T01:26:00.000Z" itemprop="datePublished">2016-07-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      斯坦福机器学习课程 第二周 (2)多元线性回归分析
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="多特征量"><a href="#多特征量" class="headerlink" title="多特征量"></a>多特征量</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features">视频地址</a></p>
<p>在这段视频中，我们将开始介绍一种新的更为有效的线性回归形式，这种形式适用于多个变量，或者多特征量的情况。</p>
<p>比如说：在之前我们学习过的线性回归中，我们只有一个单一特征量：‘房屋面积x’，我们希望用这个特征量来预测房子的价格，这就是我们的假设 。</p>
<p><img src="/img/16_07_29/001.png"></p>
<p>但是想象一下，如果我们不仅有房屋面积作为预测房屋价格的特征量或者变量，我们还知道卧室的数量楼层的数量以及房子的使用年限，这样就给了我们更多可以用来预测房屋价格的信息:</p>
<p><img src="/img/16_07_29/002.png"></p>
<h3 id="多特征量的表示"><a href="#多特征量的表示" class="headerlink" title="多特征量的表示"></a>多特征量的表示</h3><p>先简单介绍一下记法：</p>
<ul>
<li>$n$ = 来表示特征量的数目</li>
</ul>
<p>这个例子中有4个特征量，因此，这里n=4</p>
<ul>
<li>$x^{(i)}$ = 第$i$个训练样本的输入特征值</li>
</ul>
<p>这个例子中$x^{(2)}$就是表示第二个训练样本的特征向量：</p>
<p>$$<br>x^{(2)}<br>=<br>\begin{bmatrix}<br>1416 \\<br>3 \\<br>2 \\<br>40<br>\end{bmatrix}<br>$$</p>
<ul>
<li>$x_j^{(i)}$ = 第$i$个训练样本的第$j$个特征量 </li>
</ul>
<p>这个例子中，$x_3^{(2)}$代表着第2个训练样本里的第3个特征量。这里值是2。</p>
<h3 id="多特征量的假设函数"><a href="#多特征量的假设函数" class="headerlink" title="多特征量的假设函数"></a>多特征量的假设函数</h3><p>这是我们之前使用的假设形式：</p>
<p>$$<br>h_{θ}(x) = θ_0 + θ_1 * x<br>$$</p>
<p>其中$x$就是我们唯一的特征量，但现在我们有了多个特征量我们就不能再 使用这种简单的表示方式了。取而代之的，我们将把线性回归的假设改成这样：</p>
<p>$$<br>h_{θ}(x) = θ_{0} + θ_{1} x_{1} + θ_{2} x_{2} + … + θ_{n} x_{n}<br>$$</p>
<p>接下来，我要来介绍简化这个等式的表示方式：</p>
<p><strong>为了表示方便,我要将$x_{0}$的值设为1。</strong>具体而言这意味着对于第$i$个样本，都有一个向量$x^{(i)}$，并且$x^{(i)}_{0} = 1$。你可以认为我们定义了一个额外的第0个特征量。因此，我过去有$n$个特征量 因为我们有$x_{1},x_{2}…x_{n}$，由于我另外定义了第0个特征向量$x_{0}$，并且它的取值总是1，所以我现在的特征向量$x$是一个从0开始标记的$n+1$维的向量：</p>
<p>$$<br>x<br>=<br>\begin{bmatrix}<br>x_{0} \\<br>x_{1} \\<br>x_{2} \\<br>… \\<br>x_{n}<br>\end{bmatrix}<br>\in<br>\mathbb{R}^{n+1}<br>$$</p>
<p>但我要从0开始标记，同时我也想把我的参数都看做一个向量，所以我们的参数就是：</p>
<p>$$<br>θ<br>=<br>\begin{bmatrix}<br>θ_{0} \\<br>θ_{1} \\<br>θ_{2} \\<br>… \\<br>θ_{n}<br>\end{bmatrix}<br>\in<br>\mathbb{R}^{n+1}<br>$$</p>
<p>所以我的假设，现在可以写成：</p>
<p>$$<br>h_{θ}(x) = θ_{0} x_{0} + θ_{1} x_{1} + θ_{2} x_{2} + … + θ_{n} x_{n}<br>$$</p>
<blockquote>
<p>请注意，这里$x_{0} = 1$</p>
</blockquote>
<p>下面我要把这种形式假设等式写成:</p>
<p>$$<br>h_{θ}(x) = θ^{T} x<br>$$</p>
<p>其中$θ^{T}$是$θ$向量的转置，是一个$(n+1)×1$维的矩阵，也被称为行向量：</p>
<p>$$<br>\underbrace{<br>\begin{bmatrix}<br>θ_{0} θ_{1} θ_{2} … θ_{n}<br>\end{bmatrix}<br>}_{θ^{T}}<br>$$</p>
<p>因此这个假设函数的完整推导过程如下:</p>
<p>$$<br>\begin{align*}<br>h_{θ}(x)<br>&amp;=<br>θ_{0} x_{0} + θ_{1} x_{1} + θ_{2} x_{2} + … + θ_{n} x_{n}\\<br>&amp;=<br>\begin{bmatrix}<br>θ_{0} θ_{1} θ_{2} … θ_{n}<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_{0} \\<br>x_{1} \\<br>x_{2} \\<br>… \\<br>x_{n}<br>\end{bmatrix}\\<br>&amp;=<br>θ^{T} x<br>\end{align*}<br>$$</p>
<p>这就为我们表示假设的更加便利的形式：</p>
<p>$$<br>h_{θ}(x) = θ^{T} x<br>$$</p>
<p>即用参数向量$θ$以及特征向量$X$的内积来表示。</p>
<p>这样的表示方法让我们可以以这种紧凑的形式写出假设，这就是多特征量情况下的假设形式。起另一个名字：就是<strong>所谓的多元线性回归</strong>。</p>
<h2 id="多特征的梯度下降"><a href="#多特征的梯度下降" class="headerlink" title="多特征的梯度下降"></a>多特征的梯度下降</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables">视频地址</a></p>
<p>在之前的视频中，我们谈到了一种线性回归的假设形式，这是一种有多特征或者是多变量的形式。在本节视频中我们将会谈到如何找到满足这一假设的参数，尤其是如何使用梯度下降法来解决多特征的线性回归问题。</p>
<p>为尽快让你理解，假设现有多元线性回归并约定$x_{0}=1$。</p>
<p>**假设函数(Hypothesis)**如下：</p>
<p>$$<br>h_{θ}(x) = θ^{T}x = θ_{0} x_{0} + θ_{1} x_{1} + θ_{2} x_{2} + … + θ_{n} x_{n}<br>$$</p>
<p>该模型的参数是从$θ_{0}$到$θ_{n}$，不要认为这是$n+1$个单独的参数。你可以把这$n+1$个$θ$参数想象成一个$n+1$维的向量$θ$。</p>
<p>**参数(Parameters)**如下：</p>
<p>$$<br>θ_{0} θ_{1} θ_{2} … θ_{n}<br>$$</p>
<p>我们的代价函数是从$θ_{0}$到$θ_{n}$的函数$J$并给出了误差项平方的和。但同样地，不要把函数$J$想成是一个关于$n+1$个自变量的函数而是看成带有一个$n+1$维向量的函数:</p>
<p>**代价函数(Cost function)**如下：</p>
<p>$$<br>J(θ_{0},θ_{1},…,θ_{n}) =<br>\frac{1}{2m}<br>\sum_{i=1}^{m}<br>(h_{θ}(x^{(i)}) - y^{(i)})^{2}<br>$$</p>
<p>**梯度下降法(Gradient descent)**：</p>
<p><img src="/img/16_07_29/003.png"></p>
<p>我们将会不停地用$θ_{j}$减去$α$倍的导数项来替代$θ_{j}$。</p>
<table>
<thead>
<tr>
<th align="center">当特征$n=1$时，梯度下降的情况：</th>
<th align="center">当特征$n&gt;=1$时，梯度下降的情况：</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="/img/16_07_29/004.png"></td>
<td align="center"><img src="/img/16_07_29/005.png"></td>
</tr>
</tbody></table>
<p>下图中，为了更好的理解这个梯度下降更新规则，蓝圈圈起来的部分其实是等价的：</p>
<p><img src="/img/16_07_29/006.png"></p>
<p>最后，我想让你明白为什么新旧两种算法实际上是一回事儿。</p>
<p>考虑这样一个情况：有两个或以上个数的特征，同时我们有对$θ_{1}$、$θ_{2}$、$θ_{3}$的三条更新规则(当然可能还有其它参数)。如果你观察$θ_{0}$的更新规则，你会发现这跟之前$n=1$的情况相同。它们之所以是等价的，是因为在我们的标记约定里有$x^{i}_{0}=1$。</p>
<p><img src="/img/16_07_29/007.png"></p>
<p><img src="/img/16_07_29/008.png"></p>
<p>从上图可以看出，对于$n &gt;= 1$的梯度下降规则，其实是一个通用规则。请务必看懂上图，确保理解之后再继续学习！</p>
<h2 id="梯度下降实用技巧-1：特征缩放-Feature-Scaling-以及均值归一化-mean-normalization"><a href="#梯度下降实用技巧-1：特征缩放-Feature-Scaling-以及均值归一化-mean-normalization" class="headerlink" title="梯度下降实用技巧 1：特征缩放(Feature Scaling)以及均值归一化(mean normalization)"></a>梯度下降实用技巧 1：特征缩放(Feature Scaling)以及均值归一化(mean normalization)</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling">视频地址</a></p>
<p>在这段视频以及下一段视频中，我想告诉你一些关于梯度下降运算中的实用技巧。</p>
<p>在这段视频中我会告诉你两个称为<strong>特征缩放(feature scaling)**以及</strong>均值归一化(mean normalization)**的方法。</p>
<p>这两个方法都是为了<strong>保证特征的取值在合适的范围内</strong>的。</p>
<h3 id="特征缩放-feature-scaling"><a href="#特征缩放-feature-scaling" class="headerlink" title="特征缩放(feature scaling)"></a><strong>特征缩放(feature scaling)</strong></h3><p>其中，<strong>特征缩放(feature scaling)**大致的思路是这样的：</strong>梯度下降算法中，在有多个特征的情况下，如果你能确保这些不同的特征都处在一个相近的范围，这样梯度下降法就能更快地收敛。**</p>
<p>举个例子来说明：</p>
<p>$$<br>x_{1} = size(0-2000 feet^{2})\\<br>x_{2} = 卧室的数量(1-5)<br>$$</p>
<p>假如你有一个具有两个特征的问题，其中$x_{1}$是房屋面积大小，它的取值在0到2000之间；$x_{2}$是卧室的数量，可能这个值的取值范围在1到5之间。其代价函数$J(θ)$是一个关于参数$θ_{0}$，$θ_{1}$和$θ_{2}$的函数。但这里我们暂时不考虑$θ_{0}$并假想一个函数的变量只有$θ_{1}$和$θ_{2}$。</p>
<p>如果$x_{1}$的取值范围远远大于$x_{2}$的取值范围的话，那么最终画出来的代价函数$J(θ)$的轮廓图就会呈现出这样一种非常偏斜并且椭圆的形状:</p>
<p><img src="/img/16_07_29/009.png"></p>
<p>如果你用这个代价函数来运行梯度下降的话，你要得到梯度值最终可能需要花很长一段时间，并且可能会来回波动，然后会经过很长时间最终才收敛到全局最小值。</p>
<p><img src="/img/16_07_29/010.gif"></p>
<p>事实上如果这些轮廓再被放大一些的话，如果你画的再夸张一些把它画的更细更长，那么可能情况会更糟糕，梯度下降的过程可能更加缓慢，需要花更长的时间反复来回振荡，最终才找到一条正确通往全局最小值的路。</p>
<p>在这样的情况下一种有效的方法是**进行特征缩放(feature scaling)**。</p>
<p>具体来说把特征$x$定义为：</p>
<p>$$<br>x_{1} = \frac{size(feet^2)}{2000}<br>$$</p>
<p>$$<br>x_{2} = \frac{卧室的数量}{5}<br>$$</p>
<p>通过这样的变化，表示代价函数$J(θ)$的轮廓图的形状就会变得偏移没那么严重，可能看起来更圆一些了。</p>
<p><img src="/img/16_07_29/011.png"></p>
<p>如果你用这样的代价函数来执行梯度下降的话，那么可以从数学上来证明梯度下降算法将会找到一条更捷径的路径通向全局最小，而不是像刚才那样  沿着一条让人摸不着头脑的路径，来找到全局最小值。</p>
<p>因此在这个例子中，通过特征缩放，我们最终得到的两个特征$x_{1}$和 $x_{2}$都在0和1之间，这样你得到的梯度下降算法就会更快地收敛。</p>
<p>更一般地，<strong>我们执行特征缩放时，我们通常的目的是将特征的取值约束到$-1$到$+1$的范围内</strong>。其中，特征$x_{0}$总是等于1，因此这已经是在这个范围内了，但对于其他的特征，你可能需要通过除以不同的数来让它们处于同一范围内。</p>
<p>$-1$和$+1$这两个数字并不是太重要，所以如果你有一个特征$x_{1}$它的取值在<code>0 ~ 3</code>之间，这没问题 如果你有另外一个特征取值在<code>-2 ~ +0.5</code>之间，这也没什么关系，因为这也非常接近<code>-1 ~ +1</code>的范围。</p>
<p>但如果你有另一个特征$x_{3}$，假如它的范围在<code>-100 ~ +100</code>之间，那么这个范围跟<code>-1 ~ +1</code>就有很大不同了。所以这可能是一个不那么好的特征。类似地，如果你的特征在一个非常非常小的范围内，比如另外一个特征$x_{4}$，它的范围在<code>-0.0001 ~ +0.0001</code>之间,那么这同样是一个比<code>-1 ~ +1</code>小得多的范围，因此我同样会认为这个特征也不太好。所以可能你认可的范围，也许可以大于或者小于<code>-1 ~ +1</code>，但是也别太大或太小，只要与<code>-1 ~ +1</code>范围偏差不多就可以接受。</p>
<p>因此，总的来说不用过于担心你的特征是否在完全相同的范围或区间内，但是只要它们足够接近的话，梯度下降法就会正常地工作。</p>
<h3 id="均值归一化-mean-normalization"><a href="#均值归一化-mean-normalization" class="headerlink" title="均值归一化(mean normalization)"></a><strong>均值归一化(mean normalization)</strong></h3><p>除了在特征缩放中将特征除以最大值以外，有时候我们也会进行一个称为**均值归一化(mean normalization)**的工作。</p>
<p>具体做法就是：如果你有一个特征$x_{i}$你就用$x_{i} - μ_{i}$来替换。这样做的目的是为了<strong>让你的特征值具有为0的平均值</strong>。很明显 我们不需要把这一步应用到$x_{0}$中，因为$x_{0}$总是等于1的，所以它不可能有为0的的平均值。</p>
<p>但是对其他的特征来说，比如房子的大小取值介于<code>0 ~ 2000</code>，并且假如房子面积的平均值是等于1000的，那么你可以用这个公式 </p>
<p>$$<br>x_{1} = \frac{size - 1000}{2000}<br>$$</p>
<p>类似地，如果你的房子有五间卧室，并且平均一套房子有两间卧室，那么你可以使用这个公式来归一化你的第二个特征$x_{2}$：</p>
<p>$$<br>x_{2} = \frac{卧室数 - 2}{5}<br>$$</p>
<p>在这两种情况下你可以算出新的特征$x_{1}$和$x_{2}$，它们的范围可以在<code>-0.5 ~ +0.5</code>之间，当然这肯定不对，$x_{2}$的值实际上肯定会大于0.5。更一般的规律是用:</p>
<p>$$<br>\frac{x_{n} - μ_{n}}{S_{n}}<br>$$</p>
<p>来替换原来的特征$x_{n}$。其中定义$μ_{n}$的意思是在训练集中特征$x_{n}$的平均值。而$S_{n}$是该特征值的范围（最大值减去最小值）。</p>
<blockquote>
<p>最后直的一提的是：特征缩放其实并不需要太精确，其目的只是为了让梯度下降能够运行得更快一点，让梯度下降收敛所需的循环次数更少一些而已。</p>
</blockquote>
<h2 id="梯度下降实用技巧-2：学习速率-Learning-Rate"><a href="#梯度下降实用技巧-2：学习速率-Learning-Rate" class="headerlink" title="梯度下降实用技巧 2：学习速率(Learning Rate)"></a>梯度下降实用技巧 2：学习速率(Learning Rate)</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate">视频地址</a></p>
<p>在本段视频中，我将集中讨论**学习率$α$**。具体来说这是梯度下降算法的更新规则。</p>
<p>$$<br>θ_{j} := θ_{j} -α \frac{∂}{∂θ_{0}} J(θ)<br>$$</p>
<p>这里我想要告诉大家两点： </p>
<ul>
<li>如何调试(Debugging) : 也就是我认为应该如何确定梯度下降是正常工作的。</li>
<li>如何选择学习率$α$ : 如何选择这个参数才能保证梯度下降正常工作。</li>
</ul>
<h3 id="收敛的判断"><a href="#收敛的判断" class="headerlink" title="收敛的判断"></a>收敛的判断</h3><p>梯度下降算法所做的事情就是为你找到一个$θ$值，并希望它能够最小化代价函数$J(θ)$。</p>
<p>我通常会在梯度下降算法运行时，绘出代价函数$J(θ)$的值。这里的$x$轴是表示梯度下降算法的迭代步数：</p>
<p><img src="/img/16_07_29/012.png"></p>
<blockquote>
<p>**注意:**这里的$x$轴是迭代步数，在我们以前看到的$J(θ)$曲线中$x$轴，曾经用来表示参数$θ$但这里不是。</p>
</blockquote>
<p>具体来说，这一点的含义是这样的：比如，当我运行完100步的梯度下降迭代之后，我将得到一个$θ$值，根据这个$θ$值，我将算出代价函数$J(θ)$的值。而这个点的垂直高度就代表梯度下降算法100步迭代之后得到的$θ$算出的$J(θ)$值：</p>
<p><img src="/img/16_07_29/013.png"></p>
<p>而这个点则是梯度下降算法迭代200次之后得到的$θ$算出的$J(θ)$值：</p>
<p><img src="/img/16_07_29/014.png"></p>
<p>所以这条曲线显示的是梯度下降算法迭代过程中代价函数$J(θ)$的值。</p>
<p><strong>如果梯度下降算法正常工作，那么每一步迭代之后$J(θ)$都应该下降。</strong></p>
<p>这条曲线中，当迭代达到300步到400步之间时（如下图所示），看起来$J(θ)$并没有下降多少。也就是说在400步迭代的时候，梯度下降算法基本上已经收敛了，因为代价函数并没有继续下降：</p>
<p><img src="/img/16_07_29/015.png"></p>
<p>所以说，看这条曲线可以帮助你判断梯度下降算法是否已经收敛。</p>
<blockquote>
<p>对于每一个特定的问题，梯度下降算法所需的迭代次数可以相差很大。也许对于某一个问题，梯度下降算法只需要30步迭代就可以收敛，然而换一个问题，也许梯度下降算法就需要3000步迭代，对于另一个机器学习问题则可能需要三百万步迭代。</p>
</blockquote>
<p>实际上，我们很难提前判断梯度下降算法需要多少步迭代才能收敛。通常我们需要画出这类<strong>代价函数随迭代步数数增加的变化曲线</strong>，通常我会通过看这种曲线来试着判断梯度下降算法是否已经收敛。另外也可以进行一些自动的<strong>收敛测试</strong>（也就是说用一种算法来告诉你梯度下降算法是否已经收敛）。</p>
<p>自动收敛测试一个非常典型的例子是：如果代价函数$J(θ)$的下降小于一个很小的值$ε$那么就认为已经收敛。比如可以选择$10^{-3}$作为阈值$ε$。但通常要选择一个合适的阈值$ε$是相当困难的。</p>
<p><strong>因此，为了检查梯度下降算法是否收敛,我实际上还是通过看上边的这条曲线图，而不是依靠自动收敛测试。</strong></p>
<h3 id="判断梯度下降算法是否正常工作"><a href="#判断梯度下降算法是否正常工作" class="headerlink" title="判断梯度下降算法是否正常工作"></a>判断梯度下降算法是否正常工作</h3><p>此外这种曲线图也可以在算法没有正常工作时提前警告你。具体地说如果代价函数$J(θ)$随迭代步数的变化曲线是这个样子:</p>
<p><img src="/img/16_07_29/016.png"></p>
<p>很明确的表示梯度下降算法没有正常工作，而这样的曲线图通常意味着你应该使用较小的学习率$α$。如果$J(θ)$在上升，那么最常见的原因是你在最小化一个函数时，如果你的学习率太大，梯度下降算法可能将冲过最小值达到最小值另一侧的更大的一个点。下次迭代时，也可能再次冲过最小值，达到更大的一个点，然后一直这样下去。</p>
<p><img src="/img/16_07_29/017.png"></p>
<p>所以，如果你看到这样一个曲线图，通常的解决方法是使用较小的$α$值，当然也要确保，你的代码中没有错误。</p>
<p>同样的，有时你可能看到这种形状的$J(θ)$曲线:</p>
<p><img src="/img/16_07_29/018.png"></p>
<p>它先下降，然后上升，接着又下降，然后又上升，然后再次下降，再次上升，如此往复。而解决这种情况的方法通常同样是选择较小$α$值。</p>
<p>我不打算证明这一点，但对于我们讨论的线性回归可以很容易从数学上证明：只要学习率足够小，那么每次迭代之后，代价函数$J(θ)$都会下降。因此如果代价函数没有下降，那可能意味着学习率过大，这时你就应该尝试一个较小的学习率。当然，你也不希望学习度太小，因为如果这样，那么梯度下降算法可能收敛得很慢，你需要迭代很多次才能到达最低点。因此如果学习率$α$太小，梯度下降算法的收敛将会很缓慢。</p>
<p><strong>总结：</strong></p>
<ul>
<li><strong>如果学习率$α$太小，你会遇到收敛速度慢的问题。</strong></li>
<li><strong>如果学习率$α$太大，代价函数$J(θ)$可能不会在每次迭代都下降，甚至可能不收敛。</strong></li>
</ul>
<blockquote>
<p>在某些情况下，如果学习率$α$过大，也可能出现收敛缓慢的问题。</p>
</blockquote>
<h3 id="如何选择-α"><a href="#如何选择-α" class="headerlink" title="如何选择$α$"></a>如何选择$α$</h3><p>为了调试，当我运行梯度下降算法时，我通常会尝试一系列$α$值，比如0.001， 0.01， 0.1， 1 … 这里每隔10倍取一个值，然后对于这些不同的$α$值绘制$J(θ)$随迭代步数变化的曲线，然后选择看上去使得$J(θ)$快速下降的一个$α$值。</p>
<p>事实上在为梯度下降算法选择合适的学习率时，我大致是按3的倍数来取值的。例如：0.001， 0.003， 0、01，0.03，0.1，0.3，1…</p>
<p>所以我会尝试一系列$α$值，直到我找到不能再小了的值，同时找到另一个不能再大的值，然后我尽量挑选其中最大的那个$α$值，或者一个比最大值略小一些的合理的值。当我做了以上工作时，我通常就可以得到一个不错的学习率。</p>
<h2 id="特征的选择和多项式回归"><a href="#特征的选择和多项式回归" class="headerlink" title="特征的选择和多项式回归"></a>特征的选择和多项式回归</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Rqgfz/features-and-polynomial-regression">视频地址</a></p>
<p>你现在了解了多变量的线性回归，在本节中我想告诉你一些用来选择特征的方法，以及如何得到不同的学习算法。当选择了合适的特征后，这些算法往往是非常有效的。另外，我也想给你们讲一讲多项式回归，它使得你们能够使用，线性回归的方法来拟合非常复杂的函数，甚至是非线性函数。</p>
<h3 id="特征的选择"><a href="#特征的选择" class="headerlink" title="特征的选择"></a>特征的选择</h3><p>以预测房价为例，假设你有两个特征，分别是房子临街的宽度和垂直宽度。这就是我们想要卖出的房子的图片：</p>
<p><img src="/img/16_07_29/019.png"></p>
<p>临街宽度其实就是拥有的土地的宽度，而纵向深度就是你的房子的深度。你可能会建立一个像这样的线性回归模型：</p>
<p><strong>Housing prices prediction</strong></p>
<p>$$<br>h_{θ}(x) = Θ_{0} + Θ_{1} × frontage + Θ_{2} × depth<br>$$</p>
<p>其中临街宽度是你的第一个特征$frontage$，纵深是你的第二个特征$depth$。但当我们在运用线性回归时，你不一定非要直接用给出的$frontage$和$depth$作为特征，其实你可以自己创造新的特征。因此，如果我要预测房子的价格，我真正要需做的也许是确定真正能够决定我房子大小，或者说我土地大小的因素是什么。因此，我可能会创造一个新的特征，我称之为$x$。它是临街宽度与纵深的乘积：</p>
<p>$$<br>x = frontage × depth<br>$$</p>
<p>$$<br>h_{θ}(x) = Θ_{0} + Θ_{1}x<br>$$</p>
<p>这得到的就是我拥有的土地的面积。然后我可以把假设选择为使其只使用一个特征，也就是我的土地的面积。</p>
<p>由于矩形面积的计算方法是矩形长和宽相乘，因此这取决于你从什么样的角度去审视一个特定的问题，而不是直接去使用临街宽度和纵深，这两个我们只是碰巧在开始时使用的特征。有时通过定义新的特征，你确实会得到一个更好的模型。</p>
<h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><p>与选择特征的想法密切相关的一个概念，被称为**多项式回归(polynomial regression)**。</p>
<p>比方说你有这样一个住房价格的数据集：</p>
<p><img src="/img/16_07_29/020.png"></p>
<p>为了拟合它，可能会有多个不同的模型供选择。其中一个你可以选择的是像这样的二次模型：</p>
<p>$$<br>θ_{0} + θ_{1}x + θ_{2}x^{2}<br>$$</p>
<p>因为直线似乎并不能很好地拟合这些数据，因此也许你会想到用这样的二次模型去拟合数据。</p>
<p>但是你可能会觉得二次函数的模型并不好用，因为一个二次函数最终会降回来。而我们并不认为房子的价格在高到一定程度后会下降回来：</p>
<p><img src="/img/16_07_29/021.png"></p>
<p>因此也许我们会选择一个不同的多项式模型，并转而选择使用一个三次函数：</p>
<p>$$<br>θ_{0} + θ_{1}x + θ_{2}x^{2} + θ_{3}x^{3}<br>$$</p>
<p>我们用这个三次函数进行拟合，我们可能得到这样的模型（绿线部分）：</p>
<p><img src="/img/16_07_29/022.png"></p>
<p>也许这条绿色的线对这个数据集拟合得更好，因为它不会在最后下降回来。</p>
<p>那么，我们到底应该如何将模型与我们的数据进行拟合呢？使用多元线性回归的方法，我们可以通过将我们的算法做一个非常简单的修改来实现它：</p>
<p>$$<br>\begin{align*}<br>h_{θ}(x) &amp;= θ_{0} + θ_{1}x_{1} + θ_{2}x_{2} + θ_{3}x_{3}<br>\\&amp;=<br>θ_{0} + θ_{1}(size) + θ_{2}(size)^{2} + θ_{3}(size)^{3}<br>\end{align*}<br>$$</p>
<p>$$<br>x_{1} = (size) \\<br>x_{2} = (size)^{2} \\<br>x_{3} = (size)^{3} \\<br>$$</p>
<p>我还想再说一件事，如果你像这样选择特征，那么特征的归一化就变得更重要了。因此，如果房子的大小范围在<code>1</code>到<code>1000</code>之间，那么房子面积的平方的范围就是<code>1</code>到<code>10000000</code>（也就是$1000^{2}$），而你的第三个特征$x_{3}$ 它是房子面积的立方,范围会扩大到1到$10^{9}$。这三个特征的范围有很大的不同，因此，如果你使用梯度下降法，应用特征值的归一化是非常重要的，这样才能将他们的值的范围变得具有可比性。</p>
<p>最后一个例子，除了建立一个三次模型以外，你也许有其他的选择特征的方法。这里有很多可能的选项，但是给你另外一个合理的选择的例子：</p>
<p>$$<br>h_{θ}(x) = θ_{0} + θ_{1}(size) + θ_{2}\sqrt{(size)}<br>$$</p>
<p>这样的一种函数曲线看起来趋势是上升的，但慢慢变得平缓一些，而且永远不会下降回来：</p>
<p><img src="/img/16_07_29/023.png"></p>
<p>在这段视频中，我们探讨了多项式回归。也就是如何将一个多项式如一个二次函数或一个三次函数拟合到你的数据上。除了这个方面我们还讨论了：在使用特征时的选择性。例如：我们不使用房屋的临街宽度和纵深，也许你可以把它们乘在一起从而得到房子的土地面积这个特征。实际上这似乎有点难以抉择，这里有这么多不同的特征选择，我该如何决定使用什么特征呢？</p>
<p>在之后的课程中我们将探讨一些算法，它们能够<strong>自动选择要使用什么特征</strong>。因此你可以使用一个算法观察给出的数据，并自动为你选择到底应该选择一个二次函数，或者一个三次函数，还是别的函数。但是，在我们学到那种算法之前，现在我希望你知道，你需要选择使用什么特征并且通过设计不同的特征，你能够用更复杂的函数去拟合你的数据，而不是只用一条直线去拟合。特别是你也可以使用多项式函数，有时候通过采取适当的角度来观察 特征就可以得到一个更符合你的数据的模型。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2016/07/29/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%8C%E5%91%A8%20(2)%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/" data-id="ckkwc438c0093pas916ki2ujp" data-title="斯坦福机器学习课程 第二周 (2)多元线性回归分析" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B/" rel="tag">斯坦福课程</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/08/12/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%8C%E5%91%A8%20(3)%E5%8F%82%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%E5%88%86%E6%9E%90/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          斯坦福机器学习课程 第二周 (3)参数的计算分析
        
      </div>
    </a>
  
  
    <a href="/2016/07/26/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%8C%E5%91%A8%20(1)%E7%BC%96%E7%A8%8B%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE-Octave:MATLAB/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">斯坦福机器学习课程 第二周 (1)编程环境设置-Octave/MATLAB</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/%E5%B7%A5%E5%85%B7/">工具</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/R/">R</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/django/">django</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/gradle/">gradle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/other/">other</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E5%AD%A6%E4%B9%A0/">工具学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/%E5%85%AC%E5%BC%80%E8%AF%BE/">公开课</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/">Tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/cs231n/">cs231n</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">线性代数</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-QI/" rel="tag">AI-QI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Android/" rel="tag">Android</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gradle/" rel="tag">gradle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kaggle/" rel="tag">kaggle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag">人工智能</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" rel="tag">傅里叶变换</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%85%B6%E4%BB%96/" rel="tag">其他</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E5%85%B7%E5%AD%A6%E4%B9%A0/" rel="tag">工具学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag">数学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%85%AC%E5%BC%80%E8%AF%BE/" rel="tag">斯坦福大学公开课</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B/" rel="tag">斯坦福课程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" rel="tag">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BF%BB%E8%AF%91/" rel="tag">翻译</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/" rel="tag">论文翻译</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag">读书笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI-QI/" style="font-size: 10px;">AI-QI</a> <a href="/tags/Android/" style="font-size: 18px;">Android</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/R/" style="font-size: 11px;">R</a> <a href="/tags/Tensorflow/" style="font-size: 17px;">Tensorflow</a> <a href="/tags/django/" style="font-size: 11px;">django</a> <a href="/tags/gradle/" style="font-size: 16px;">gradle</a> <a href="/tags/java/" style="font-size: 11px;">java</a> <a href="/tags/kaggle/" style="font-size: 10px;">kaggle</a> <a href="/tags/linux/" style="font-size: 13px;">linux</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/scala/" style="font-size: 10px;">scala</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 10px;">人工智能</a> <a href="/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" style="font-size: 11px;">傅里叶变换</a> <a href="/tags/%E5%85%B6%E4%BB%96/" style="font-size: 10px;">其他</a> <a href="/tags/%E5%B7%A5%E5%85%B7%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">工具学习</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 12px;">数学</a> <a href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%85%AC%E5%BC%80%E8%AF%BE/" style="font-size: 11px;">斯坦福大学公开课</a> <a href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B/" style="font-size: 19px;">斯坦福课程</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">神经网络</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 16px;">线性代数</a> <a href="/tags/%E7%BF%BB%E8%AF%91/" style="font-size: 10px;">翻译</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/" style="font-size: 10px;">论文翻译</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 15px;">设计模式</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 11px;">读书笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/11/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/">隐马尔科夫模型</a>
          </li>
        
          <li>
            <a href="/2018/10/29/%E3%80%90%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E3%80%9102-%E5%B0%86%E4%B8%80%E8%88%AC%E5%91%A8%E6%9C%9F%E5%87%BD%E6%95%B0%E8%A1%A8%E7%A4%BA%E4%B8%BA%E7%AE%80%E5%8D%95%E5%91%A8%E6%9C%9F/">【傅里叶变换及其应用讲义】第一章 傅里叶级数</a>
          </li>
        
          <li>
            <a href="/2018/10/27/%E3%80%90%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E3%80%9101-%E5%91%A8%E6%9C%9F%E6%80%A7%EF%BC%8C%E4%B8%89%E8%A7%92%E5%87%BD%E6%95%B0%E8%A1%A8%E7%A4%BA%E5%A4%8D%E6%9D%82%E5%87%BD%E6%95%B0/">【傅里叶变换及其应用】01-周期性，三角函数表示复杂函数</a>
          </li>
        
          <li>
            <a href="/2018/05/11/NumPy%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">NumPy入门教程</a>
          </li>
        
          <li>
            <a href="/2018/05/03/Docker%E5%85%A5%E9%97%A8Part6-%E5%8F%91%E5%B8%83%E4%BD%A0%E7%9A%84app/">Docker入门Part6-发布你的app</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>