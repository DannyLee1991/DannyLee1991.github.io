<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="愿你的努力终取得成果">
<meta property="og:type" content="website">
<meta property="og:title" content="圣巢">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="圣巢">
<meta property="og:description" content="愿你的努力终取得成果">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="DannyLee">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>圣巢</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8d12c5d1bc83189640335b2363468a74";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">圣巢</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/06/13/%E3%80%90%E7%BF%BB%E8%AF%91%E3%80%91%E8%A7%86%E8%A7%89%E4%BF%A1%E6%81%AF%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/13/%E3%80%90%E7%BF%BB%E8%AF%91%E3%80%91%E8%A7%86%E8%A7%89%E4%BF%A1%E6%81%AF%E8%AE%BA/" class="post-title-link" itemprop="url">【翻译】图解信息论</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-06-13 12:23:00" itemprop="dateCreated datePublished" datetime="2017-06-13T12:23:00+00:00">2017-06-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-09-Visual-Information/">原文地址</a></li>
</ul>
<p>我很喜欢那种获得一种新的方式来思考我们的世界时的那种感觉。尤其是在一些模糊的想法变成一个具体的概念的时候。<strong>信息论</strong>就是这样一种把模糊的概念具象化的典型的例子。</p>
<p>信息理论为我们描述很多事情提供了准确的语言。关于某个问题我到底有多不确定？问题A的答案会告诉我们多少关于问题B的答案的信息？一组信息和另一组信息的相似度如何衡量？在我小的时候，我就已经形成了一些关于这些问题的模糊的想法，但是信息论将我的这些想法形成了精确的强大的理论。这些理论有着巨大的应用范围，例如数据压缩、量子物理学、机器学习以及其他更广泛的领域。</p>
<p>不过，信息论看起来似乎有些不容易理解，但我不认为信息论应该是这样难以理解的。事实上，信息论中许多核心理论都可以完全用图解的方式来解释！</p>
<h2 id="可视化概率分布"><a href="#可视化概率分布" class="headerlink" title="可视化概率分布"></a>可视化概率分布</h2><p>在我们深入信息理论之前，让我们考虑一下如何可视化简单的概率分布。</p>
<p>我住在加利福尼亚州。有些时候这里是下雨天，但大多数时间里这里都是晴天！我们假设有75%的时间是晴天。我们很容易就可以将这些信息可视化的展示出来：</p>
<p><img src="/img/17_06_13/001.png" width=100 height=100 /></p>
<p>大多数的时间，我都穿T恤衫，但在某些时间，我会穿外套。那么假设我38%的时间里是穿外套的。可视化这个信息也很容易：</p>
<p><img src="/img/17_06_13/002.png" width=100 height=100 /></p>
<p>如果我想将这两件事同时可视化该怎么做呢？如果这两件事情相互不影响（相互独立），那么这很容易做到。例如，我穿T恤或者外套与下一周的天气并不相关。我们可以通过两个轴分别表示两种不同的变量的方式来可视化这些信息：</p>
<p><img src="/img/17_06_13/003.png" width=300 height=300 /></p>
<p>请注意水平方向和竖直方向上穿过的直线。<em>这就是独立性的标识！</em>我穿外套的概率并不受那一周天气都是下雨天而改变。换句话说，在下周要下雨并且我恰好也穿外套的概率，是我穿外套的概率与下雨天的概率之积。它们互不影响。</p>
<p>当变量之间相互作用时，对于某组相互排斥的变量而言，他们组成的概率会减少；而某组相互促进影响的变量而言，他们组成的概率会增加。对于在下雨天我穿外套的这种情况，会有额外的概率增加，因为下雨天和外套这两个变量是相关的，它们使得彼此之间更有可能发生。在下雨天我穿外套的概率，比我在其他某天穿外套的概率高。</p>
<p>可视化之后，这些看起来有肿胀的部分是因为额外增加的概率导致的，而其他收缩的部分，是因为这一对事件不太可能同时出现。</p>
<p><img src="/img/17_06_13/004.png" width=300 height=300 /></p>
<p>这张图虽然这看起来很酷，但对于理解具体发生了什么并不是很有帮助。</p>
<p>相反，让我们焦点集中在其中的某一个变量上，例如天气。我们知道某天是晴天或者下雨天的可能性。对于是否穿外套的两种情况，我们可以看看<em>条件概率</em>。我在晴天穿外套的可能性有多大？如果是下雨天，那么我穿外套的可能性又是多大？</p>
<p><img src="/img/17_06_13/005.png" width=500 height=300 /></p>
<p>有25%的时间是下雨天。如果是下雨天，那么有75%的可能性我会穿外套。所以，我在下雨天穿外套的概率是25%乘以75%，结果大约是19%。下雨天的概率乘以我在下雨天的情况下穿外套的概率，就是在所有情况下，天气为下雨天并且我穿外套的概率。我们用这样的式子来描述：</p>
<script type="math/tex; mode=display">
p(\text{rain}, \text{coat}) = p(\text{rain}) \cdot p(\text{coat} ~|~ \text{rain})</script><p>这就是概率论的最基本的定理之一：</p>
<script type="math/tex; mode=display">
p(x,y) = p(x)\cdot p(y|x)</script><p>我们把一个整体的概率拆分成两部分来看。首先，我们来看其中一个变量的概率的值，例如不同天气的概率。然后我们观察另一个变量，例如我的穿衣情况，观察它在第一个变量的条件下的概率。</p>
<p>具体选择哪个变量开始，是任意的。我们可以以我的穿衣情况为第一个被观察的条件，这也许会有点不直观，因为我们知道穿衣情况是受到天气情况影响的，而天气情况并不会受到我的穿衣情况影响的…但其实这里两种方式的效果是一样有效的！</p>
<p>让我们来看一个例子。我已经知道穿外套的概率是38%。假设我已经知道我在某一天是穿外套的，那么这一天下雨的概率有多大呢？我在下雨天比在晴天穿外套的可能性更大，但在加州是很少下雨的，所以有50%的概率这一天是下雨天。因此，某天是雨天并且我穿了外套的概率就等于我穿外套的概率（38%）乘以我穿外套的情况下是下雨天的概率（50%），结果大约是19%。</p>
<script type="math/tex; mode=display">
p(\text{rain}, \text{coat}) = p(\text{coat}) \cdot p(\text{rain} ~|~ \text{coat})</script><p>这为我们提供了第二种来可视化这些概率分布的方法：</p>
<p><img src="/img/17_06_13/006.png" width=300 height=500 /></p>
<p>注意，这里的标签与之前图中稍有不同：T恤和外套现在是<em>边际概率</em>，我穿外套的概率是不需要考虑天气情况的。另一方面，现在有两个雨天和晴天的标签，分别来标识我穿T恤情况下的概率和我穿外套情况下的概率。</p>
<blockquote>
<p>你可能已经听说过<strong>贝叶斯定理</strong>。其实你也完全可以使用贝叶斯定理来解释这部分内容。</p>
</blockquote>
<h2 id="Aside：辛普森悖论"><a href="#Aside：辛普森悖论" class="headerlink" title="Aside：辛普森悖论"></a>Aside：辛普森悖论</h2><p>这些可视化概率分布的技巧真的很有用吗？我认为的确很有用！在我们图解信息论之前，我还需要一些时间来用可视化的技术探索一下辛普森悖论。辛普森悖论是一种非常不直观的统计状况。它真的很不容易被直观的理解。迈克尔·尼尔森（Michael Nielsen）撰写了一篇很有爱的文章：<a target="_blank" rel="noopener" href="http://michaelnielsen.org/reinventing_explanation/">“创新说明”</a>，探讨了不同方式来解释它。而我想尝试使用我们在上一节中学到的技巧来解释它。</p>
<p>测试两种肾结石治疗方法。对一半的患者使用A治疗方案，而另一半则使用B治疗方案。接受治疗方案B的患者比接受治疗方案A的患者更容易存活。</p>
<p><img src="/img/17_06_13/007.png" width=400 height=400 /></p>
<p>然而，如果接受治疗方案A的话，患肾小结石的患者更有可能存活。然而大肾结石患者如果接受治疗方案A也更有可能存活！这怎么可能呢？</p>
<p>这个问题的核心是因为患者样本没有被正确随机分组。接受治疗方案A的患者大部分可能患有大肾结石，而接受治疗方案B的患者大部分可能患有小肾结石。</p>
<p><img src="/img/17_06_13/008.png" width=400 height=400 /></p>
<p>事实证明，肾结石较小的患者通常更有可能生存。</p>
<p>为了更好地理解这一点，我们可以将前两个图表组合为一个三维图，其中将小型和大型肾结石的存活率分开展示。</p>
<p><img src="/img/17_06_13/009.png" width=400 height=400 /></p>
<p>我们现在可以看到，在两种小肾结石和大肾结石的病例中，治疗方案A是超过治疗方案B的。治疗方案B看起来似乎更好，仅仅是因为应用它的患者本身更有可能存活！</p>
<h2 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h2><p>现在我们有了可视化概率的方法，我们可以深入到信息论中了。</p>
<p>首先我向你介绍一位我假想的朋友，Bob。Bob很喜欢动物。他一直不停在谈论动物。事实上，他只会说四个字：“dog”, “cat”, “fish” 和 “bird”。</p>
<p>几周前（虽然这只是我的想象)Bob搬到了澳大利亚。另外，他决定只想要使用二进制来交流。来自Bob的所有（虚构的）消息都是这样的：</p>
<p><img src="/img/17_06_13/010.png" width=300 height=100 /></p>
<p>为了沟通，Bob和我必须建立一个编码，一种将单词映射成位序列的方式。</p>
<p><img src="/img/17_06_13/011.png" width=300 height=300 /></p>
<p>要发送消息，Bob用相应的编码字来替换每个单词，然后将它们连接在一起形成编码的字符串。</p>
<p><img src="/img/17_06_13/012.png" width=300 height=150 /></p>
<h3 id="可变长度码"><a href="#可变长度码" class="headerlink" title="可变长度码"></a>可变长度码</h3><p>不幸的是，在假想中的澳大利亚，通信服务是昂贵的。从Bob收到的每条消息中，每一位（bit）数据我都必须支付5美元。我是不是还没有提到Bob喜欢说很多话这件事？为了防止我破产，Bob和我决定来研究一些可以使我们的平均消息长度变得更短的方法。</p>
<p>事实证明，并不是所有的词Bob都会经常说。Bob很喜欢狗，他一直在说“dog”。同时他也会不定期地说其他的动物，尤其是他的狗喜欢追的猫，但它大多数时间他还是在说“dog”。这是一个他的单词频率图：</p>
<p><img src="/img/17_06_13/013.png" width=300 height=200 /></p>
<p>这似乎看来是有优化空间的。因为我们可以看到，按照旧的编码方式，对于不同的词无论它们是多么常见，都在使用2位长的码字来表示。</p>
<p>我们可以用下面这张图来可视化这些信息。在下面这张图中，我们使用竖直方向的坐标轴来表示每个词的概率$p(x)$，水平方向的坐标轴来表示相应的码字的长度$L(x)$。可以看到在这种情况下平均码字是2位。</p>
<p><img src="/img/17_06_13/014.png" width=500 height=300 /></p>
<p>也许我们可以用一种非常聪明的方式来制作一种可变长度的编码方式，使得其中常用单词对应的码字特别短。这样做的挑战就是码字之间是存在竞争关系的，我们需要让一部分词对应的码字变短，而另一部分则会变长。为了最大限度地减少消息长度，我们希望理想状态下所有的码字都很短，尤其是那些常用的词我们更希望它们能变短。因此，最终得到的编码对于常用词（例如“dog”）具有较短的码字，并且较长的码字被应用到了不常用的词之上（例如“bird”）。</p>
<p><img src="/img/17_06_13/015.png" width=300 height=300 /></p>
<p>让我们再次可视化这些信息。注意那些最常用的词变短了，那些不常用的词变长了。经过这样处理之后，对于传递与之前相同的信息，我们的流量开销的确变少了。平均来说，码字长度现在是1.75位！</p>
<p><img src="/img/17_06_13/016.png" width=500 height=300 /></p>
<blockquote>
<p>你也许会好奇：为什么不用1来作为码字？因为可惜的是，如果我们这么做，当我们对编码的字符串进行解码时，会引起歧义。我们稍后会介绍这些内容。</p>
</blockquote>
<p>事实证明，这种编码方式是最好的。对于这个分配问题，没有其他的编码方式能带来平均码字长度小于1.75位的效果了。</p>
<p>这是一个简单的基本限制。这要求我们传达的信息平均码字长度至少是1.75位。无论我们用多么聪明的代码，都不可能让平均消息长度变得更短。我们称这种限制为<strong>熵</strong>。关于熵的内容，我们将在稍后详细讨论。</p>
<p><img src="/img/17_06_13/017.png" width=500 height=300 /></p>
<p>如果我们想了解这个平均最短码字长度的限制，这个问题的关键在于理解如何在使得一些码字变短但同时其他码字变长的过程中进行权衡。一旦我们弄明白了如何进行这种权衡，我们就能够找出最好的编码方式了。</p>
<h3 id="码字的空间"><a href="#码字的空间" class="headerlink" title="码字的空间"></a>码字的空间</h3><p>码字长度为1时，能表示2种编码：0和1的编码；码字长度为2时，能表示4中编码：00，01，10和11。下面是每增加一位码字长度，能表示的编码种类数量的递增情况：</p>
<p><img src="/img/17_06_13/018.png" width=350 height=300 /></p>
<p>但我们感兴趣的是那些可变长度编码。一种比较简单的情况是有八个长度为3位的码字。我们也可能有一些比较复杂的组合，比如长度为2的两个码字，与长度为3的四个码字的组合。那么究竟是什么决定了我们可以有不同长度的码字呢？</p>
<p>回想一下，Bob通过用它的码字替换每个单词并将它们连接在一起，将他的消息转换成编码的字符串。</p>
<p><img src="/img/17_06_13/019.png" width=300 height=200 /></p>
<p>当制作可变长度代码时，需要注意一个微妙的问题。我们如何将编码过的字符串分割回码字呢？当所有的码字长度相同时，这很简单。我们只需将这个字符串按照固定的码字长度分割成不同的码字即可。但是由于存在不同长度的码字，所以我们需要关注编码过的字符串的内容。</p>
<p>我们希望只有一种方式来解码编码过的字符串。我们不希望我们的编码方式在解码过程中变得含糊不清。如果我们有一些特殊的“码字结束”符号，这个问题这就会变得很容易。但其实我们没有这种符号，我们只能够发送0和1。因此，我们需要能够查看一系列连续的码字，并且说明每个码字应该停止的位置。</p>
<p>而这，很可能出现编码不能被唯一解码的情况。例如，假设0和01都是码字，那么我们就会搞不清楚符串0100111的第一个码字到底是什么。因为这个字符串的第一个码字既可以是0也可以是01！因此，我们想要的效果是没有哪个码字是另一个码字的前缀。这称为<strong>前缀属性</strong>，遵守它的编码称为<strong>前缀编码</strong>。</p>
<p>思考这个问题的一个好的方法是每个码字需要从可能的码字的空间中做出牺牲。如果我们取代码字01，那么我们就失去了使用任何以01前缀开头的码字的能力。我们不能使用010或011010110，因为会产生歧义。</p>
<p><img src="/img/17_06_13/020.png" width=400 height=400 /></p>
<p>由于所有码字中，有$\frac{1}{1}$都是从01开始，所以我们牺牲了$\frac{1}{1}$的所有可能的码字。这是我们付出的代价，而换来的结果只是使得其中一个码字的长度缩短到2位长！反过来，这种牺牲意味着所有其他码字需要更长一些。在不同码字的长度之间总是有这种折衷。为获取一个比较短的码字，需要你牺牲更多的其他码字的空间。而我们需要弄清楚，正确的权衡方式是什么？</p>
<h3 id="最佳编码"><a href="#最佳编码" class="headerlink" title="最佳编码"></a>最佳编码</h3><p>可以理解为简短编码的长度是有一个限定的度量在里面，每减短一个bit的密文就会牺牲一些编码的可能性从而让其它密文的长度增长。 </p>
<p>购买长度为0的码字的成本为1，会牺牲掉所有的码字空间，如果这样做，就不会有其他码字了。长度为1的码字（如“0”）的代价是$\frac{1}{2}$的码字空间，因为有一半的码字以“0”开始的。长度为2的码字的成本(例如“01”)是$\frac{1}{4}$码字空间，因为有$\frac{1}{4}$的码字以“01”开始的。一般来说，码字的代价随代码字的长度而呈指数减小。</p>
<p><img src="/img/17_06_13/021.png" width=600 height=300 /></p>
<p>如果代价是以指数衰减，那么这个高度和面积也是成指数衰减。</p>
<p>我们想要通过得到比较短的码字来缩短平均消息长度。每个码字的期望长度是每个码字出现的概率乘以码字的长度。假设我们有一个4bit的密文，出现概率是50%，那么我们的期望就是4*50%=2bit。我们可以作图来表示。</p>
<p><img src="/img/17_06_13/022.png" width=300 height=300 /></p>
<p>这两个值（代价和期望长度）与码字的长度相关。信息的长度决定了码字的平均长度。我们可以把这两者画在一起，就像这样。</p>
<p><img src="/img/17_06_13/023.png" width=600 height=300 /></p>
<p>短码字减少了平均消息长度，但是越短的码字越昂贵，而长码字增加了平均消息长度，但长码字相对便宜。</p>
<p><img src="/img/17_06_13/024.png" width=600 height=300 /></p>
<p>使用有限预算的最佳方法是什么？我们应该在每个事件的码字上投入多少花费呢？</p>
<p>就像一个人想要把较多的钱投入在经常定期使用的工具上一样，我们更愿意对那些频繁使用的码字支付更高的花费。有一种特别顺其自然的处理方式：按照事件的普遍程度分配我们的预算。如果一个事件发生的概率是50％，那么我们就花费预算的50％来购买这个短的码字。但是，如果一个事件的发生概率只有1％，那么我们就只花1％的预算，因为我们并不是很在意这个不常见的码字变得很长。</p>
<p>这是一件很顺其自然的解决方案，但它是最优解吗？没错，它是，我接下来会证明它！</p>
<p><em>以下是可视化的证明过程，应该是可以理解的，但理解起来有一定难度，而这部分绝对是这篇文章中最难的部分。读者应该自由地选择是否默认接受这一结论跳过这一部分。</em></p>
<p>我们来看一个具体的例子，我们需要比较两个可能发生的事件。事件a发生的概率是$p(a)$，事件b发生的概率是$p(b)$。我们按照上述自然的方式分配预算，花费$p(a)$的预算在a获得一个较短的码字上，花费$p(b)$的预算在b获得较短的码字上。</p>
<p><img src="/img/17_06_13/025.png" width=600 height=300 /></p>
<p>花费的成本和对应的长度贡献的边界完美的对齐了，这是否意味着什么？</p>
<p>那么，请考虑一个问题，如果我们稍微改变码字的长度，会对花费的成本和码字长度贡献值有什么样的影响呢？如果我们稍微增加码字的长度，则消息长度的贡献值将与其边界的高度成比例地增加，而成本则会与边界的高度成正比。</p>
<p><img src="/img/17_06_13/026.png" width=600 height=300 /></p>
<p>所以，使码字a缩短的代价的值是$p(a)$。同时，我们不关心每个码字的具体长度，我们关心的是使用它们的占比。在a这种情况下，占比就是$p(a)$。对我们来说，就是$p(a)$使码字a的长度变得更短一些。</p>
<p>然而有趣的是，这两者的导数都是一样的。这意味着我们的初始预算有一个有趣的属性，如果你花费了一些用于缩短码字的投入，那么这个投入对于投在任何码字上，效果都是一样的。我们真正关心的是最终的利益/成本比例 - 这就决定了我们应该投资多少。在这种情况下，这个比例是$\frac{p(a)}{p(a)}$等于1。这与$p(a)$的值无关，因为这个结果总是1。我们也可以将相同的参数应用于其他事件。利益/成本总是一，所以在其中任何一方投资更多是平等的。</p>
<p><img src="/img/17_06_13/027.png" width=600 height=300 /></p>
<p>无论如何，改变预算是没有意义的。但这不证明这是最好的预算。为了证明这一点，我们会考虑使用一个不同的预算。我们将在$b$中投资$ε$，并在$a$中投资相反的花费。这使得a的码字稍短一些，而b的码字稍长一些。</p>
<p>现在为a购买一个较短的码字的成本是$p(a)+ε$，为b购买较短码字的费用b是$p(b)-ε$。但收益还是一样的。这导致了购买$a$的收益成本比例为一个小于1的值：$\frac{p(a)}{p(a) + ε}$。而另一边，对于购买$b$的收益成本比例是一个大于1的值：$\frac{p(b)}{p(b) - ε}$</p>
<p><img src="/img/17_06_13/028.png" width=600 height=300 /></p>
<p>价格不再保持平衡。b相对于a来说是一笔更好的交易。投资者们尖叫道：“买b！卖a！”我们通过这样做，结束了我们原来的预算计划。所有预算都可以通过转向原始预算的方式来得到改进。</p>
<p>原始预算 - 按照我们使用频率的比例投入每个码字 - 这不仅仅是自然而然的事情，而且这是最好的方法。（虽然这个证明仅适用于两个码字，但它很容易泛化为更多的情况。）</p>
<blockquote>
<p>认真的读者可能已经注意到，我们的最优预算有可能提出码字具有分数长度的编码，这似乎很有用！这是意味着什么呢？当然，在实践中，如果你想通过发送一个单码字，你必须取整，但是我们稍后会看到，当我们一次发送很多码字时，可以发送分数码字！现在请你耐心的看我现在的讲解！</p>
</blockquote>
<h3 id="计算熵"><a href="#计算熵" class="headerlink" title="计算熵"></a>计算熵</h3><p>回想一下，一个长度为$L$的消息代价是$\frac{1}{2^L}$。再做一个简单的变换后，这个信息的长度就是：$\log_2\left(\frac{1}{\text{cost}}\right)$。因为我们对每个码字$x$花费$p(x)$的成本，长度是$\log_2\left(\frac{1}{p(x)}\right)$。这是长度的最佳选择。</p>
<p><img src="/img/17_06_13/029.png" width=600 height=300 /></p>
<p>早些时候，我们讨论了如何从一个特定的概率分布$p$中获取平均消息来传达事件的基本限制。这个将平均消息长度缩短到最优的编码的限制，我们称之为$p$的熵，$H(p)$。现在我们知道了码字的最佳长度，而且我们可以精确的计算出来！</p>
<script type="math/tex; mode=display">
H(p) = \sum_x p(x)\log_2\left(\frac{1}{p(x)}\right)</script><blockquote>
<p>人们通常将熵写作$H(p) = - \sum p(x)\log_2(p(x))$这种形式，因为$\log(1/a) = -\log(a)$。但我认为第一种写法更直观，并且我在这篇文章的后续部分会继续使用这种形式。</p>
</blockquote>
<p>无论我做什么，平均来说，如果我想要进行通信交流，信息平均长度最少都是这个值。</p>
<p>这个需要发送的信息的平均长度对信息的压缩有明显的影响，这个熵还有其它方面的意义吗？当然！它描述了随机性，并且提供了一种量化信息的方式。</p>
<p>如果我确实知道会发生什么事件，我根本就不必发消息！如果有两件可能发生的概率为50％，我只需要发送1bit的数据。如果有64个不同的事情可能以相等的概率发生，我必须发送6bit。发生的概率越集中在某一件事上，就越能用巧妙的编码方式来减少信息的平均长度。概率扩散的范围越大，信息越长。发生的概率越分散到不同事情上，我需要发送的信息平均长度就越长。</p>
<p>结果越不明确，在发生事情时就能学到越多的东西。</p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>在Bob搬到澳大利亚不久之后，他和我假想的另一个女孩Alice结婚了(让我惊讶的是，我脑海里居然还有其他角色)。然而Alice并不是一个狗狗爱好者。她是一个猫猫爱好者。尽管如此，他们俩也可以在仅有非常有限的关于动物的词汇的情况下，找到共同话题。</p>
<p><img src="/img/17_06_13/030.png" width=300 height=300 /></p>
<p>他们两以不同的频率说着相同的话。Bob总是谈论狗，而Alice却总是谈论猫。</p>
<p>最初，Alice使用Bob的编码向我发送消息。不幸的是，她的消息比他们实际需要的要长。Bob的编码是按照Bob的概率分布优化得到的。而Alice与Bob有不同的信息概率分布，并且Bob编码对她来说并不是最佳的。当Bob使用自己的编码时，码字的平均长度是1.75，但Alice在使用这组编码时，她的码字平均长度是2.25。如果他们两的概率密度越不相似，这个结果就越糟糕。</p>
<p>这种根据一边信息概率分布优化过的编码方式来传输另一种概率分布不同的信息时，消息的平均长度称为交叉熵。关于交叉熵，更正式的定义如下：</p>
<script type="math/tex; mode=display">
H_p(q) = \sum_x q(x)\log_2\left(\frac{1}{p(x)}\right)</script><p>在这种情况下，Alice的猫爱好者的词频与Bob的狗爱好者词频存在交叉熵。</p>
<p><img src="/img/17_06_13/031.png" width=300 height=300 /></p>
<p>为了让我们的通信的花费减少，我让Alice来使用她自己的编码。让我感到安慰的是，这使得她的平均消息长度变短了。但是，这也引入了一个新的问题：有时Bob会不小心使用Alice的编码。令人惊讶的是，Bob使用Alice的编码时，Bob信息的平均长度比Alice用Bob编码时的最优方式还长</p>
<p>所以，现在我们有四种可能的情况：</p>
<ul>
<li>Bob使用它自己的编码$(H(p) = 1.75 ~\text{bits})$</li>
<li>Alice使用Bob的编码$(H_p(q) = 2.25 ~\text{bits})$</li>
<li>Alice使用她自己的编码$(H(q) = 1.75 ~\text{bits})$</li>
<li>Bob使用Alice的编码$(H_q(p) = 2.375 ~\text{bits})$</li>
</ul>
<p>这并不能直观的展示出这种值之间的关联关系。我们可以使用下图来寻找它们之间的关系。</p>
<p>在下图中，每个子图分别表示这四种情况之一。每个子图以与之前相同的方式来显示平均消息长度。处于同一行的两种情况有相同的概率分布，处于同一列的两种情况有相同的编码方式。通过这种方式，可以可视化的观测到概率分布和编码方式的关系。</p>
<p><img src="/img/17_06_13/032.png" width=500 height=500 /></p>
<p>你能看出来为什么$H_p(q) \neq H_q(p)$吗？$H_q(p)$比较大因为在概率分布$p$之下有一个非常常见的码字比较长的事件（蓝色），因为这个事件在概率分布$q$的情况下并不常见。然而，另一边，在$q$的情况下比较常见的事件，在$p$的情况下比较少见，但差异较小，所以$H_p(q)$不是很高。</p>
<p>因此我们发现交叉熵是不对称的。</p>
<p>那么，为什么要关心交叉熵呢？交叉熵给我们一种表达两种概率分布不同的方法。概率分布$p$和$q$的差异越大，那么$p$关于$q$的交叉熵就比$p$的熵越大。</p>
<p><img src="/img/17_06_13/033.png" width=300 height=300 /></p>
<p>同样的，概率分布$p$和$q$差异越大，$q$关于$p$的交叉熵就比$q$本身的熵越大。</p>
<p><img src="/img/17_06_13/034.png" width=300 height=300 /></p>
<p>真正有趣的东西是熵与交叉熵之间的差异。差异在于我们的信息具体长多少，因为我们对不同的概率分布使用了同一套优化过的编码方式。如果概率分布相同，那么这个差异将为零。随着差异的增长，这个差值将变得更大。</p>
<p>我们称这种差异为Kullback-Leibler分歧，或者简称为KL分歧。概率分布$p$关于$q$的KL分歧$D_q(p)$定义如下：</p>
<script type="math/tex; mode=display">
D_q(p) = H_q(p) - H(p)</script><p>关于KL分歧真正在做的事情，其实就像是在衡量不同概率分布之间的差异程度。（如果继续研究这个概念就会进入信息几何的领域。）</p>
<p>交叉熵和KL分歧在机器学习中有着难以置信的用处。通常，我们希望一个分布与另一个分布相互靠近。例如，我们希望一个用于预测的概率分布与真实情况更接近。KL分歧为我们提供了一种很自然的方式来做到这一点，所以它在任何地方都有用武之地。</p>
<h2 id="熵和多变量"><a href="#熵和多变量" class="headerlink" title="熵和多变量"></a>熵和多变量</h2><p>让我回到之前关于天气和穿着的例子：</p>
<p><img src="/img/17_06_13/035.png" width=300 height=300 /></p>
<p>我的妈妈，就像大多数的家长一样，都会担心我的衣着跟天气不搭配。（她的这种担心不是没有道理的，因为我在冬天经常不穿外套。）所以她总是想要知道我这边有关天气和穿衣的信息。我需要发送几个bit的数据来传达这些信息呢？</p>
<p>为了更方便的分析这个问题，我们把概率扁平化分析：</p>
<p><img src="/img/17_06_13/036.png" width=600 height=300 /></p>
<p>现在我们可以找出这些概率的事件的最优码字，并计算平均消息长度：</p>
<p><img src="/img/17_06_13/037.png" width=600 height=300 /></p>
<p>我们把这个称之为$X$和$Y$的联合熵，定义如下：</p>
<script type="math/tex; mode=display">
H(X,Y) = \sum_{x,y} p(x,y) \log_2\left(\frac{1}{p(x,y)}\right)</script><p>这与我们关于熵的正规定义完全相同，唯一的区别就是这里有两个变量，而之前是一个。</p>
<p>我们再把这个图形表现得更形象话一些，加入码字长度作为第三个维度，让图形更立体。现在，熵的大小就是体积的大小。</p>
<p><img src="/img/17_06_13/038.png" width=300 height=600 /></p>
<p>假设我的妈妈已经知道了天气信息（她可以通过新闻知道这些信息）。那么现在我需要提供多少的信息量呢？</p>
<p>似乎我需要发送很多信息来传达我的穿衣情况。但实际情况，我发送的信息比之前要少，因为天气与我的衣着有着密切的联系！我们来分别思考关于下雨天和晴天下的两种情况。</p>
<p><img src="/img/17_06_13/039.png" width=400 height=400 /></p>
<p>在这两种情况下，平均来看，我不需要发送特别多的信息，因为天气信息对我的穿衣情况有着很好的预示。当阳光明媚的晴天，我可以使用一个特定的“晴天优化版”编码，对于下雨天，我可以使用一个特定的“雨天优化版”编码。分别在这两种情况下用不同的编码会比用同一种通用编码的长度少很多。为了得到我需要发送给我妈妈的平均信息量，我把这两个例子放在一起…</p>
<p><img src="/img/17_06_13/040.png" width=200 height=300 /></p>
<p>我们称之为条件熵。可以把它写成如下的数学表达式：</p>
<script type="math/tex; mode=display">
H(X|Y) = \sum_y p(y) \sum_x p(x|y) \log_2\left(\frac{1}{p(x|y)}\right)</script><script type="math/tex; mode=display">
~~~~ = \sum_{x,y} p(x,y) \log_2\left(\frac{1}{p(x|y)}\right)</script><h2 id="交互信息"><a href="#交互信息" class="headerlink" title="交互信息"></a>交互信息</h2><p>在上一节，我们得到一个结论，那就是在知道一个变量的情况下，可能会意味着导致传达另一个变量需要更少的信息。</p>
<p>一种思考这种情况的很好方式是将信息的总量想象成一个条。如果不同的消息之间在共享信息，就叠加显示。例如，$X$和$Y$存在一部分共享信息，因此$H(X)$和$H(Y)$是有重叠部分。并且$H(X,Y)$是两者的共同信息，是$H(X)$和$H(Y)$的集合。</p>
<p><img src="/img/17_06_13/041.png" width=300 height=200 /></p>
<p>一旦我们开始以这种方式思考事情，很多事情就变得简单了。</p>
<p>例如，我们之前注意到同时传递$X$和$Y$（“联合熵”$H(X,Y)$）比仅仅使用$X$来通信（“边际熵”$H(X)$）需要更多的信息。如果你已经知道了$Y$，那么它会用更少的信息来对$X$进行通信（“条件熵”$H(X|Y)$）。</p>
<p><img src="/img/17_06_13/042.png" width=500 height=400 /></p>
<p>这听起来有点复杂，但从条状显示图来看，就很简单了。$H(X|Y)$是指我们在向已经知道$Y$的信息的人在传达$X$的信息时的熵，蕴含在$X$中的信息并不包含在$Y$中。从视觉上来看，这意味着$H(X|Y)$是$H(X)$的条中，不与$H(Y)$重叠的那一部分。</p>
<p>你可以从下图中得出这个不等式:$H(X,Y) \geq H(X) \geq H(X|Y)$。</p>
<p><img src="/img/17_06_13/043.png" width=300 height=200 /></p>
<p>另一个特性：$H(X,Y) = H(Y) + H(X|Y)$。也就是说$X$和$Y$的信息也就是$Y$的信息加上不在$Y$中的$X$的信息。</p>
<p><img src="/img/17_06_13/044.png" width=500 height=400 /></p>
<p>从方程中很难看出这些信息，但如果你将这些信息用条状图显示出来，就变得容易了。</p>
<p>在这里，我们以多种方式组织了$X$和$Y$的信息。我们有每个变量所包含的信息$H(X)$和$H(Y)$。我们有着两种信息的交集$H(X,Y)$。我们有只存在与一个变量但不存在与另一个变量的信息$H(X|Y)$和$H(Y|X)$。还有很多这些围绕着变量之间共享的信息，即他们之间信息的交集。我们称之为“相互信息”$I(X,Y)$，定义如下：</p>
<script type="math/tex; mode=display">
I(X,Y) = H(X) + H(Y) - H(X,Y)</script><p>这个定义是有效的，因为$H(X) + H(Y)$存在有两个相互信息的副本，因为他们同时蕴含在$X$中以及$Y$中，而$H(X,Y)$只有一份。(想想上一个条形图。)</p>
<p>与信息密切相关的是信息的变化。信息的变化是变量之间不共享的信息。我们可以这样定义它：</p>
<script type="math/tex; mode=display">
V(X,Y) = H(X,Y) - I(X,Y)</script><p>信息的变化很有趣，因为它给了我们一个不同变量之间度量距离的概念。两个变量如果知道一个值的值可以得出另一个变量的值，那么这两者之间的信息变化是零，并且这个信息变化的值随着他们彼此之间变得更加独立而增加。</p>
<p>这和同样可以告诉我们距离信息的KL分歧之间的关系是什么呢？KL分歧给了我们两个分布之间在同一个变量或一组变量上的距离。相反，信息的变化给我们两个相同分布的变量之间的距离。KL分歧是应用在不同信息的概率分布中的，标识这概率分布中的信息变化。</p>
<p>我们可以将所有这些信息全部汇集成一个单独的图表：</p>
<p><img src="/img/17_06_13/045.png" width=400 height=400 /></p>
<h2 id="分数形式的位"><a href="#分数形式的位" class="headerlink" title="分数形式的位"></a>分数形式的位</h2><p>关于信息理论的一个非常不直观的事情是我们可以拥有小数位数。这看起来很奇怪，半个位(0.5bit)意味着什么？</p>
<p>有一种简单的答案：通常，我们对消息的平均长度感兴趣，而不是任何特定的消息长度。如果一半的时间发送一个位的长度，一半时间发送两个位的长度，那么平均发送的长度是1.5位。对于平均数是小数的情况并没有什么奇怪的。</p>
<p>但是，这个答案其实是在避开这个问题。通常，码字的最佳长度都是分数形式。这意味着什么呢？</p>
<p>具体来说，让我们考虑一个概率分布，其中事件$a$发生的概率是$71%$，另一个事件$b$发生的概率为$29%$。</p>
<p><img src="/img/17_06_13/046.png" width=300 height=300 /></p>
<p>最佳编码将使用0.5位来表示$a$，用1.7位来表示$b$。那么，如果我们要发送这些码字中的任何一个，都是不可能的。我们会强制被安排发送整数位数的信息，并且发送的平均消息长度是1位。</p>
<p>但是，如果我们一次性发送多条消息，我们可以处理的更好。让我们来考虑按照这种概率分布的情况下来同时传递两个事件。如果我们每次只发送一个事件，那么我们需要发送两位的长度。我们可以处理的更好吗？</p>
<p><img src="/img/17_06_13/047.png" width=300 height=300 /></p>
<p>有一半的概率，我们需要传递$aa$的信息，有21%的概率我们需要发送$ab$或者$ba$，而我们传递$bb$的概率只有8%。我们再一次把编码优化到理想的小数位数的形式。</p>
<p><img src="/img/17_06_13/048.png" width=500 height=300 /></p>
<p>如果我们对码字长度取整，我们将得到这样的结果：</p>
<p><img src="/img/17_06_13/049.png" width=500 height=300 /></p>
<p>这个编码给我们带来的平均消息长度为1.8位。这比我们每次分别只传递一个事件的平均消息长度2位要小。另一种思考这个问题的方式是我们平均每个事件发送的信息是0.9位。如果我们一次发送更多的事件，那么这个值会更小。由于$n$趋向于无穷大，取整造成的损失将逐渐减少，并且每个码字的位数将接近熵。</p>
<p>另外，请注意，$a$的理想码字为0.5位，$aa$的理想码字是1位。即使理想码字的长度是小数，它们的长度也在增加！所以，如果我们一次传达很多事件时，长度会增加。</p>
<p>即使在实际的编码中，只能使用整数长度的码字，这里也有使用小数位数码字的实际的意义。</p>
<blockquote>
<p>在实践中，人们使用在不同程度上有效的特定编码方案。<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Huffman_coding">霍夫曼编码</a>，是一种基本类似于我们在这里草拟的这种编码，并没有非常优雅地处理小数位。你必须像上面那样对符号进行分组，或者使用更复杂的技巧来处理熵限制。<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Arithmetic_coding">算术编码</a>有点不同，它优雅地处理了小数位以渐近最优。</p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果我们关心如何使用最少数量数据进行通信，那么这些想法显然是最基本的。如果我们关心压缩数据，信息理论将可以解决数据压缩的核心问题，并给出我们从根本上正确的抽象。但是，如果我们不在乎这些问题，那么除了好奇，我们还有其他什么原因呢？</p>
<p>来自信息理论的观点在很多情况下都会出现：机器学习，量子物理学，遗传学，热力学，甚至赌博。这些领域的从业者通常不关心信息理论。量子纠缠可以用熵来描述，通过假定关于你不知道的事物的最大熵，可以得出统计力学和热力学中的许多结果。赌徒的胜利或损失与KL分歧直接相关，特别是迭代设置。</p>
<p>信息理论在所有这些地方出现，因为它为许多我们需要表达的事情提供了具体的，有原则的形式化表示。它给了我们衡量和表达不确定性的方法，两组数据有多么的不同，不同概率分布之间的距离是多少，以及但对于某个问题的答案中蕴含了多少其他问题的信息：扩散概率是怎样的，概率分布之间的距离以及相互依赖的两个变量是怎样的。有其他类似的理论或者观点吗？当然。但是信息论的思想是干净的，它们具有很好的性质和原则性的起源。在某些情况下，它们正是你所关心的，而在其他情况下，它是混乱世界中的一个方便的代理。</p>
<p>机器学习是我最了解的领域，所以我们来谈一下。在机器学习中，分类是一种非常常见的问题。假设我们想看一张图片，并预测它是一只狗还是一只猫。我们的模型可能会告诉我们“这张图片中有80%的概率是一只狗，有20%的概率是一只猫”。那么我们就说正确答案是狗，那么我们以80%的概率做出的判断是好还是坏呢？以85%的概率做出的判断会比80%好多少呢？</p>
<p>这是一个重要的问题，因为我们需要一些衡量我们模型好坏的概念，以便把它优化的更好。我们应该优化什么呢？正确的答案确实取决于我们使用的模型：我们是只关心最顶部的猜测是否正确呢，还是关心我们在获取正确答案中的信心呢？错误的自信会有多糟？没有一个正确的答案。而且通常不可能知道正确的答案，因为我们不知道如何以精确的方式使用模型来形式化我们最终关心的内容。然而很多问题中，交叉熵都是我们真正需要在意的，但并不总是如此。更常见的情况是我们并不知道我们关心什么，而交叉熵就是解决这些问题的一个很好的工具。</p>
<p>信息为我们提供了一个强大的新框架来思考我们的世界。有时它能完美的解决一些问题，而有时并不可以，但它依然很有用。这篇文章仅仅是对信息论的一个概览，而信息论中还有许多其他主要的概念，例如我们没有提到的纠错码，但我希望我已经展示出了信息论是一个很美丽的主题，它并不是那么高不可攀。</p>
<h2 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h2><p>这里是<strong>克劳德·香农（Claude Shannon）</strong>关于信息论的原创论文<a target="_blank" rel="noopener" href="http://worrydream.com/refs/Shannon%20-%20A%20Mathematical%20Theory%20of%20Communication.pdf">通信数学理论</a>。（这在早期的信息论论文中，似乎是一个重复的模式。是时代原因吗？还是因为缺少页面限制？或者是来自贝尔实验室的文化？）</p>
<p>封面和托马斯的信息要素理论似乎是标准参考。我觉得很有帮助。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>我非常感谢<a target="_blank" rel="noopener" href="https://github.com/danmane">Dan Mané</a>，<a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~dga/">David Andersen</a>，<a target="_blank" rel="noopener" href="http://obsessionwithregression.blogspot.com/">Emma Pierson</a>和Dario Amodei抽出时间来给出这篇文章令人难以置信的详细和广泛征求意见。我也很感激<a target="_blank" rel="noopener" href="http://michaelnielsen.org/">Michael Nielsen</a>，<a target="_blank" rel="noopener" href="http://research.google.com/pubs/GregCorrado.html">Greg Corrado</a>，<a target="_blank" rel="noopener" href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio</a>，<a target="_blank" rel="noopener" href="https://aaroncourville.wordpress.com/">Aaron Courville</a>，<a target="_blank" rel="noopener" href="http://www.nickbeckstead.com/">Nick Beckstead</a>，<a target="_blank" rel="noopener" href="http://research.google.com/pubs/JonathonShlens.html">Jon Shlens</a>，Andrew Dai，<a target="_blank" rel="noopener" href="http://research.google.com/pubs/ChristianHoward.html">Christian Howard</a>和<a target="_blank" rel="noopener" href="http://www.bewitched.com/">Martin Wattenberg</a>的评论。</p>
<p>还感谢我的前两个神经网络研讨会系列作为这些想法的豚鼠。</p>
<p>最后，感谢读者发现错误和遗漏。尤其感谢Connor Zwick，Kai Arulkumaran，Jonathan Heusser，Otavio Good，以及匿名评论者。</p>
<h2 id="更多的文章"><a href="#更多的文章" class="headerlink" title="更多的文章"></a>更多的文章</h2><p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">了解卷积</a></p>
<p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-12-Groups-Convolution/">分组和分组卷积</a></p>
<p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">神经网络，歧管和拓扑</a></p>
<p><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-01-Visualizing-Representations/">可视化展示深度学习和人类</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/06/05/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E6%B1%87%E6%80%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/05/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E6%B1%87%E6%80%BB/" class="post-title-link" itemprop="url">斯坦福机器学习课程汇总</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-06-05 23:37:00" itemprop="dateCreated datePublished" datetime="2017-06-05T23:37:00+00:00">2017-06-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先感谢<strong>吴恩达</strong>建立<a target="_blank" rel="noopener" href="https://www.coursera.org">Coursera</a>这样一个优秀的在线学习平台，以及他发布在这个平台上的<a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/">机器学习</a>课程。</p>
<p>这门课程将整个机器学习领域的基础知识，用浅显易懂的方式，深入浅出的进行了介绍。使得一个拥有高中数学知识的学生也能听得明白。</p>
<p>如果你想要涉足机器学习、人工智能领域，或者对这一领域有浓厚的兴趣想要深入了解，那么你会发现很多机器学习入门课程推荐的资料中，都有吴恩达老师的这一系列课程。甚至在大多数资料中，都把这门课放在了首选的位置上。</p>
<p>因此，我把吴恩达老师的课程整理成了MarkDown的格式，方便查阅学习。以下是具体章节的目录，其中每篇文章都有对应的视频连接地址：</p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li>第一周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1bwH">欢迎来到机器学习</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1Ju5">监督学习</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1XxR">无监督学习</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1onR">一元线性回归</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1N6G">参数学习-梯度下降算法</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1WEE">线性代数复习</a></li>
</ul>
</li>
<li>第二周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1T2O">编程环境设置-Octave:MATLAB</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1RnN">多元线性回归分析</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1nj4">参数的计算分析</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1BP3">Octave/Matlab 使用说明</a></li>
</ul>
</li>
<li>第三周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSC1epl">分类和表达式</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBwf7">分类</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCB4bV">假设函数表达式</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBhNP">决策边界</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBccU">Logistic回归模型</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBMeK">逻辑回归的代价函数</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBogn">简化代价函数以及梯度下降</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBpkr">高级优化</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBWU1">多类别分类问题：一对多</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBjW6">正则化：解决过拟合问题</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBT6v">解决过拟合问题</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCB8Vt">代价函数</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBELF">正则化线性回归</a></li>
</ul>
</li>
</ul>
</li>
<li>第四周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBBZe">神经网络引入</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBDHu">非线性假设</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCBsN4">神经网络和大脑</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCrZ1M">神经网络</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCrUfK">神经网络应用实例</a></li>
</ul>
</li>
<li>第五周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCr5m2">训练神经网络</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCrf4H">代价函数</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCriIb">反向传播(B-P)</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCr9Rc">反向传播算法的直观介绍</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCr0yH">BP算法</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCrHqO">神经网络实现自动驾驶</a></li>
</ul>
</li>
<li>第六周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCruh7">评价一个学习算法</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCr1gH">如何少走弯路？</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCrd7a">评估假设函数</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCruh7">多项式模型的选择以及训练集/验证集/测试集的划分</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCdL5Q">偏差VS方差</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCdL5Q">偏差VS方差</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCd6C0">正则化和偏差/方差</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCdo2l">学习曲线(Learning Curves)</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCdCvC">重新审视决定下一步做什么</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCd0fl">机器学习系统设计</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCd0fl">构建垃圾邮件分类器</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCdH6b">误差分析</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSh83NE">操作偏斜数据</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCdBSe">偏移类的错误度量</a></li>
<li><a target="_blank" rel="noopener" href="http://sina.lt/eXAz">查准率和召回率练习</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgyhD">使用大数据集</a></li>
</ul>
</li>
<li>第七周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCg4WP">大间距分类 SVM</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCg4WP#优化目标">优化目标</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCg4WP#大间距的直觉">大间距的直觉</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCg4WP#大间距分类器背后的数学原理(选学">大间距分类器背后的数学原理(选学)</a>)</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgJw1">核函数</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgXmt">使用SVM</a></li>
</ul>
</li>
<li>第八周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgKuS">聚类</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgKuS#">无监督学习介绍</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgKuS#K-Means算法">K-Means算法</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgKuS#优化目标">优化目标</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgKuS#随机初始化">随机初始化</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgKuS#选择簇的数量">选择簇的数量</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCg8PK">PCA 降维</a></li>
</ul>
</li>
<li>第九周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgubu">密度估计&amp;异常检测</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgBQV">构建一个异常检测系统</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCgDCU">多元高斯分布（选学）</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCevr6">预测电影评分</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCevr6#预测电影评分">预测电影评分</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCevr6#协同过滤">协同过滤</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCevr6#低秩矩阵分解">低秩矩阵分解</a></li>
</ul>
</li>
</ul>
</li>
<li>第十周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeLpk">大数据集梯度下降</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeLpk#处理大数据的学习算法">处理大数据的学习算法</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeLpk#随机梯度下降">随机梯度下降</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeLpk#小批量梯度下降">小批量梯度下降</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeLpk#随机梯度下降的收敛">随机梯度下降的收敛</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeii5">高级主题</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeii5#在线学习">在线学习</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCeii5#MapReduce和数据并行">Map Reduce 和数据并行</a></li>
</ul>
</li>
</ul>
</li>
<li>第十一周<ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCe9Bv">照片OCR</a><ul>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCe9Bv#问题描述和流水线（Pipeline）">问题描述和流水线（Pipeline）</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCe9Bv#滑动窗体">滑动窗体</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCe9Bv#获取大量数据和人工数据">获取大量数据和人工数据</a></li>
<li><a target="_blank" rel="noopener" href="http://t.cn/RSCe9Bv#上限分析：流水线上的下一步工作是什么">上限分析：流水线上的下一步工作是什么</a></li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/06/04/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8%20(1)%E7%85%A7%E7%89%87OCR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%8D%81%E4%B8%80%E5%91%A8%20(1)%E7%85%A7%E7%89%87OCR/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第十一周 (1)照片OCR</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-06-04 19:40:00" itemprop="dateCreated datePublished" datetime="2017-06-04T19:40:00+00:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题描述和流水线（Pipeline）"><a href="#问题描述和流水线（Pipeline）" class="headerlink" title="问题描述和流水线（Pipeline）"></a>问题描述和流水线（Pipeline）</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/iDBMm/problem-description-and-pipeline">视频地址</a></p>
<blockquote>
<p>在这一段和下几段视频中，我想向你介绍一种 机器学习的应用实例：<strong>照片OCR技术</strong>。我想介绍这部分内容的原因主要有以下三个:</p>
<ul>
<li><p>第一，我想向你展示一个复杂的机器学习系统是如何被组合起来的。</p>
</li>
<li><p>第二，我想介绍一下<strong>机器学习流水线（machine learning pipeline）</strong>的有关概念以及在决定下一步做什么时如何分配资源。</p>
</li>
<li><p>最后，我也想通过介绍<strong>照片OCR</strong>问题的机会来告诉你机器学习的诸多有意思的想法和理念。其中之一是如何将机器学习应用到计算机视觉问题中，第二是有关<strong>人工数据合成（artificial data synthesis）</strong>的概念。</p>
</li>
</ul>
</blockquote>
<p><strong>照片OCR</strong>是指<strong>照片光学字符识别（photo optical character recognition）</strong>。</p>
<p>随着数码摄影的日益流行，以及近年来手机中拍照功能的逐渐成熟，我们现在很容易就会有一大堆从各地拍摄的数码照片。吸引众多开发人员的其中一个应用是如何让计算机更好地理解这些照片的内容。这种照片OCR技术主要解决的问题是让计算机读出照片中拍到的文字信息。</p>
<p>照片OCR技术的一个应用场景是方便照片的搜索。例如你找出下面这张照片时，你只需要输入图片中的文字“LULA B’s ANTIQUE MALL”即可把照片找出来：</p>
<p> <img src="/img/17_06_04/001.png" width = "500" height = "500" align=center /></p>
<blockquote>
<p>虽然现在OCR对扫描的文档来说已经是一个比较简单的问题了，但对于数码照片来说现在还是一个比较困难的机器学习问题。研究这个的目的不仅仅是因为这可以让计算机更好地理解我们的户外图像，更重要的是它衍生了很多应用。比如在帮助盲人方面，假如你能为盲人提供一种照相机，这种相机可以“看见”他们前面有什么东西，可以告诉他们面前的路牌上写的是什么字。现在也有研究人员将照片OCR技术应用到汽车导航系统中，想象一下你的车能读出街道的标识，并且将你导航至目的地。</p>
</blockquote>
<h3 id="OCR的大概步骤"><a href="#OCR的大概步骤" class="headerlink" title="OCR的大概步骤"></a>OCR的大概步骤</h3><p>照片OCR的大体步骤如下：</p>
<ul>
<li>1.<strong>文字识别技术（Text detection）</strong></li>
</ul>
<p>首先使用<strong>文字识别技术（Text detection）</strong>将给定的图片扫描一遍，找出这张图片中哪里有文字信息：</p>
<p> <img src="/img/17_06_04/002.png" width = "500" height = "500" align=center /></p>
<ul>
<li>2.字符切分</li>
</ul>
<p>接下来就是重点关注这些文字区域，对这些文字区域的矩形轮廓进行字符切分。</p>
<p> <img src="/img/17_06_04/003.png" width = "500" height = "500" align=center /></p>
<p><img src="/img/17_06_04/004.gif" alt=""></p>
<ul>
<li>3.字符分类</li>
</ul>
<p>当文字被分割成独立的字符之后，我们可以尝试运行一个分类器，输入这些可识别的字符，然后试着识别出上面的字符。</p>
<p><img src="/img/17_06_04/005.png" alt=""></p>
<p>因此通过完成所有这些工作，按理说你就能识别出 这个字段写的是“LULAB’s ANTIQUE MALL”，然后图片中其他有文字的地方也是类似的方法进行处理。</p>
<hr>
<blockquote>
<p>其实有很多照片OCR系统会进行更为复杂的处理，比如在最后会进行拼写校正。</p>
<p>假如你的字符分割和分类系统告诉你它识别到的字是“C1eaning”，那么很多拼写修正系统会告诉你，这可能是单词“Cleaning”的拼写，你的字符分类算法刚才把字母“l”识别成了数字“1”。</p>
<p>但在本课中，我们会不考虑最后这一步拼写检查，只关注前面三个步骤。</p>
</blockquote>
<h3 id="机器学习流水线（machine-learning-pipeline）"><a href="#机器学习流水线（machine-learning-pipeline）" class="headerlink" title="机器学习流水线（machine learning pipeline）"></a>机器学习流水线（machine learning pipeline）</h3><p>像上面这样的一个系统，我们把它称之为<strong>机器学习流水线（machine learning pipeline）</strong>。</p>
<p>下面是OCR的流水线：</p>
<p><img src="/img/17_06_04/006.png" alt=""></p>
<p>具体来说，这幅图表示的就是照片OCR的流水线。</p>
<p>我们有一幅图像，然后传给文字检测系统，识别出文字以后，我们将字段分割为独立的字符，最后我们对单个的字母进行识别。</p>
<p>在很多复杂的机器学习系统中，这种流水线形式都非常普遍。在流水线中会有多个不同的模块，比如在本例中我们有文字检测、字符分割和字母识别，其中每个模块都可能是一个机器学习组件。</p>
<p>如果你要设计一个机器学习系统，其中你需要作出的最重要的决定，就是你要怎样组织好这个流水线。换句话说，在这个照片OCR问题中，你应该如何将这个问题分成一系列不同的模块。你需要设计这个流程，以及你的流水线中的每一个模块，这通常会影响到你最终的算法的表现。</p>
<p>如果你有一个工程师的团队在完成同样类似的任务，那么通常你可以让不同的人来完成不同的模块。我可以假设文字检测这个模块需要大概1到5个人、字符分割部分需要另外1到5个人、字母识别部分还需要另外1到5个人：</p>
<p><img src="/img/17_06_04/007.png" alt=""></p>
<p>因此使用流水线的方式通常提供了一个很好的办法来将整个工作分给不同的组员去完成。（当然所有这些工作都可以由一个人来完成，如果你希望这样做的话。）</p>
<p>在复杂的机器学习系统中，流水线的概念已经渗透到各种应用中。你刚才看到的只是一种照片OCR流水线的运作过程。在接下来的几段视频中，我还将继续向你介绍更多的一些关于流水线的内容。我们还将使用这个例子来展示机器学习中其他一些非常重要的概念。</p>
<h2 id="滑动窗体"><a href="#滑动窗体" class="headerlink" title="滑动窗体"></a>滑动窗体</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/bQhq3/sliding-windows">视频地址</a></p>
<blockquote>
<p>在上一节我们谈到了照片OCR流水线以及其工作原理。我们讲到可以照一张照片然后将其通过一系列机器学习组件来尝试读出图片中的文字信息。</p>
<p>在本节，我想再多介绍一些照片OCR流水线中的组件是如何工作的。</p>
</blockquote>
<p>本节我们介绍一下<strong>滑动窗体(sliding windows)</strong>的分类器。在照片OCR中，我们可以使用<strong>滑动窗体(sliding windows)</strong>分类器来识别图片中的文字。</p>
<h3 id="滑动窗体步骤"><a href="#滑动窗体步骤" class="headerlink" title="滑动窗体步骤"></a>滑动窗体步骤</h3><ul>
<li>1.文字检测</li>
</ul>
<p>滑动窗的第一个步骤是<strong>文字检测(text detection)</strong>。</p>
<p>对于OCR问题，我们想要达到下面这种从图片中识别文字的功能：</p>
<p><img src="/img/17_06_04/008.gif" alt=""></p>
<p>文字识别是计算机视觉中的一个非同寻常的问题，因为取决于你想要找到的文字的长度。这些长方形区域会呈现不同的宽高比。</p>
<h4 id="例子：行人检测问题"><a href="#例子：行人检测问题" class="headerlink" title="例子：行人检测问题"></a>例子：行人检测问题</h4><p>为了更好地介绍图像检测，我们从一个简单一点的行人探测的例子开始。</p>
<p>假设我们想要实现一个从图中识别出行人的应用：</p>
<p><img src="/img/17_06_04/009.gif" alt=""></p>
<p>这个问题似乎比文字检测的问题更简单，因为大部分的行人都比较相似。因此可以使用一个固定宽高比的矩形（就是上图中的红色矩形）来分离出你希望找到的行人。但在文字检测中，文字区域的宽高比就无法固定了。</p>
<p>虽然在行人检测的问题中，行人可能会与相机处于不同的距离位置，因此这些矩形的高度也取决于他们离相机的距离远近，但这个矩形的宽高比应该是一样的。</p>
<p>为了建立一个行人检测系统，以下是具体步骤：</p>
<ul>
<li><p>1.指定行人矩形比例</p>
<p>  假如说我们把宽高比标准化到$82：36$这样一个比例。</p>
</li>
<li><p>2.搜集样本</p>
<p>  接下来我们要做的就是到街上去收集一大堆正负训练样本。</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">正样本$(y=1)$</th>
<th style="text-align:center">负样本$(y=0)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_06_04/010.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_06_04/011.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>在典型的行人识别应用中，我们可以有1000个到10000个训练样本不等，甚至更多。</p>
</blockquote>
<ul>
<li><p>3.训练算法</p>
<p>  如果你能得到大规模训练样本的话，然后你要做的事是训练一个神经网络或者别的什么学习算法，输入这些$82×36$维的图像块，然后对$y$进行分类，把图像块分成”有行人”和”没有行人”两类。</p>
<p>  因此这一步实际上是一个监督学习。你通过一个图像块然后决定这个图像块里有没有行人。</p>
</li>
</ul>
<hr>
<p>现在假如我们获得一张新的测试样本图像：</p>
<p> <img src="/img/17_06_04/012.png" width = "500" height = "500" align=center /></p>
<p>我们如果想要从这张图中找到行人，首先要做的是对这个图像取一小块长方形（$82×36$）：</p>
<p> <img src="/img/17_06_04/013.png" width = "500" height = "500" align=center /></p>
<p>我们将这个图像块通过我们训练得到的分类器来确定这个图像块中是不是有行人。</p>
<p>然后我们把这个绿色的长方形图片滑动一点点：</p>
<p> <img src="/img/17_06_04/014.png" width = "500" height = "500" align=center /></p>
<p>得到一个新的图像块，并同样把它传入我们的分类器 看看这里面有没有行人。</p>
<p>完成这一步之后，我们再向右滑动一点窗口，同样地把图像块传入分类器。</p>
<p>你每次滑动窗口的大小是一个参数，通常被称为<strong>步长(step size)</strong>，有时也称为<strong>步幅参数(stride parameter)</strong>。步长为1代表每次移动一个像素，这样通常表现得最好但可能计算量比较大，因此通常使用4个像素、或者8个像素、或者更多像素作为步长值。</p>
<p>通过固定步长，你的窗体去逐步扫描完整个图，并在每一步扫描过程中，将窗体扫描到的图片代入之前训练的行人识别的分类器中，直到窗体滑过图片中所有不同的位置：</p>
<p><img src="/img/17_06_04/015.gif" alt=""></p>
<p>但这个矩形是非常小的，只能探测到某种尺寸的行人。接下来我们要做的是看看更大的图像块。因此我们用更大矩形来滑过图片，传入分类器运行：</p>
<p><img src="/img/17_06_04/016.gif" alt=""></p>
<blockquote>
<p>顺便说一下，<strong>“用更大一些的图像块”</strong>的意思是当你用这样的图像块时，我们需要将扫描得到的图片重新压缩到分类器可以识别的尺寸($82×36$像素)。</p>
</blockquote>
<p>以此类推，接下来你可以用一个更大的矩形，以同样的方式滑动窗口。直到完成最后的扫描过程之后，你的算法应该就能检测出图像中是否出现行人了。</p>
<p>因此整个步骤就是：训练一个分类器，然后用一个滑动窗分类器来找出图像中出现的行人。</p>
<hr>
<h4 id="OCR文字检测"><a href="#OCR文字检测" class="headerlink" title="OCR文字检测"></a>OCR文字检测</h4><p>接下来我们转向文字识别的例子，对于照片OCR流水线中，要检测出文字需要的步骤如下：</p>
<h5 id="训练分类器"><a href="#训练分类器" class="headerlink" title="训练分类器"></a>训练分类器</h5><p>跟行人检测类似你也可以先收集一些带标签的训练集：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">正样本：出现文字的区域$(y=1)$</th>
<th style="text-align:center">负样本：没有出现文字的区域$(y=0)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_06_04/017.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_06_04/018.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>通过使用这些训练集来训练识别文字的分类器。</p>
<h5 id="滑动窗体，识别文字区域"><a href="#滑动窗体，识别文字区域" class="headerlink" title="滑动窗体，识别文字区域"></a>滑动窗体，识别文字区域</h5><p>训练完了以后，我们就可以把它应用到测试集图片中了。我们以这幅图片为例：</p>
<p><img src="/img/17_06_04/001.png" width = "500" height = "500" align=center /></p>
<p>这里我们用一个固定的比例的矩形作为窗体来运行滑动窗体。如果我这样做的话，最终得到的结果是这样的：</p>
<p><img src="/img/17_06_04/019.png" width = "500" height = "500" align=center /></p>
<p>白色区域代表找到了文字的区域，黑色区域代表没有找到文字。不同的灰度表示分类器给出的输出结果的概率值，所以比如有些灰色的阴影表示分类器在这片区域似乎发现了文字，但并不十分确信；而比较白亮的区域则表示分类器预测这个区域有文字 的概率比较大。</p>
<h5 id="代入“展开器”-expansion-operator"><a href="#代入“展开器”-expansion-operator" class="headerlink" title="代入“展开器”(expansion operator)"></a>代入“展开器”(expansion operator)</h5><p>现在我们还没完成文字检测呢，因为我们实际上想做的是在图像中有文字的各区域都画上矩形窗，所以我们还需要完成一步。我们取出分类器的输出，然后输入到一个被称为<strong>“展开器”(expansion operator)</strong>的东西。</p>
<p>展开器的作用就是取过这张图片，对每一个白色的小点都扩展为一块白色的区域。</p>
<p><img src="/img/17_06_04/020.png" width = "800" height = "500" align=center /></p>
<p>从数学上来讲，对于每一个像素，我们都考察一下它是不是在左边这幅图中的某个白色像素的范围之内。比如说，某一个像素点在最左边那幅图中白色像素点的五或十个像素范围中，那么我们将把右边那幅图的相同像素设为白色。</p>
<p>这样做的效果就是，我们把左边图中的所有的白色小点都扩展了一下，让它们都变大了一些。现在我们可以根据右边的这张图锁定那些连接部分（也就是这些连续的白色区域）然后围绕着它们画个框就行了。</p>
<p>具体来讲，如果我们分析这些白色区域，我们可以简单地凭直觉来判断哪些区域是比较奇怪的，因为我们知道有文字的区域应该不是很高的，而是比较宽的。所以我们忽略那些又高又瘦的白块，这两个：</p>
<p><img src="/img/17_06_04/021.png" width = "500" height = "500" align=center /></p>
<p>然后对剩下的那些，从比例上来看比较像正常的文字区域的白块画上矩形窗：</p>
<p><img src="/img/17_06_04/022.png" width = "500" height = "500" align=center /></p>
<p>这个例子中漏掉了一些字，因为这些字的宽高比看起来不正常，以及写在玻璃上的文字比较难读出来，但整体来讲，检测效果还不错。</p>
<p>这就是使用滑动窗来进行文字检测。</p>
<p>找到这些有文字的长方形以后，我们现在就能够剪下这些图像区域，然后应用流水线的后面步骤对文字进行识别。</p>
<h5 id="字符分割"><a href="#字符分割" class="headerlink" title="字符分割"></a>字符分割</h5><p>如果你还记得的话，你应该知道流水线的第二步是字符分割。所以给出下面这样的图像我们应该怎样分割出图像中的单个字符呢？</p>
<p><img src="/img/17_06_04/023.png" width = "500" height = "200" align=center /></p>
<p>同样地，我们还是使用一种监督学习算法，用一些是否存在<strong>字符之间的分割区域</strong>的正样本和一些负样本来训练一个分类器。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">正样本：存在字符间分各区域$(y=1)$</th>
<th style="text-align:center">负样本：不存在字符间分割区域$(y=0)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_06_04/024.png" width = "300" height = "200" align=center /></td>
<td style="text-align:center"><img src="/img/17_06_04/025.png" width = "300" height = "200" align=center /></td>
</tr>
</tbody>
</table>
</div>
<p>因此我们要做的就是使用神经网络或者其他的学习算法来训练一个分类器，试着对这些正负样本进行分类。训练好这个分类器以后，我们就要把这个分类器应用到我们文字中。</p>
<p>使用同样的<strong>窗体滑动</strong>方式（只不过使用的分类器不同），扫描文字检测系统输出的文字区域图像：</p>
<p><img src="/img/17_06_04/026.png" width = "300" height = "200" align=center /></p>
<p>分类器告诉我们$y=1$时，就意味着我们需要在中间画一条线，分开两个字符，否则就跳过。如果正常的话，分类器会告诉我们应该在什么地方来将图像分割为独立的字符。</p>
<p><img src="/img/17_06_04/027.png" width = "300" height = "200" align=center /></p>
<p><img src="/img/17_06_04/028.png" width = "300" height = "200" align=center /></p>
<p><img src="/img/17_06_04/029.png" width = "300" height = "200" align=center /></p>
<h2 id="获取大量数据和人工数据"><a href="#获取大量数据和人工数据" class="headerlink" title="获取大量数据和人工数据"></a>获取大量数据和人工数据</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/K0XQT/getting-lots-of-data-and-artificial-data">视频地址</a></p>
<blockquote>
<p>想要获得一个高效的机器学习系统，我们通常需要在低偏差的算法中代入大量的训练数据。但是我们如何获取大量的训练数据呢？</p>
<p>其实在机器学习中有一个很棒的想法，叫做<strong>“人工数据合成”（artificial data synthesis）</strong>。</p>
</blockquote>
<p><strong>人工数据合成</strong>的概念通常包含两种不同的变体：</p>
<ul>
<li>第一种，是我们白手起家来创造新的数据。</li>
<li>第二种，是我们通过扩大一个已经存在的带标签的小的训练集，来获得数据。</li>
</ul>
<p>这节课中我们将对这两种方法进行介绍。</p>
<h3 id="创造新样本"><a href="#创造新样本" class="headerlink" title="创造新样本"></a>创造新样本</h3><p>为了介绍人工数据合成的概念，让我们还是用之前用过的照片OCR流水线中的字母识别问题来举例。 </p>
<p>假如我们可以在别处收集到一大堆标签数据：</p>
<p><img src="/img/17_06_04/030.png" width = "300" height = "300" align=center /></p>
<p>我们的目标就是对任意一个图像块，我们能够识别出图像中心的那个字符。</p>
<p>同时，为了方便，我把这些图像都视为灰度图像而不是彩色图像（实际上用彩色的图像对这个问题的解决也起不了多大作用）。</p>
<p>我们有了这些原始数据，那么我们怎样才能获得一个更大的训练集呢？</p>
<p>众所周知，现代计算机中通常都有一个很大的字体库，我们也可以下载到很多免费的字体样式：</p>
<p><img src="/img/17_06_04/031.png" width = "300" height = "200" align=center /></p>
<p>所以如果你想要获得更多的训练样本，其中一种方法是你可以采集同一个字符的不同种字体，然后将这些字符加上不同的随机背景来创造训练样本。</p>
<p>通过这样的操作之后，你可以得到这样一个合成之后的训练集：</p>
<p><img src="/img/17_06_04/032.png" width = "300" height = "200" align=center /></p>
<blockquote>
<p>在生成模拟数据的时候，需要考虑对模拟的样本进行模糊、变形、旋转等操作，因为这样创造出来的样本比较真实。如果你草率的生成一些样本，那么最终训练出来的算法可能效果不是很好。</p>
</blockquote>
<p>因此通过使用合成的数据，实际上已经获得了无限的训练样本。这就是人工数据合成。</p>
<h3 id="通过已有的样本创造新样本"><a href="#通过已有的样本创造新样本" class="headerlink" title="通过已有的样本创造新样本"></a>通过已有的样本创造新样本</h3><p>人工数据合成的第二种方法是使用你已经有的样本。</p>
<p>我们选取一个真实的样本，然后通过添加别的数据来扩大你的训练集。</p>
<p>比如下图中字母A，来自于一个真实的图像（不是一个合成的图像）：</p>
<p><img src="/img/17_06_04/033.png" width = "100" height = "100" align=center /></p>
<p>为了方便描述，我在图像上加了一些灰色的网格。实际上是没有这些格子的。你要做的就是取出这个图像，进行人工扭曲，或者人工变形：</p>
<p><img src="/img/17_06_04/034.png" width = "500" height = "500" align=center /></p>
<p>这样从一个图像A就能生成16种新的样本。</p>
<p>所以用这种方法，你可以把一个很小的带标签训练集突然一下扩大，得到更多的训练样本。</p>
<p>同样地，要把这个概念投入应用，还是需要仔细考虑的。比如要考虑什么样的变形是合理的。</p>
<h4 id="引入形变方法的注意事项"><a href="#引入形变方法的注意事项" class="headerlink" title="引入形变方法的注意事项"></a>引入形变方法的注意事项</h4><p>如果你想要通过对原始数据形变产生新数据，那么在选择要引入的干扰或变形要能代表你可能会在 测试集中看到的噪音源或干扰项。</p>
<p>比如对于字符识别这个例子，这种扭曲的方法事实上还是很合理的：</p>
<p><img src="/img/17_06_04/035.png" width = "500" height = "300" align=center /></p>
<p>因为这种程度的扭曲，在测试集中是会遇到的。</p>
<p>相对而言，为你的数据添加一些纯随机的噪声，通常来讲是没什么用的。我不确定你从这里能否看出下图中对样本的变形：</p>
<p><img src="/img/17_06_04/036.png" width = "500" height = "300" align=center /></p>
<p>这里我们队这四幅图像中每一个图像的每一个像素都加了一些随机高斯噪声。这其实是完全没有意义的。除非你觉得在你的测试集中会遇到这些像素的噪声，否则的话这些随机的噪声是无意义的，起不到多大作用。</p>
<hr>
<p>人工数据合成的过程并没有什么技巧可言，有时候你只能一遍遍地尝试，然后观察效果。</p>
<p>但你在确定需要添加什么样的变形时，你一定要考虑好你添加的那些额外的变形量是有意义的，能让你产生的训练样本至少在某种程度上是具有一定的代表性，能代表你可能会在测试集中看到的某种图像。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后，我将介绍一些在获取更多训练数据时的注意事项。</p>
<h4 id="尝试获取更多数据之前，先优化好你的分类器"><a href="#尝试获取更多数据之前，先优化好你的分类器" class="headerlink" title="尝试获取更多数据之前，先优化好你的分类器"></a>尝试获取更多数据之前，先优化好你的分类器</h4><p>首先，在考虑如何产生大量人工训练样本之前，通常最好应该先保证你已经有了一个低偏差的分类器。这样得到大量的数据才真的会起作用。</p>
<p>标准的方法是画出学习曲线，然后确保你已经有了一个低偏差或者高方差的分类器。如果你没有得到一个低偏差的分类器，你还可以尝试增大分类器的特征数，或者在神经网络中增大隐藏层单元数，直到你得到一个偏差比较小的分类器。</p>
<p>你一定要避免的是，花了几个星期的时间或者几个月的工夫，考虑好了怎么样能获得比较好的人工合成数据，然后才意识到即使获得了大量的训练数据，自己的学习算法的表现依然没有提高多少。</p>
<h4 id="尝试和你的团队沟通，头脑风暴"><a href="#尝试和你的团队沟通，头脑风暴" class="headerlink" title="尝试和你的团队沟通，头脑风暴"></a>尝试和你的团队沟通，头脑风暴</h4><p>第二，当我在解决机器学习问题时，通常我会问我的团队或者我的学生：“我们要付出多少工作量来获得10倍于我们现有的数据量？”</p>
<p>但让我经常感到有一点吃惊的是，他们的回答都是：“这并不是什么难事，最多花上几天时间，我们就能给一个机器学习问题获得十倍于我们现有数据量的数据。”而且通常来说，如果你能得到10倍的数据量，那么你一般都能让你的学习算法表现更好。</p>
<p>因此，如果你加入某个产品设计小组，要设计某个机器学习的应用产品，可以问问你的团队这个问题。说不定几分钟的头脑风暴以后，你的团队就会想出一种方法，真的一下子能获得10倍的数据量。</p>
<h4 id="“众包”-crowd-sourcing-—人工标记样本"><a href="#“众包”-crowd-sourcing-—人工标记样本" class="headerlink" title="“众包” (crowd sourcing)—人工标记样本"></a>“众包” (crowd sourcing)—人工标记样本</h4><p>另一种很好的办法，我们称之为<strong>“众包” (crowd sourcing)</strong>的办法。</p>
<p>现在已经有一些网站，或者一些服务机构能让你通过网络雇一些人替你完成标记大量训练数据的工作。通常都很廉价。</p>
<p>很明显这种方法，就像学术文献一样，它也是很复杂的，同时取决于标记人的可靠性。</p>
<p>可能<strong>“亚马逊土耳其机器人”（Amazon Mechanical Turk）</strong>就是当前最流行的一个众包选择。</p>
<h2 id="上限分析：流水线上的下一步工作是什么"><a href="#上限分析：流水线上的下一步工作是什么" class="headerlink" title="上限分析：流水线上的下一步工作是什么"></a>上限分析：流水线上的下一步工作是什么</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/LrJbq/ceiling-analysis-what-part-of-the-pipeline-to-work-on-next">视频地址</a></p>
<blockquote>
<p>在前面的课程中，我不止一次地说过<strong>在你开发机器学习系统时，你最宝贵的资源就是你的时间</strong>。</p>
<p>作为一个开发者，你需要正确选择下一步的工作。或者也许你有一个开发团队共同开发一个机器学习系统，同样最宝贵的还是开发系统所花费的时间。</p>
<p>你需要尽量避免的你和你的团队花费了大量时间 在某一个模块上，在几周甚至几个月的努力以后才意识到所有这些付出的劳动，都对你最终系统的表现并没有太大的帮助。</p>
<p>在这本节，我将介绍一下关于<strong>上限分析(ceiling analysis)</strong>的内容。这种方式通常能提供一种很有价值的信号，告诉你流水线中的哪个部分最值得你花时间。</p>
</blockquote>
<h3 id="上限分析主要思想"><a href="#上限分析主要思想" class="headerlink" title="上限分析主要思想"></a>上限分析主要思想</h3><p>依然以<strong>照片OCR流水线</strong>为例：</p>
<p><img src="/img/17_06_04/006.png" width = "500" height = "300" align=center /></p>
<p>当我们面对这样一个流水线时，你应该怎样分配资源呢？哪一个方框最值得你投入精力，投入时间去改善效果呢？</p>
<p>为了回答这个问题，我们可以对学习系统使用一个<strong>数值评价量度</strong>。</p>
<p>假如我们用字符准确度作为这个量度，给定一个 测试样本图像，这个数值就表示我们对测试图像中的文字识别正确的比例。</p>
<p>我们假设整个系统的估计准确率为72%（对测试集上的图像分别运行流水线上的每一个模块操作之后，整个测试集的准确率是72%）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模块</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">整个系统</td>
<td style="text-align:center">72%</td>
</tr>
</tbody>
</table>
</div>
<p>下面是<strong>上限分析</strong>的主要思想：</p>
<p>首先，我们要模拟在<strong>文字检测</strong>准确率100%的情况下，得出当前系统的准确率。（我们可以通过人工的方式找出这种样本）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模块</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">整个系统</td>
<td style="text-align:center">72%</td>
</tr>
<tr>
<td style="text-align:center"><strong>文字检测</strong></td>
<td style="text-align:center"><strong>89%</strong></td>
</tr>
</tbody>
</table>
</div>
<p>然后以同样的方式，得出在<strong>文字检测</strong>，以及<strong>字符切分</strong>准确率100%的情况下，当前系统的准确率。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模块</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">整个系统</td>
<td style="text-align:center">72%</td>
</tr>
<tr>
<td style="text-align:center">文字检测</td>
<td style="text-align:center">89%</td>
</tr>
<tr>
<td style="text-align:center"><strong>字符切分</strong></td>
<td style="text-align:center"><strong>90%</strong></td>
</tr>
</tbody>
</table>
</div>
<p>最后，我们也要写出在<strong>文字检测</strong>、<strong>字符切分</strong>以及<strong>字符识别</strong>准确率100%的情况下，当前系统的准确率（当然是100%）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模块</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">整个系统</td>
<td style="text-align:center">72%</td>
</tr>
<tr>
<td style="text-align:center">文字检测</td>
<td style="text-align:center">89%</td>
</tr>
<tr>
<td style="text-align:center">字符切分</td>
<td style="text-align:center">90%</td>
</tr>
<tr>
<td style="text-align:center"><strong>字符识别</strong></td>
<td style="text-align:center"><strong>100%</strong></td>
</tr>
</tbody>
</table>
</div>
<p>有了这些数据，我们就知道了每一个模块进行改善它们各自的上升空间是多大。</p>
<p>我们可以看到，如果我们拥有完美的文字检测模块，那么整个系统的表现将会从准确率72%上升到89%，因此效果的增益是17%。这就意味着，如果你在现有系统的基础上花费时间和精力改善文字检测模块的效果，那么系统的表现可能会提高17%。</p>
<p>而相对来讲，如果我们取得完美的字符分割模块，那么最终系统表现只提升了1%。这给我们提供了一个很重要的信息，那就是不管我们投入多大精力在字符分割上，系统效果的潜在上升空间也都是很小很小。所以你就不会让一个比较大的工程师团队花时间忙于字符分割模块，因为通过上限分析我们知道了即使你把字符分割模块做得再好，再怎么完美，你的系统表现最多也只能提升1%。</p>
<p>最后，如果我们取得完美的字符识别模块，那么整个系统的表现将提高10%。所以，同样你也可以分析10%的效果提升值得投入多少工作量。</p>
<h3 id="上限分析的原理"><a href="#上限分析的原理" class="headerlink" title="上限分析的原理"></a>上限分析的原理</h3><p>下面我换一个复杂一点的例子再来演绎一下上限分析的原理。</p>
<p>假如说你想对这张图像进行人脸识别：</p>
<p><img src="/img/17_06_04/037.png" width = "300" height = "500" align=center /></p>
<blockquote>
<p>这是一个偏人工智能的例子，当然这并不是现实中的人脸识别技术，但我想通过这个例子来向你展示一个流水线，并且给你另一个关于上限分析的实例。</p>
</blockquote>
<p>假设我们有如下流水线：</p>
<p><img src="/img/17_06_04/038.png" width = "500" height = "500" align=center /></p>
<p>其中具体步骤对应的操作如下：</p>
<p><strong>预处理（移除背景图）:</strong></p>
<p><img src="/img/17_06_04/039.gif" alt=""></p>
<p><strong>检测人脸:</strong></p>
<p><img src="/img/17_06_04/040.png" width = "300" height = "500" align=center /></p>
<p><strong>眼睛分割:</strong></p>
<p><img src="/img/17_06_04/041.png" width = "300" height = "500" align=center /></p>
<p><strong>鼻子分割:</strong></p>
<p><img src="/img/17_06_04/042.png" width = "300" height = "500" align=center /></p>
<p><strong>嘴分割:</strong></p>
<p><img src="/img/17_06_04/043.png" width = "300" height = "500" align=center /></p>
<p>那么对这个流水线怎么进行上限分析呢？</p>
<p>我们同样需要每次关注一个步骤，来计算整体的准确率：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模块</th>
<th style="text-align:center">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">整个系统</td>
<td style="text-align:center">85%</td>
</tr>
<tr>
<td style="text-align:center">预处理（移除背景图）</td>
<td style="text-align:center">85.1%</td>
</tr>
<tr>
<td style="text-align:center">检测人脸</td>
<td style="text-align:center">91%</td>
</tr>
<tr>
<td style="text-align:center">眼睛分割</td>
<td style="text-align:center">95%</td>
</tr>
<tr>
<td style="text-align:center">鼻子分割</td>
<td style="text-align:center">96%</td>
</tr>
<tr>
<td style="text-align:center">嘴分割</td>
<td style="text-align:center">97%</td>
</tr>
<tr>
<td style="text-align:center">逻辑回归</td>
<td style="text-align:center">100%</td>
</tr>
</tbody>
</table>
</div>
<p>从数据中，我们可以很明显的看出来，预处理阶，准确率提高了0.1%。这是个很明显的信号，它告诉我们即便把背景分割做得很好，但整个系统的表现也并不会提高多少。所以似乎并不值得花太多精力在预处理或者背景移除上。</p>
<p>在每次通过这个系统的时候，随着使用有正确标签的测试集的模块越来越多，整个系统的表现逐步上升，这样你就能很清楚地看到通过不同的步骤，系统的表现增加了多少（比如有了完美的脸部识别，整个系统的表现似乎提高了5.9%，这告诉你也许在脸部检测上多做点努力是有意义的）。</p>
<p>因此，通过<strong>上限分析</strong>，很清楚地指出了哪一个模块是最值得花精力去完善的。</p>
<blockquote>
<p><strong>一个真实的故事</strong>:</p>
<p>原来有一个大概两个人的研究小组，花了整整18个月都在完善背景移除的效果（我不详细地讲具体的细节和原因是什么），就为了得到一个更好的背景移除效果。事实上他们确实研究出了非常复杂的算法，貌似最后还发表了一篇文章，但最终他们发现所有付出的这些劳动，都不能给他们研发系统的整体表现带来比较大的提升。而如果要是之前，他们组某个人做一下上限分析，他们就会提前意识到这个问题。</p>
<p>后来，他们中有一个人跟我说，如果他们之前也做了某种这样的分析，他们就会可以把精力花在 其他更重要的模块上，而不是把18个月花在背景移除上。</p>
</blockquote>
<h3 id="一些建议"><a href="#一些建议" class="headerlink" title="一些建议"></a>一些建议</h3><p>经过这么多年在机器学习中的摸爬滚打，我已经学会了<strong>不要凭自己的直觉来判断应该改进哪个模块</strong>，相反地如果要解决某个机器学习问题，最好能把问题分成多个模块，然后做一下上限分析。</p>
<p>这通常它可以告诉你一个更可靠的，关于该把劲儿往哪儿使的方法。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/06/03/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%8D%81%E5%91%A8%20(2)%E9%AB%98%E7%BA%A7%E4%B8%BB%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/03/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%8D%81%E5%91%A8%20(2)%E9%AB%98%E7%BA%A7%E4%B8%BB%E9%A2%98/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第十周 (2)高级主题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-06-03 12:19:00" itemprop="dateCreated datePublished" datetime="2017-06-03T12:19:00+00:00">2017-06-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/ABO2q/online-learning">视频地址</a></p>
<blockquote>
<p>在本节，我将会讨论一种新的大规模的机器学习机制，叫做<strong>在线学习机制</strong>。在拥有连续一波数据或连续的数据流涌进来，而我们又需要一个算法来从中学习的时候来模型化问题时，我们就需要用到<strong>在线学习机制</strong>。 </p>
<p>今天许多大型网站，或者许多大型网络公司，都在使用不同版本的在线学习机制算法从大批的涌入又离开网站的用户身上进行学习。</p>
<p>特别要提及的是，如果你有一个由连续的用户流引发的连续的数据流，用户流进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息，来优化一些关于网站的决策。</p>
</blockquote>
<h3 id="在线学习机制举例"><a href="#在线学习机制举例" class="headerlink" title="在线学习机制举例"></a>在线学习机制举例</h3><h4 id="运输服务对于定价的预测"><a href="#运输服务对于定价的预测" class="headerlink" title="运输服务对于定价的预测"></a>运输服务对于定价的预测</h4><p>假如你有一个提供运输服务的网站，在网站上提供用户选择包裹邮寄的起始地址和目的地址，并显示运输价格。然和我们需要根据用户对给出邮费价格的选择接受或者走掉，来作为正样本（$y=1$）和负样本（$y=0$），通过这些样本来构建学习算法，来帮助我们找到用户的特点，从而给出一个合理的邮费价格预测。</p>
<p>我们使用这些优化后的价格，很有可能会提高我们的利润。</p>
<p>对于持续运行的网站来说，以下就是在线学习算法要做的：</p>
<ul>
<li>网站一直保持在线学习状态。</li>
<li>当一个用户偶然访问网站时，我们会得到当前用户对应的数据$(x,y)$(特征$x$是指用户所指定的起始地与目的地以及我们这一次提供给客户的价格，而$y$的取值是0或1，具体值取决于用户最终是否选择了我们的运输服务)。</li>
<li>通过该用户的数据$(x,y)$来更新$\theta$。<ul>
<li>$\theta_j := \theta_j - \alpha(h_{\theta}(x)-y)x_j \ \ \ \  \ \ \ (j=0,…,n)$</li>
</ul>
</li>
</ul>
<p>与以往的学习过程不同的是，在线学习中，每次梯度下降使用的这个当前用户的样本数据$(x,y)$，我们使用一次之后就丢弃了，我们永远都不会再次使用它。这就是为什么我们在一个时间点只会处理一个样本的原因。</p>
<p>如果你真的运行一个大型网站，在这个网站里你有一个连续的用户流登陆网站，那么这种在线学习算法，是一种非常合理的算法。因为数据本质上是自由的，而且数据本质上是无限的，那么或许就真的没必要重复处理一个样本。当然，如果我们只有少量的用户，那么我们就不选择像这样的在线学习算法，这种情况下最好是要保存好所有的数据，然后对这个数据集使用某种算法。</p>
<p>我也必须要提到一个这种在线学习算法会带来的有趣的效果，那就是它可以对正在变化的用户偏好进行调适。</p>
<h4 id="搜索中对于搜索结果的优化"><a href="#搜索中对于搜索结果的优化" class="headerlink" title="搜索中对于搜索结果的优化"></a>搜索中对于搜索结果的优化</h4><p>在搜索引擎中，我们想使用一种学习机制来学习如何反馈给用户好的搜索列表。</p>
<p>举个具体的例子，假如你有一个在线卖手机的网站，你有一个可以提供搜索的用户界面。在你的网站中，有100部正在售卖的手机，用户每次搜索时会提供10部手机的搜索结果。那么当用户输入类似“安卓 手机 1080p 摄像头”这样的搜索词时，我们应该推荐哪十部手机给用户呢？</p>
<p>我们想要做的是拥有一个在线学习机制来帮助我们 找到在这100部手机中哪十部手机是我们真正应该反馈给用户的，而且这个返回的列表是对类似这样的用户搜索条目最佳的回应。</p>
<p>接下来要说的是一种解决问题的思路。</p>
<p>对于每一个手机以及一个给定的用户搜索命令，我们可以构建一个特征矢量$x$，那么这个特征矢量$x$可能会抓取手机的各种特点，它可能会抓取类似于用户搜索命令与这部电话的类似程度有多高这样的信息，我们获取类似于：这个用户搜索命令中有多少个词，可以与这部手机的名字相匹配；或者这个搜索命令中有多少词，与这部手机的描述相匹配。</p>
<script type="math/tex; mode=display">
x=手机的特征，\\\\用户的搜索词中有多少个词与手机名称匹配，\\\\用户的搜索词中有多少个词与手机的描述匹配，\\\\等等</script><p>所以特征矢量$x$获取手机的特点，并且它还会获取这部手机与搜索命令的结果在各个方面的匹配程度。我们想要做的就是估测一个概率，这个概率是指用户将会点进某一个特定的手机的链接的概率。因为我们想要给用户提供那些他们很可能在浏览器中点进去查看的手机，所以我定义$y=1$是指用户点击了手机的链接，而$y=0$是指用户没有点击链接。</p>
<script type="math/tex; mode=display">
y=1 （如果用户点击了链接） \\\\
y=0 （用户没有点击链接）</script><p>然后我们想要做的就是学习到用户将会点击某一个特定的手机的概率：</p>
<script type="math/tex; mode=display">
p(y=1|x;\theta)</script><p>你知道的，特征$x$获取了手机的特点以及搜索条目与手机的匹配程度。这类问题其实被称作<strong>预估点击率CTR</strong>，它仅仅代表这学习用户将点击某一个特定的、你提供给他们的链接的概率。所以<strong>CTR</strong>是<strong>点击率(Click Through Rate)</strong>的简称。</p>
<p>如果你能够估计任意一个特定手机的点击率，我们可以做的就是利用这个来给用户展示十个他们最有可能点击的手机。因为从这一百个手机中，我们可以计算出每一部手机的可能的点击率，然后选择10部用户最有可能点击的手机，那么这就是一个非常合理的来决定要展示给用户的十个搜索结果的方法。</p>
<p>更明确地说，假定每次用户进行一次搜索，我们回馈给用户十个结果。在线学习算法会真正地提供给我们十个$(x,y)$数据对样本。每当一个用户来到 我们网站时就给了我们十个样本，因为对于这十部我们选择要展示给用户的手机的每一个我们会得到 一个特征矢量x，而且对于这10部手机中的任何一个手机，我们还会得到$y$的取值。这些取值是根据用户有没有点击那个网页链接来决定的。这样运行此类网站的一种方法就是连续给用户展示你的十个最佳猜测，这十个推荐是指用户可能会喜欢的其他的手机，那么每次一个用户访问，你将会得到十个$(x,y)$样本数据对，然后利用一个在线学习 算法来更新你的参数。更新过程中会对这十个样本利用10步梯度下降法，然后你可以丢弃你的数据了。</p>
<p>如果你真的拥有一个连续的用户流进入你的网站，这将会是一个非常合理的学习方法，来学习你的算法中的参数从而来给用户展示十部他们最有可能点击查看的手机。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>上面几个例子的原理，你应该也能懂得其他应用的原理。例如优惠券推荐、新闻推荐等。</p>
<p>而且实际上，如果你有一个协作过滤系统，你可以想象到一个协作过滤系统可以给你更多的特征，这些特征可以整合到逻辑回归的分类器，从而可以尝试着预测对于你可能推荐给用户的不同产品的点击率。</p>
<p>这就是<strong>在线学习机制</strong>，与<strong>随机梯度下降算法</strong>非常类似，唯一的区别的是我们不会使用一个固定的数据集，而是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去。而且如果你对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。</p>
<p>当然在线学习的一个优点，就是如果你有一个变化的用户群、又或者你在尝试预测的事情在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。</p>
<h2 id="Map-Reduce-和数据并行"><a href="#Map-Reduce-和数据并行" class="headerlink" title="Map Reduce 和数据并行"></a>Map Reduce 和数据并行</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/10sqI/map-reduce-and-data-parallelism">视频地址</a></p>
<blockquote>
<p>在之前，我们讨论了<strong>随机梯度下降</strong>，以及梯度下降算法的其他一些变种，包括如何将其运用于在线学习。然而所有这些算法都只能在一台计算机上运行，但有的时候，机器学习的运算量巨大，以至于单台机器处理起来特别耗时，所以人们希望能在多台机器上同时执行运算来提高效率。</p>
<p>在本节中，我将介绍进行大规模机器学习的另一种方法，称为<strong>Map Reduce</strong>。尽管我们用了很多内容来讲解随机梯度下降算法，而对于<strong>Map Reduce</strong>的介绍比较少，但是请不要根据我们介绍内容的长短来判断哪一种技术更加重要。事实上 许多人认为<strong>Map Reduce</strong>与梯度梯度下降算法相比至少是同等重要的，还有人认为<strong>Map Reduce</strong>甚至比梯度下降方法更重要。</p>
<p>我们之所以在<strong>Map Reduce</strong>上花的时间比较少，只是因为它相对简单，容易解释。然而实际上相比于随机梯度下降方法，<strong>Map Reduce</strong>能够处理更大规模的问题。</p>
</blockquote>
<h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p>假设我们要拟合一个线性回归模型，或者逻辑回归模型，或者其他的什么模型。让我们再次从随机梯度下降算法开始：</p>
<p><img src="/img/17_06_03/001.png" alt=""></p>
<p>假设我们有400个样本$m=400$（实际应用中会比这个数字大很多）：</p>
<script type="math/tex; mode=display">
(x^{(1)},y^{(1)}),...,
(x^{(400)},y^{(400)})</script><p>根据<strong>Map Reduce</strong>的思想，一种解决方案是将训练集划分成几个不同的子集，然后分别放在多台机器上并行的处理我们的训练数据（这里我们把数据集划分成四份，并且放在4台机器上并行处理）。</p>
<p>每台机器内部都去计算对应的子集的求和运算：</p>
<p><img src="/img/17_06_03/002.png" alt=""></p>
<p>最后，当这些计算机全都完成了各自的工作，我们将这些临时变量收集到一起，送到一个中心计算服务器，这台服务器会将这些临时变量合并起来：</p>
<p><img src="/img/17_06_03/003.png" alt=""></p>
<p>图中右侧更新的公式如下：</p>
<script type="math/tex; mode=display">
\theta_j :=
\theta_j - \alpha\frac{1}{400}
(temp_j^{(1)} + temp_j^{(2)} + temp_j^{(3)} + temp_j^{(4)}) 
\\\\
(j = 0,...,n)</script><p>其实这个公式计算的数值和原先的梯度下降公式计算的数值是完全一样的。</p>
<h3 id="Map-Reduce工作原理"><a href="#Map-Reduce工作原理" class="headerlink" title="Map Reduce工作原理"></a>Map Reduce工作原理</h3><blockquote>
<p>这里值得说明的一点是<strong>Map Reduce</strong>基本思想来自于<strong>Jeffrey Dean</strong>和<strong>Sanjay Ghemawat</strong>这两位研究者。其中<strong>Jeffrey Dean</strong>是硅谷最为传奇的一位工程师，今天Google所有的服务所依赖的后台基础架构，有很大一部分是他创建的。</p>
</blockquote>
<p>总结来说<strong>Map Reduce</strong>技术是这么工作的：</p>
<ul>
<li>将原始训练样本均分成4份</li>
<li>将这4份训练样本的子集送给4台不同的计算机，每一台计算机对四分之一的训练数据进行求和运算</li>
<li>最后这4个求和结果 被送到一台中心计算服务器负责对结果进行汇总。</li>
</ul>
<p><img src="/img/17_06_03/004.png" alt=""></p>
<p>通过<strong>Map Reduce</strong>技术，我们相较于之前的方式提升了4倍的速度。</p>
<p>特别的，如果没有网络延时也不考虑通过网络来回传输数据所消耗的时间，那么你可能可以得到4倍的加速。当然，在实际工作中，因为网络延时数据汇总额外消耗时间，以及其他的一些因素，你能得到的加速总是略小于4倍的。但是，不管怎么说，这Map Reduce算法确实让我们能够处理之前单台机器无法处理的大规模数据。</p>
<h3 id="使用Map-Reduce的条件"><a href="#使用Map-Reduce的条件" class="headerlink" title="使用Map Reduce的条件"></a>使用Map Reduce的条件</h3><p>如果你打算将Map Reduce技术用于加速某个机器学习算法，也就是说你打算运用多台不同的计算机并行的进行计算，那么你需要问自己一个很关键的问题，那就是<strong>你的机器学习算法是否可以表示为训练样本的某种求和</strong>。</p>
<p>事实证明，很多机器学习算法的确可以表示为关于训练样本的函数求和。而在处理大数据时，这些算法的主要运算量在于对大量训练数据求和。</p>
<h3 id="Map-Reduce处理逻辑回归"><a href="#Map-Reduce处理逻辑回归" class="headerlink" title="Map Reduce处理逻辑回归"></a>Map Reduce处理逻辑回归</h3><p>对于某些使用<strong>逻辑回归</strong>的优化算法（比如说<strong>LBFGS算法</strong>，或者<strong>共轭梯度算法</strong>等等），我们需要计算两个值：</p>
<h4 id="计算优化目标代价函数"><a href="#计算优化目标代价函数" class="headerlink" title="计算优化目标代价函数"></a>计算优化目标代价函数</h4><p>首先，我们需要提供一种方法用于计算优化目标的代价函数值。比如逻辑回归的代价函数计算如下：</p>
<p><img src="/img/17_06_03/005.png" alt=""></p>
<p>如果你想在10台计算机上并行计算，那么你需要将训练样本分给这10台计算机，让每台计算机计算10份之一训练数据。</p>
<h4 id="计算梯度下降的偏导数项"><a href="#计算梯度下降的偏导数项" class="headerlink" title="计算梯度下降的偏导数项"></a>计算梯度下降的偏导数项</h4><p>另外，高级优化算法还需要提供某种偏导数的计算方法。对于逻辑回归，这些偏导数可以表示为训练数据的求和：</p>
<p><img src="/img/17_06_03/006.png" alt=""></p>
<p>因此，和之前的例子类似，你可以让每台计算机只计算部分训练数据上的求和。最后当这些求和计算完成之后，求和结果会被发送到一台中心计算服务器上，这台服务器将对结果进行再次求和。这等同于对临时变量$temp^{(i)}_j$进行求和。而这些临时标量是第$i$台计算机算出来的。</p>
<p>中心计算服务器对这些临时变量求和得到了总的代价函数值，以及总的偏导数值，然后你可以将这两个值传给高级优化函数。</p>
<p>因此更广义的来说，通过将机器学习算法表示为 求和的形式，或者是训练数据的函数求和形式，你就可以运用<strong>Map Reduce</strong>技术来将算法并行化，这样就可以处理大规模数据了。</p>
<h3 id="单台设备运行Map-Reduce"><a href="#单台设备运行Map-Reduce" class="headerlink" title="单台设备运行Map Reduce"></a>单台设备运行Map Reduce</h3><p>目前我们只讨论了在多台计算机上运用<strong>Map Reduce</strong>技术实现并行计算。但实际上有时即使我们只有一台计算机，我们也可以运用这种技术。</p>
<p>具体来说，现在的许多计算机都是多核的，你可以有多个CPU，而每个CPU又包括多个核。</p>
<p>如果你有一个很大的训练样本，那么你可以使用一台四核的计算机，然后将训练样本分成4份，让每一个核处理其中一份子样本：</p>
<p><img src="/img/17_06_03/007.png" alt=""></p>
<p>这样，在单台计算机或者单个服务器上，你也可以实现利用Map Reduce技术来划分计算任务了。</p>
<p>相对于多台计算机，这样在单台计算机上使用Map Reduce技术的一个优势在于：你不需要担心网络延时问题，因为所有的通讯所有的来回数据传输都发生在一台计算机上。</p>
<h4 id="一个细节问题"><a href="#一个细节问题" class="headerlink" title="一个细节问题"></a>一个细节问题</h4><p>如果你的算法实现使用的是某种自动利用多核的线性代数运算库，那么你就没有必要去手动在单台机器上实现Map Reduce了。但并不是所有的库都会自动并行运算。</p>
<h3 id="一个广告：Hadoop"><a href="#一个广告：Hadoop" class="headerlink" title="一个广告：Hadoop"></a>一个广告：Hadoop</h3><p>今天，网上有许多优秀的开源Map Reduce实现。</p>
<p>实际上一个称为<strong>Hadoop</strong>的开源系统已经拥有了众多的用户。通过自己实现Map Reduce算法 或者使用别人的开源实现，你就可以利用Map Reduce技术来并行化机器学习算法。这样你的算法将能够处理单台计算机处理不了的大数据。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/06/01/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%8D%81%E5%91%A8%20(1)%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/01/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%8D%81%E5%91%A8%20(1)%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第十周 (1)大数据集梯度下降</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-06-01 22:27:00" itemprop="dateCreated datePublished" datetime="2017-06-01T22:27:00+00:00">2017-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="处理大数据的学习算法"><a href="#处理大数据的学习算法" class="headerlink" title="处理大数据的学习算法"></a>处理大数据的学习算法</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/CipHf/learning-with-large-datasets">视频地址</a></p>
<blockquote>
<p>在接下来的几节中，我们会讲<strong>大规模的机器学习</strong>（就是用来处理大数据的算法）。如果我们看近5到10年的机器学习的历史，我们会发现现在的学习算法比5年前的好很多，其中的原因之一就是我们现在拥有很多可以训练算法的数据。</p>
</blockquote>
<p>在机器学习领域有一种说法是：</p>
<ul>
<li><p>“It`s not who has the best algorithm that wins. It`s who has the most data.”</p>
</li>
<li><p>“谁的数据最多，谁才能笑到最后。”</p>
</li>
</ul>
<p>因此，我们更期望在大的数据集上做训练。</p>
<p>但训练大的数据集也有它自己的问题，特别是计算量的问题。</p>
<p>假设我们的训练集的大小$m=100,000,000$。</p>
<blockquote>
<p>这对于现代的数据集其实是很现实的。比如对于美国的人口普查数据集来说美国有3亿人口，我们通常都能得到上亿条的数据。</p>
<p>如果我们看一下很受欢迎的网站的浏览量，我们也很容易得到上亿条的记录。</p>
</blockquote>
<p>假设我们要训练一个线性回归模型或者是逻辑回归模型，这是梯度下降的规则：</p>
<p><img src="/img/17_06_01/001.gif" alt=""></p>
<p>当你在计算梯度下降的时候，这里的$m$是一个上亿的值时，你需要通过计算上亿个数的导数项的和来计算仅仅一步的梯度下降。</p>
<p>在接下来的内容中，我们会介绍如何优化这个算法，或者换一个更高效率的算法。</p>
<p>在这一系列讲大型的机器学习的课程后，我们会知道如何拟合包括上亿数据的<strong>线性回归</strong>、<strong>逻辑回归</strong>、<strong>神经网络</strong>等等的例子。</p>
<p>当然，在我们训练一个上亿条数据的模型之前，我们还应该问自己为什么不用几千条数据呢？也许我们可以随机从上亿条的数据集里选个一千条的子集，然后用我们的算法计算。</p>
<p>在我们投入精力和开发软件来训练大数据的模型之前，我们往往会在一个比较小的数据集上来验证算法是否合适。</p>
<p>我们通常的方法是画出学习曲线，如果你画了学习曲线而且你的训练目标看上去像这样：</p>
<p><img src="/img/17_06_01/002.png" alt=""></p>
<p>$J_{train}(\theta)$是训练集上的代价函数，$J_{cv}(\theta)$是验证集上的代价函数。从图像中可以看出来，这个算法看起来像是处于<strong>高方差</strong>的状态。因此，我们需要增加训练集。</p>
<p>相比之下，如果你画出的学习曲线是这样的：</p>
<p><img src="/img/17_06_01/003.png" alt=""></p>
<p>这看起来像经典的<strong>高偏差</strong>学习算法。在这个例子中，我们可以看出来，数据量增长到一定程度之后，算法并不会好很多。因此我们没必要花费精力来扩大算法的规模。当然，如果你遇到了这种情况，一个很自然的方法是多加一些特征；或者在你的神经网络里加一些隐藏的单元等等。通过这些操作，你的算法的表现会越来越趋于第一种情况的图形：</p>
<p><img src="/img/17_06_01/002.png" alt=""></p>
<p>而这个过程中，我们应该花时间在添加基础设施来改进算法，而不是用多于一千条数据来建模，因为改进算法会更加有效果。</p>
<p>所以在大规模的机器学习中，我们喜欢找到合理的计算量的方法或高效率的计算量的方法来处理大的数据集。</p>
<h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochastic-gradient-descent">视频地址</a></p>
<blockquote>
<p>对于线性回归、逻辑回归、神经网络等等很多机器学习算法，其实现都是通过得出某个代价函数或者某个最优化的目标来实现的，然后使用梯度下降这样的方法来求得代价函数的最小值。当我们的训练集较大时，梯度下降算法则显得计算量非常大。</p>
<p>在本节，我想介绍一种跟普通梯度下降不同的方法：<strong>随机梯度下降(stochastic gradient descent)</strong>。用这种方法我们可以将算法运用到较大训练集的情况中。</p>
</blockquote>
<h3 id="梯度下降回顾"><a href="#梯度下降回顾" class="headerlink" title="梯度下降回顾"></a>梯度下降回顾</h3><blockquote>
<p>关于<strong>随机梯度下降</strong>我们这里虽然是以梯度下降为例，但其思想也可以应用于其他的学习算法中。</p>
</blockquote>
<p>首先来复习一下之前学过的<strong>线性回归模型</strong>，我们的假设函数如下：</p>
<script type="math/tex; mode=display">
h\_{\theta}(x)=
\sum\_{j=0}^n\theta\_jx\_j</script><p>代价函数如下：</p>
<script type="math/tex; mode=display">
J\_{train}(\theta)=
\frac{1}{2m}
\sum\_{i=1}^m
(h\_\theta(x^{(i)})-y^{(i)})^2</script><p>如果我们的参数是一个二维向量，只有$\theta_1$和$\theta_2$，那么它在三维空间中的代价函数图形如下：</p>
<p><img src="/img/17_06_01/004.png" alt=""></p>
<p>我们的梯度下降内部循环执行的操作如下：</p>
<p><img src="/img/17_06_01/005.png" alt=""></p>
<p>通过反复的更新$\theta_j$，我们的代价函数会逐渐找到局部最优解。</p>
<p>这是梯度下降执行过程中，轮廓图观察到的迭代轨迹的变化情况：</p>
<p><img src="/img/17_06_01/006.png" alt=""></p>
<p>可以看到其沿着梯度下降方向快速的收敛到了全局最小。</p>
<p>而梯度下降在大量数据的情况下，每一次的梯度下降的计算量就变得非常大，因为需要对所有的训练样本求和。因此，这种在每次迭代中对所有数据都进行计算的梯度下降算法也被称为<strong>批量梯度下降(batch gradient descent)</strong>。</p>
<h3 id="随机梯度下降原理"><a href="#随机梯度下降原理" class="headerlink" title="随机梯度下降原理"></a>随机梯度下降原理</h3><p>由于<strong>批量梯度下降</strong>算法在大量数据的情况下，运算量巨大，并不适用。因此这里我们介绍一种能快速处理大量数据的梯度下降算法：<strong>随机梯度下降(stochastic gradient descent)</strong>。</p>
<p>随机梯度下降的代价函数如下：</p>
<script type="math/tex; mode=display">
cost(\theta,(x^{(i)},y^{(i)}))
=\frac{1}{2}
(h\_\theta(x^{(i)})-y^{(i)})^2
\\\\
J\_{train}(\theta)=
\frac{1}{m}
\sum\_{i=1}^m
cost(\theta,(x^{(i)},y^{(i)}))</script><p>随机梯度下降算法中，我们的步骤如下：</p>
<ul>
<li>1.将所有数据打乱。</li>
<li>2.重复执行梯度下降计算，注意，这里每一次计算$\theta_j$不是遍历全部的训练集$m$，而是从$m$个训练集里取出1个样本来计算。所以每次梯度下降的计算只需要一个样本代入计算。这一点是和<strong>批量梯度下降</strong>最大的不同：</li>
</ul>
<p><img src="/img/17_06_01/007.png" alt=""></p>
<p>随机梯度下降过程中，相比于批量梯度下降，会更曲折一些，但每一次的迭代都会更快，因为我们不需要对所有样本求和，每一次只需要保证拟合一个样本即可：</p>
<p><img src="/img/17_06_01/008.png" alt=""></p>
<p>实际上，你运行随机梯度下降和批量梯度下降这两种算法的收敛形式是不同的，你会发现随机梯度下降最终会在靠近全局最小值的区域内徘徊，而不是直接逼近全局最小值并停留在那里。但实际上这并没有太大问题，只要参数最终移动到某个非常靠近全局最小值的区域内，这也会得到一个较为不错的假设。这对于目前绝大部分实际应用的目的来说，已经足够了。</p>
<p>由于随机梯度下降每一次的梯度下降计算只需要计算单个样本，而不是像批量梯度下降那样每次计算全部样本，所以随机梯度下降的下降过程会很快。</p>
<p>最后还有一个细节点，在随机梯度下降的过程中，我们定义了一个外层循环，那么这个外层循环应该定义为多少次呢？</p>
<p><img src="/img/17_06_01/009.png" alt=""></p>
<p>那么这个外层循环设置为多少是合适的呢？</p>
<p>这取决于你样本的数量。一般情况下一次就够了，最多10次是比较典型的。所以我们通常循环执行1到10次。</p>
<p>例如对于300,000,000个美国公民的样本数据来说，如果我们循环执行3次，这里的总计算量就是$3 × 300000000 = 900000000$次。</p>
<p>这就是随机梯度下降算法的全部。</p>
<h2 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/9zJUs/mini-batch-gradient-descent">视频地址</a></p>
<blockquote>
<p>在之前我们讨论了<strong>随机梯度下降</strong>以及它是怎样比批量梯度下降更快的。在本节，让我们讨论基于这些方法的另一种变形，叫做<strong>小批量梯度下降</strong>这种算法有时候甚至比随机梯度下降还要快一点。</p>
</blockquote>
<h3 id="小批量梯度下降概念"><a href="#小批量梯度下降概念" class="headerlink" title="小批量梯度下降概念"></a>小批量梯度下降概念</h3><p>在<strong>批量梯度下降</strong>中每次迭代我们都要用所有的m个样本；然而在<strong>随机梯度下降</strong>中每次迭代我们只用一个样本；<strong>小批量梯度下降</strong>的做法介于它们之间。准确地说在这种方法中我们每次迭代使用b个样本，b是一个叫做<strong>“小批量规模”</strong>的参数。</p>
<p>举个例子来说明：</p>
<p>假如我们有1000个训练样本$m=1000$，用<strong>小批量规模</strong>$b=10$的<strong>小批量梯度下降算法</strong>来处理，我们梯度下降的过程如下：</p>
<p><img src="/img/17_06_01/010.png" alt=""></p>
<p>我们每次梯度下降的过程中，都去取10个样本来计算梯度下降的更新。</p>
<p>这就是<strong>小批量梯度下降算法</strong>。它也比批量梯度下降快很多。</p>
<h3 id="小批量梯度下降vs随机梯度下降"><a href="#小批量梯度下降vs随机梯度下降" class="headerlink" title="小批量梯度下降vs随机梯度下降"></a>小批量梯度下降vs随机梯度下降</h3><p>那么<strong>小批量梯度下降算法</strong>和<strong>随机梯度下降算法</strong>相比，有什么优势呢？</p>
<p>答案是<strong>向量化</strong>。具体来说，<strong>小批量梯度下降算法</strong>比<strong>随机梯度下降算法</strong>更好的原因在于前者每次梯度下降过程中，批量处理的数据可以用一种更向量化的方法来实现，允许你部分并行计算10个样本的和，而<strong>随机梯度下降算法</strong>每次只去计算一个样本，没有太多的并行计算。</p>
<p><strong>小批量梯度下降算法</strong>相比<strong>随机梯度下降算法</strong>的一个缺点，是有额外的参数$b$。因此你需要一些时间来调试小批量$b$的大小。但是如果你有一个好的向量化实现，这种方式会比随机梯度下降更快一些。</p>
<h2 id="随机梯度下降的收敛"><a href="#随机梯度下降的收敛" class="headerlink" title="随机梯度下降的收敛"></a>随机梯度下降的收敛</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/fKi0M/stochastic-gradient-descent-convergence">视频地址</a></p>
<blockquote>
<p>现在你已经知道了随机梯度下降算法，但是当你运行这个算法时，你如何确保调试过程已经完成并且能正常收敛呢？ 还有，同样重要的是你怎样调整随机梯度下降中学习速率$α$的值？在本节中，我们会谈到一些方法来处理这些问题，确保它能收敛以及选择合适的学习速率$α$。</p>
</blockquote>
<h3 id="随机梯度下降的收敛-1"><a href="#随机梯度下降的收敛-1" class="headerlink" title="随机梯度下降的收敛"></a>随机梯度下降的收敛</h3><p>在批量梯度下降的算法中，我们确定梯度下降已经收敛的一个标准方法是<strong>画出最优化的代价函数</strong>$J_{train}(\theta)$。</p>
<script type="math/tex; mode=display">
J\_{train}(\theta)=
\frac{1}{2m}
\sum\_{i=1}^m
(h\_{\theta}(x^{(i)})-y^{(i)})^2</script><p>我们要确保代价函数在每一次的迭代中都是下降的。当训练集比较小的时候，计算这个求和是比较方便的，因此我们不难完成。但当你的训练集非常大的时候，这种求和就不方便了，因此我们引入<strong>随机梯度下降</strong>，每次只考虑一个样本，不需要时不时的扫描一遍全部的训练集。</p>
<p>具体来说，对于<strong>随机梯度下降算法</strong>，为了检查算法是否收敛，我们可以进行下面的工作：</p>
<p>随机梯度下降过程中，在每一次梯度下降的迭代执行前，我们都去用当前的随机样本$(x^{(i)},y^{(i)})$来计算当前的关于$\theta$的cost函数：</p>
<script type="math/tex; mode=display">
cost(\theta,(x^{(i)},y^{(i)}))=
\frac{1}{2}
(h\_{\theta}(x^{(i)})-y^{(i)})^2</script><p>在每次迭代之后都去更新$\theta$，每个样本迭代一次。</p>
<p>最后为了检查随机梯度下降的收敛性，我们要做的是每1000次迭代，我们可以画出前一步中计算出的cost函数。</p>
<p>我们把这些cost函数画出来，并对算法处理的最后1000个样本的cost值求平均值。如果你这样做的话它会很有效地帮你估计出你的算法在最后1000个样本上的表现。所以，我们不需要时不时地计算$J_{train}$，那样的话需要所有的训练样本。</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>下面是几幅画出来的图的例子：</p>
<h5 id="学习率大小对收敛的影响"><a href="#学习率大小对收敛的影响" class="headerlink" title="学习率大小对收敛的影响"></a>学习率大小对收敛的影响</h5><p>假如你已经画出了最后1000组样本的cost函数的平均值，由于它们都只是1000组样本的平均值，因此它们看起来有一点嘈杂，cost的值不会在每一个迭代中都下降，你可能会得到一种这样的图像：</p>
<p><img src="/img/17_06_01/011.png" alt=""></p>
<p>由于每次都是在一小部分的样本（1000个）上去求平均值，因此看起来是有噪声的。如果你得到像这样的图，那么你应该判断这个算法的代价值是在下降的，并且后来趋于平缓，这基本说明你的算法已经收敛了。</p>
<p>如果你想试试更小的学习速率，那么你很有可能看到的是算法的学习变得更慢了，因此代价函数的下降也变慢了，但同时由于你使用了更小的学习速率，你很有可能会让算法收敛到一个好一点的解。下图中红色的曲线代表随机梯度下降使用一个更小的学习速率：</p>
<p><img src="/img/17_06_01/012.png" alt=""></p>
<p>出现这种情况是因为随机梯度下降不是直接收敛到全局最小值，而是在局部最小附近反复振荡，所以使用一个更小的学习速率，最终的振荡就会更小。有时候这一点小的区别可以忽略，但有时候一点小的区别你就会得到更好一点的参数。</p>
<h5 id="每次计算代价函数的最小样本数对收敛的影响"><a href="#每次计算代价函数的最小样本数对收敛的影响" class="headerlink" title="每次计算代价函数的最小样本数对收敛的影响"></a>每次计算代价函数的最小样本数对收敛的影响</h5><p>假如你还是运行随机梯度下降，对1000组样本取cost函数的平均值并且画出图像，那么可能得到的图形如下：</p>
<p><img src="/img/17_06_01/013.png" alt=""></p>
<p>如果你把这个数1000提高到5000组样本，那么可能你会得到一条更平滑的曲线：</p>
<p><img src="/img/17_06_01/014.png" alt=""></p>
<p>通过在5000个样本中求平均值，你会得到比刚才1000组样本更平滑的曲线。这是你增大平均的训练样本数的情形。当然增大它的缺点就是现在每5000个样本才能得到一个数据点，因此你所得到的关于学习算法表现的反馈，就显得有一些“延迟”。</p>
<h5 id="代价函数不再收敛的情况"><a href="#代价函数不再收敛的情况" class="headerlink" title="代价函数不再收敛的情况"></a>代价函数不再收敛的情况</h5><p>有时候你运行梯度下降可能也会得到这样的图像：</p>
<p><img src="/img/17_06_01/015.png" alt=""></p>
<p>如果出现这种情况，可能你的代价函数就没有在减小了。如果你对这种情况时，也用更大量的样本进行平均，你很可能会观察到红线所示的情况：</p>
<p><img src="/img/17_06_01/016.png" alt=""></p>
<p>能看得出，实际上代价函数是在下降的只不过蓝线用来平均的样本数量太小了，并且蓝线太嘈杂你看不出来代价函数的趋势确实是下降的。所以可能用5000组样本来平均比用1000组样本来平均更能看出趋势。</p>
<p>当然，即使是使用一个较大的样本数量，你还是可能会发现这条学习曲线还是比较平坦的。如果是这样的话，那可能就更肯定地说明，由于某种原因导致算法确实没怎么学习好。这时你就需要调整学习速率，或者改变特征变量，或者改变其他的什么。</p>
<h5 id="代价函数发散时"><a href="#代价函数发散时" class="headerlink" title="代价函数发散时"></a>代价函数发散时</h5><p>如果你画出曲线是这样一种上升的情况时：</p>
<p><img src="/img/17_06_01/017.png" alt=""></p>
<p>这是一个很明显的告诉你算法正在发散的信号。那么你要做的事，就是用一个更小一点的学习速率$α$。</p>
<h3 id="使用可变的学习率-α"><a href="#使用可变的学习率-α" class="headerlink" title="使用可变的学习率$α$"></a>使用可变的学习率$α$</h3><p>我们已经知道，当运行<strong>随机梯度下降</strong>时算法会从某个点开始，然后曲折地逼近最小值，但它不会真的收敛，而是一直在最小值附近徘徊:</p>
<p><img src="/img/17_06_01/018.gif" alt=""></p>
<p>因此你最终得到的参数，实际上只是接近全局最小值，而不是真正的全局最小值。</p>
<p>在大多数随机梯度下降法的典型应用中，学习速率$α$一般是<strong>保持不变</strong>的。因此你最终得到的结果一般来说是上图这个样子的。如果你想让随机梯度下降确实收敛到全局最小值，你可以随时间的变化减小学习速率$α$的值。</p>
<p>所以一种典型的设置$α$的值的方法是让</p>
<script type="math/tex; mode=display">
α=
\frac{常数1}{迭代次数 + 常数2}</script><p><strong>常数1</strong>和<strong>常数2</strong>是两个额外的参数，你需要选择一下才能得到较好的表现。但很多人不愿意用这个办法的原因是你最后会把时间花在确定<strong>常数1</strong>和<strong>常数2</strong>上，这让算法显得更繁琐。但如果你能调整得到比较好的参数的话，你会得到的图形是随着迭代的增加，在最小值附近震荡的范围越来越小，最终几乎靠近全局最小的地方：</p>
<p><img src="/img/17_06_01/019.gif" alt=""></p>
<p>但由于确定这两个常数需要更多的工作量，并且我们通常也对能够很接近全局最小值的参数已经很满意了，因此在随机梯度下降中我们很少采用逐渐减小$α$的值的方法。你看到更多的还是让$α$的值为常数。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/05/28/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(4)%E9%A2%84%E6%B5%8B%E7%94%B5%E5%BD%B1%E8%AF%84%E5%88%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/05/28/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(4)%E9%A2%84%E6%B5%8B%E7%94%B5%E5%BD%B1%E8%AF%84%E5%88%86/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第九周 (4)预测电影评分</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-05-28 08:44:00" itemprop="dateCreated datePublished" datetime="2017-05-28T08:44:00+00:00">2017-05-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="预测电影评分"><a href="#预测电影评分" class="headerlink" title="预测电影评分"></a>预测电影评分</h1><p>Simple inline $a = b + c$.</p>
<h2 id="问题制定"><a href="#问题制定" class="headerlink" title="问题制定"></a>问题制定</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Rhg6r/problem-formulation">视频地址</a></p>
<p>在接下来的内容中，我想介绍给你们有关<strong>推荐系统(recommender systems)</strong>的内容。</p>
<p>我想讨论推荐系统有两个原因：</p>
<ul>
<li><p><strong>原因一：对工业界有至关重要的作用</strong></p>
<p>  在过去的几年中，我有时会去参观硅谷的各种科技类公司。我经常在那些公司里与开发机器学习应用的人交流。然后我问他们什么才是机器学习最重要的应用？或者什么样的机器学习的应用是你最想让它的表现得到改进的？我最常听到的回答其中之一就是 现在硅谷有好多个团队正试图建立更好的推荐系统。</p>
<p>  亚马逊、Netflix、eBay又或者苹果公司的iTunes Genius等，很多网站或者很多系统，试图向用户推荐新产品。比如说亚马逊向你推荐新书，Netflix 向你推荐新电影…。而这些推荐系统可能会看看你以前购买过什么书，或者以前你给哪些电影进行过评分。</p>
<p>  这些系统贡献了现今亚马逊收入的相当大一部分。而对像Netflix这样的公司，他们向用户推荐的电影占了用户观看的电影的相当大一部分。于是一个推荐系统其表现的一些改进，就能带来显著且即刻产生的影响。这种影响关系到许多公司的最终业绩。</p>
<p>  推荐系统在机器学习学术界是个很有意思的问题。如果我们去参加一个学术类的机器学习会议，<br>推荐系统的问题几乎得不到什么关注至少它是学术界当前动向里较小的一部分。但如果你看看正在发生的事对很多科技类公司而言，建立这些推荐系统似乎是优先要办的事。这就是我想在这门课中谈论推荐系统的原因之一。</p>
</li>
<li><p><strong>原因二：自动学习到优良特征</strong></p>
<p>  对于机器学习来说，特征量是重要的。你选择的特征对你学习算法的表现有很大影响。</p>
<p>  在机器学习领域有这么一个宏大的想法，就是对于一些问题而言（可能不是所有问题），存在一些算法能<strong>试图自动地替你学习到一组优良的特征量</strong>。这样与其手动设计或者手动编写特征，你或许能够采用一种算法来自动学习到使用什么特征量。而推荐系统就是这种情形的一个例子。</p>
</li>
</ul>
<h3 id="推荐系统-问题表述"><a href="#推荐系统-问题表述" class="headerlink" title="推荐系统 问题表述"></a>推荐系统 问题表述</h3><p>以预测电影评分这个时兴的问题为例，假想你是一个销售或出租电影的网站，你让用户使用1至5颗星 给不同的电影评分：</p>
<p><img src="/img/17_05_28/001.png" alt=""></p>
<p>假设下面的表格是几个用户针对五部电影给出的评分。其中”?”代表用户没有给出评分：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
</div>
<p>引入以下几个变量：</p>
<script type="math/tex; mode=display">
n\_u=用户数量 \\\\
n\_m=电影数量 \\\\
r(i,j)=如果用户j对电影i投过票，则记为1 \\\\
y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）</script><p>推荐系统问题就是：</p>
<p>在给定上面的这些数据（即$r(i,j)$和$y^{(i,j)}$）时，然后视图去预测上面表格中那些”?”的值。这样我们就可以向用户推荐他们可能还没看过的新电影了。</p>
<h2 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/uG59z/content-based-recommendations">视频地址</a></p>
<blockquote>
<p>上一节，我们引出了<strong>推荐系统</strong>的问题，这一节，我将介绍一种<strong>基于内容的推荐</strong>方法。</p>
</blockquote>
<p>这是上一节中的数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">$x_1$(爱情片)</th>
<th style="text-align:center">$x_2$(动作片)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0.99</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
</tr>
</tbody>
</table>
</div>
<p>重申一下之前的定义：</p>
<script type="math/tex; mode=display">
n\_u=用户数量 \\\\
n\_m=电影数量 \\\\
r(i,j)=如果用户j对电影i投过票，则记为1 \\\\
y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）</script><p>在这里的数据中：</p>
<script type="math/tex; mode=display">
n\_u=4 \\\\
n\_m=5</script><p>并且我们又追加了两列特征$x_1$和$x_2$，分别表示当前电影属于<strong>爱情片</strong>和<strong>动作片</strong>的程度。我们用$n=2$来代表特征数（这里不包括$x_0$）。</p>
<p>有了每部电影的类型特征数据，我们就可以用一个特征矩阵表示某一部电影了。假设上面五部电影，从上到下我们依次使用数字$1、2、3、4、5$来代替。那么对于第一部电影《爱到最后》的特征向量表示为：</p>
<script type="math/tex; mode=display">
\begin{equation}
x^{(1)}=\left[
\begin{matrix}
1\\\\
0.9\\\\
0
\end{matrix}
\right]
\end{equation}</script><blockquote>
<p>这三个数分别代表三个特征的值:$x_0$、$x_1$和$x_2$。其中$x_0$是截距特征变量，值为1。</p>
</blockquote>
<h3 id="使用一个已经训练好的模型进行预测"><a href="#使用一个已经训练好的模型进行预测" class="headerlink" title="使用一个已经训练好的模型进行预测"></a>使用一个已经训练好的模型进行预测</h3><p>为了进行预测，我们可以把对每个观众打分的预测当成一个独立的线性回归问题。</p>
<p>具体来说，比如每一个用户$j$，都学习出一个参数$\theta^{(j)}$（一般情况下，$\theta^{(j)}$的维度都是$n+1$，$n$是特征数，不包括截距项$x_0$）。</p>
<p>然后我们要根据参数向量$\theta^{(j)}$与特征$x^{(i)}$的内积来预测用户对电影$i$的评分：</p>
<script type="math/tex; mode=display">
(\theta^{(j)})^Tx^{(i)}</script><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>我们以用户1 Alice为例，Alice对应的特征向量应该是$\theta^{(1)}$，我们要预测Alice对于电影《小爱犬》的评分，这部电影的特征向量为：</p>
<script type="math/tex; mode=display">
\begin{equation}
x^{(3)}=\left[
\begin{matrix}
1\\\\
0.99\\\\
0
\end{matrix}
\right]
\end{equation}</script><p>假如你已经得到了Alice的特征参数向量$\theta^{(1)}$的值（后面我们会具体讲如何得到这个值）：</p>
<script type="math/tex; mode=display">
\begin{equation}
\theta^{(1)}=\left[
\begin{matrix}
0\\\\
5\\\\
0
\end{matrix}
\right]
\end{equation}</script><p>那么对于Alice关于《小爱犬》的平分预测如下：</p>
<script type="math/tex; mode=display">
(\theta^{(1)})^Tx^{(3)}=5×0.99=4.95</script><p>这里我们实际在做的事情就是：<strong>对每个用户应用不同的线性回归模型</strong>。</p>
<h3 id="更正式的描述"><a href="#更正式的描述" class="headerlink" title="更正式的描述"></a>更正式的描述</h3><p>关于这个计算过程，更正式的描述如下：</p>
<p>已知：</p>
<script type="math/tex; mode=display">
r(i,j)=如果用户j对电影i投过票，则记为1 \\\\
y^{(i,j)}=用户j对电影i投票的分值（只有在r(i,j)=1时，才有这个值）\\\\
\theta^{(j)} = 用户j的参数向量 \\\\
x^{(i)} = 电影i的特征向量</script><p>对于用户$j$关于电影$i$的预测评分为：</p>
<script type="math/tex; mode=display">
(\theta^{(j)})^Tx^{(i)}</script><h3 id="训练预测模型：计算参数向量-theta"><a href="#训练预测模型：计算参数向量-theta" class="headerlink" title="训练预测模型：计算参数向量$\theta$"></a>训练预测模型：计算参数向量$\theta$</h3><p>通过以下运算，可以学习到参数$\theta^{(j)}$：</p>
<p><img src="/img/17_05_28/002.gif" alt=""></p>
<blockquote>
<p><strong>公式解读：</strong></p>
<p>其中$\sum_{i:r(i,j)=1}$代表对所有满足$r(i,j)=1$的元素进行求和。</p>
<p>$(\theta^{(j)})^T(x^{(i)})-y^{(i,j)}$代表预测用户$j$对电影$i$的平分减去用户对这部电影的实际评分。</p>
<p>$\frac{\lambda}{2m^{(j)}}\sum_{k=1}^n((\theta_k^{(j)}))^2$是正则化项。</p>
</blockquote>
<p>对上面的式子通过求最小化，我们最终可以得到一个表现良好的$\theta^{(j)}$。</p>
<p>为了让公式变得更简单一些，我们可以去除掉常数项$\frac{1}{m^{(j)}}$，对最终结果无影响：</p>
<p><img src="/img/17_05_28/003.gif" alt=""></p>
<h4 id="完整的描述"><a href="#完整的描述" class="headerlink" title="完整的描述"></a>完整的描述</h4><p>接下来对训练过程进行完整的描述一遍：</p>
<h5 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h5><p>为了学习第$j$个用户的参数$\theta^{(j)}$，我们执行以下运算：</p>
<p><img src="/img/17_05_28/003.gif" alt=""></p>
<p>为了学习全部用户的参数$\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$，我们执行以下运算：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>通过这一运算，我们就能得出所有用户的参数向量$\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$，从而对所有用户作出预测了。</p>
<h5 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h5><p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>是我们最优化目标，记为$J(\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)})$。</p>
<p>为了最小化这个目标函数$J(\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)})$，我们需要使用梯度下降算法来计算：</p>
<ul>
<li><strong>k=0时</strong></li>
</ul>
<p><img src="/img/17_05_28/005.gif" alt=""></p>
<ul>
<li><strong>k≠0时</strong></li>
</ul>
<p><img src="/img/17_05_28/006.gif" alt=""></p>
<blockquote>
<p><strong>注意：</strong>其中$\alpha$是学习率。</p>
</blockquote>
<p>上面的过程实际上是在对优化目标函数$J(\theta^{(1)},\theta^{(2)},…,\theta^{(n_u)})$求偏微分的过程：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta\_k^{j}}
J(\theta^{(1)},\theta^{(2)},...,\theta^{(n\_u)})</script><p>以上就是通过梯度下降来最小化代价函数$J$的全过程，当然，如果你愿意的话，你可以尝试使用更高级的优化算比如<strong>聚类下降</strong>或者<em>L-BFGS(Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm)*</em>或者别的方法来最小化代价函数J。</p>
<blockquote>
<p>以上内容，我们介绍了使用一种线性回归的变体来预测不同用户对不同电影评分的算法，这种算法被称作<strong>“基于内容的推荐”</strong>，因为我们已知了不同电影的特征（比如电影是爱情片的程度，是动作片的程度等），从而来进行预测。但事实上，对于很多电影，我们并没有这些特征，或者很难得到这些特征。所以，在下一节中，我们将介绍一种<strong>“不基于内容的推荐系统”</strong>。</p>
</blockquote>
<h1 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h1><h2 id="协同过滤-1"><a href="#协同过滤-1" class="headerlink" title="协同过滤"></a>协同过滤</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/2WoBV/collaborative-filtering">视频地址</a></p>
<blockquote>
<p>这一节，我们将介绍一种叫做<strong>协同过滤(collaborative filtering)</strong>的推荐算法。</p>
<p>关于这个算法值得一提的一个特点，那就是它能实现<strong>对特征的学习</strong>。就是说这个算法可以实现自动学习要使用的特征值。</p>
</blockquote>
<p>在之前的电影评分的例子中，我们有下面的这组数据：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">$x_1$(爱情片)</th>
<th style="text-align:center">$x_2$(动作片)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">0.01</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0.99</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.9</td>
</tr>
</tbody>
</table>
</div>
<p>我们除了有每个用户对不同电影的评分之外，同时也有每个电影属于不同类别特征的程度值$x_1$和$x_2$。</p>
<p>但是，在实际情况中，这样做的难度很大，因为我们很难对所有的电影给出相关特征的评分。而且通常情况，我们还希望能得到除了这两个特征之外的其他特征。</p>
<p>为了能做到这一点，我们换一种形式，假设我们并不知道每部电影具体的特征值是多少：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">$x_1$(爱情片)</th>
<th style="text-align:center">$x_2$(动作片)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
</div>
<p>假设我们采访了每一位用户，而且每一位用户都告诉我们他们是否喜欢爱情电影；以及他们是否喜欢动作电影，这样Alice、Bob、Carol以及Dave就有了他们对应的参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\begin{equation}\theta^{(1)}=\left[\begin{matrix}0\\5\\0\end{matrix}\right]\end{equation}$</td>
<td style="text-align:center">$\begin{equation}\theta^{(2)}=\left[\begin{matrix}0\\5\\0\end{matrix}\right]\end{equation}$</td>
<td style="text-align:center">$\begin{equation}\theta^{(3)}=\left[\begin{matrix}0\\0\\5\end{matrix}\right]\end{equation}$</td>
<td style="text-align:center">$\begin{equation}\theta^{(4)}=\left[\begin{matrix}0\\0\\5\end{matrix}\right]\end{equation}$</td>
</tr>
</tbody>
</table>
</div>
<p>每个特征向量的第二个元素表示对爱情片的喜欢程度，第三个元素表示对动作片的喜欢程度。可以看出来Alice和Bob更喜欢爱情片，而Carol和Dave更喜欢动作片。</p>
<p>有了这些数据，我们就可以着眼于用户，看看任意用户$j$对应的不同题材电影的喜欢程度$\theta^{(j)}$。有了这些参数值，理论上我们就能推测出每部电影的特征变量$x_1$和$x_2$的值。</p>
<p>举个例子：</p>
<p>我们看第一个电影，假设我们不知道这部电影的主要内容，我们只知道Alice和Bob喜欢这部电影，而Carol和Dave不喜欢它。从他们四个人的特征向量就可以看出，Alice和Bob给出了5分，而Carol和Dave给出了0分。因此我们可以推断，这部电影可能是一部爱情片，而不太可也能是动作片。所以可能$x_1=1.0$而$x_2=0.0$。</p>
<p>其实这说明了我们在寻找能使得以下式子成立的$x^{(1)}$：</p>
<script type="math/tex; mode=display">
(\theta^{(1)})^Tx^{(1)}≈5 \\\\
(\theta^{(2)})^Tx^{(1)}≈5 \\\\
(\theta^{(3)})^Tx^{(1)}≈0 \\\\
(\theta^{(4)})^Tx^{(1)}≈0</script><p>因此，我们可以得出$x^{(1)}$的值为：</p>
<script type="math/tex; mode=display">
\begin{equation}
x^{(1)}
=\left[
\begin{matrix}
1\\\\
1.0\\\\
0.0
\end{matrix}
\right]
\end{equation}</script><blockquote>
<p>第一个元素$x^{(1)}_0$是截距。</p>
</blockquote>
<p>我们可以按照这种方式把其他电影的特征预测出来。</p>
<h3 id="协同过滤算法的正式描述"><a href="#协同过滤算法的正式描述" class="headerlink" title="协同过滤算法的正式描述"></a>协同过滤算法的正式描述</h3><p><strong>优化算法：</strong></p>
<p>在已知给定参数$\theta^{(1)},…,\theta^{(n_u)}$的情况下，对下面函数最小化，得到特征值$x^{(i)}$：</p>
<p><img src="/img/17_05_28/007.gif" alt=""></p>
<p>这就是我们如何从一部特定的电影中学习到特征的方法。</p>
<p>那么对于所有的电影，计算所有的特征，我们通过最小化下面的函数可以得到：</p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>这里我们对所有$n_m$个电影的特征代价函数求和，然后最小化这个整体的代价函数，这样就能得到针对所有电影的合理的特征值了。</p>
<h3 id="鸡生蛋？蛋生鸡？"><a href="#鸡生蛋？蛋生鸡？" class="headerlink" title="鸡生蛋？蛋生鸡？"></a>鸡生蛋？蛋生鸡？</h3><p>在上一节中，我们提到了如果给定了一系列的特征值$x^{(1)},…,x^{(n_m)}$，以及用户对电影的评分，我们就可以得到不同用户对应的参数向量$\theta^{(1)},…,\theta^{(n_u)}$。</p>
<p>在本节内容中，我们又讲到了在给定参数$\theta^{(1)},…,\theta^{(n_u)}$的情况下，我们可以预测每个电影的特征向量$x^{(1)},…,x^{(n_m)}$。</p>
<p>这有点类似<strong>先有鸡还是先有蛋</strong>的问题。我们知道了$x$就能预测出$\theta$，反之，如果我们知道了$\theta$，我们就能预测出$x$。</p>
<p>这样一来，我们能做到的就是首先随机初始化参数向量$\theta$，然后根据这些初始化得到的$\theta$来得到不同电影的特征值$x$:</p>
<script type="math/tex; mode=display">
Guess \ \ \ \ \theta → x</script><p>有了这些特征值$x$，我们就可以得到对参数$\theta$更好的估计：</p>
<script type="math/tex; mode=display">
x → \theta</script><p>同样，我们可以根据这个更好的$\theta$来得到更好的特征集$x$：</p>
<script type="math/tex; mode=display">
\theta → x</script><p>如此循环往复。</p>
<p>如果你一直重复上述的计算过程，你的算法将会收敛到一组合理的特征值$x$，以及一组合理的对不同用户参数的估计值$\theta$。</p>
<p>这就是基本的<strong>协同过滤算法</strong>，但这实际上不是我们最终使用的算法。在下一节中，我们将改进这个算法，让其在计算时更为高效。但这节课希望能让你基本了解这个算法的基本原理。</p>
<blockquote>
<p>总结一下，在本节，我们了解了最基本的协同过滤算法。</p>
<p>协同过滤算法指的是当你针对一大批用户数据执行这个算法时，这些用户实际上在高效地进行了协同合作来得到每个人对电影的评分值。只要用户对某几部电影进行评分，每个用户就都在帮助算法更好的学习出特征。这样以来，通过自己对几部电影评分之后，我就能帮助系统更好的学习到特征。这些特征可以被系统运用，为其他人做出更准确的电影预测。</p>
<p>协同的另一层意思是说每位用户都在为了大家的利益，而学习出更好的特征。这就是协同过滤。</p>
</blockquote>
<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/f26nH/collaborative-filtering-algorithm">视频地址</a></p>
<blockquote>
<p>在前几节中，我们谈到几个概念。</p>
<p>首先，如果给你几个特征表示电影，我们可以使用这些资料去获得用户的参数数据。</p>
<p>第二，如果给你用户的参数数据，你可以使用这些资料去获得电影的特征。</p>
<p>本节中，我们将会使用这些概念，并且将它们合并成<strong>协同过滤算法 (Collaborative Filtering Algorithm)</strong>。</p>
</blockquote>
<p>在之前的内容中，我们介绍了通过最小化以下目标函数，来在已知电影特征向量$x^{(1)},…,x^{(n_m)}$的情况下，预测每个用户对应的参数向量$\theta^{(1)},…,\theta^{(n_u)}$：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>也介绍了通过最小化以下目标函数，来在已知每个用户参数向量$\theta^{(1)},…,\theta^{(n_u)}$的情况下，来预测每个电影的特征向量$x^{(1)},…,x^{(n_m)}$：</p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>我们首先通过随机初始化一组参数$\theta$，然后来推导特征向量$x$；有了特征向量$x$，我们再去反推更好的参数向量$\theta$，然后如此循环往复，直到收敛。</p>
<p>而实际上，我们有一个更有效率的算法，让我们不必再这样不停地来回计算，而是能同时把$x$和$\theta$同时计算出来。</p>
<p>我们做的就是把上面两个式子合而为一：</p>
<p>同时最小化$x^{(1)},…,x^{(n_m)}$和$\theta^{(1)},…,\theta^{(n_u)}$：</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>优化目标函数：</p>
<p><img src="/img/17_05_28/010.gif" alt=""></p>
<h3 id="公式解读"><a href="#公式解读" class="headerlink" title="公式解读"></a>公式解读</h3><p>接下来，我们对上面这个式子的推导过程进行详细的解读。</p>
<h4 id="求和部分"><a href="#求和部分" class="headerlink" title="求和部分"></a>求和部分</h4><p>首先需要说明的是，在之前的这两个式子中：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>仔细观察这两部分，其实是一样的：</p>
<p><img src="/img/17_05_28/011.gif" alt=""></p>
<p><img src="/img/17_05_28/012.gif" alt=""></p>
<p>第一个是：所有的用户的评过分的电影中，预测评分和真实评分的差值平方总和。</p>
<p>第二个是：所有的电影中对它评过分的用户，预测的评分和真实评分的差值平方总和。</p>
<p>这两者的计算结果其实是一样的，只不过条件先后不同导致计算顺序不同而已。</p>
<p>因此，在最终的代价函数中：</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>这一部分：</p>
<p><img src="/img/17_05_28/013.gif" alt=""></p>
<p>其实这个表达式就是上面那两种表达式的整体的表示形式。计算结果与上面那两个式子也是相同的。</p>
<h4 id="正则化部分"><a href="#正则化部分" class="headerlink" title="正则化部分"></a>正则化部分</h4><p>最终的代价函数</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>的后半段：</p>
<p><img src="/img/17_05_28/014.gif" alt=""></p>
<p>其实就是把这两个式子中的正则化部分求和得出的：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<p>如果你把</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>中$x^{(1)},…,x^{(n_m)}$部分看做是常数，然后关于$\theta$做优化，那么这个式子其实就相当于：</p>
<p><img src="/img/17_05_28/004.gif" alt=""></p>
<p>反之，如果你把$\theta^{(1)},…,\theta^{(n_u)}$部分看做常数，然后关于$x$做优化，那么这个式子其实就相当于是：</p>
<p><img src="/img/17_05_28/008.gif" alt=""></p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>最终，我们通过合并之前的两个式子，得到了同时关于$x$和$\theta$的代价函数表达式：</p>
<p><img src="/img/17_05_28/009.gif" alt=""><br><img src="/img/17_05_28/010.gif" alt=""></p>
<p>这和之前的算法之间唯一的不同就是<strong>不需要反复计算</strong>（就是前面提到的先关于$\theta$最小化，然后再关于$x$最小化，然后再次关于$\theta$最小化，反复直到收敛）。在新版本里头 不需要不断地在$x$和$θ$这两个参数之间不停折腾，我们所要做的是将这两组参数同时化简。</p>
<h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h4><p>最后值得提醒的一件事，当我们以这样的方法学习特征量时，我们必须要保证去掉$x_0$项，这样来保证我们学习到的特征量$x$是$n$维的，而不是$n+1$维的。</p>
<p>同样地，因为参数$θ$与特征向量$x$是在同一个维度上，所以$θ$也是$n$维的。因为如果没有$x_0$，那么$θ_0$也不再需要。</p>
<h3 id="协同过滤算法的正式描述-1"><a href="#协同过滤算法的正式描述-1" class="headerlink" title="协同过滤算法的正式描述"></a>协同过滤算法的正式描述</h3><p>把上面所有的过程总结下来，就是所谓的<strong>协同过滤算法</strong>。下面是对这一算法的具体步骤描述：</p>
<ul>
<li>1.首先用较小的初始值来随机初始化参数$x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)}$。<blockquote>
<p>这一步有点像训练神经网络，在神经网络中的训练中，我们也是用小的随机数来初始化权值。</p>
</blockquote>
</li>
<li>2.通过使用梯度下降算法（或者其他更高级的优化算法）来最小化代价函数$J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})$。<ul>
<li>例如循环每一个$j=1,…,n_u,i=1,…,n_m$ 来执行对应的梯度下降：</li>
</ul>
</li>
</ul>
<p><img src="/img/17_05_28/015.png" alt=""></p>
<p>其中这一部分：</p>
<p><img src="/img/17_05_28/016.png" alt=""></p>
<p>就是对代价函数的偏微分项：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial x_k^{(i)}}
J(x^{(1)},...,x^{(n_m)})</script><p>这一部分：</p>
<p><img src="/img/17_05_28/017.png" alt=""></p>
<p>对应也是下面这个代价函数的偏微分项：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_k^{(j)}}
J(\theta^{(1)},...,\theta^{(n_u)})</script><blockquote>
<p>值得提醒的是，在这个公式中，我们不再用到$x_0=1$这一项，因此$x$是$n$维的（$x\in R^n$）而不是$n+1$维；$\theta$也是$n$维（$\theta\in R^n$）。</p>
<p>所以我们在做正则化的时候，也是对这$n$维的参数进行正则化，并不包括$x_0$和$\theta_0$。</p>
</blockquote>
<ul>
<li>3.最终，通过用户对应的参数向量$\theta$和训练得出的特征向量$x$，来预测用户给出的评分$\theta^Tx$。</li>
</ul>
<h1 id="低秩矩阵分解"><a href="#低秩矩阵分解" class="headerlink" title="低秩矩阵分解"></a>低秩矩阵分解</h1><h2 id="矢量化：低秩矩阵分解"><a href="#矢量化：低秩矩阵分解" class="headerlink" title="矢量化：低秩矩阵分解"></a>矢量化：低秩矩阵分解</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/CEXN0/vectorization-low-rank-matrix-factorization">视频地址</a></p>
<blockquote>
<p>在上几节中，我们谈到了<strong>协同过滤算法</strong>，本节视频中我将会讲到有关该算法的向量化实现，以及说说有关该算法你可以做的其他事情。</p>
<p>比如说，你可以做到<strong>相似产品推荐</strong>的功能。</p>
</blockquote>
<p>还是以之前的电影评分为例：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
</tr>
</tbody>
</table>
</div>
<p>我们将这些数据存储到矩阵$Y$中：</p>
<script type="math/tex; mode=display">
\begin{equation}
Y=\left[
\begin{matrix}
5&5&0&0\\\\
5&?&?&0\\\\
?&4&0&?\\\\
0&0&5&4\\\\
0&0&5&0
\end{matrix}
\right]
\end{equation}</script><p>$Y^{(i,j)}$代表第i行第j列的数据，即第i部电影的第j个用户的评分。</p>
<p>同时，我们也可以得出下面这个预测评分矩阵：</p>
<script type="math/tex; mode=display">
\begin{equation}
\left[
\begin{matrix}
(\theta^{(1)})^T(x^{(1)})&(\theta^{(2)})^T(x^{(1)})&...&(\theta^{(n_u)})^T(x^{(1)})\\\\
(\theta^{(1)})^T(x^{(2)})&(\theta^{(2)})^T(x^{(2)})&...&(\theta^{(n_u)})^T(x^{(2)})\\\\
┋&┋&┋&┋\\\\
(\theta^{(1)})^T(x^{(n_m)})&(\theta^{(2)})^T(x^{(n_m)})&...&(\theta^{(n_u)})^T(x^{(n_m)})
\end{matrix}
\right]
\end{equation}</script><p>其中$(\theta^{(j)})^T(x^{(i)})$代表第i个电影中第j个用户，预计给出的评分。</p>
<p>这个预测矩阵可以看做是<strong>电影的特征矩阵</strong>和<strong>用户的参数矩阵的转置</strong>的乘积。</p>
<p>电影特征矩阵如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
X=\left[
\begin{matrix}
(x^{(1)})^T\\\\
(x^{(2)})^T\\\\
┋\\\\
(x^{(n_m)})^T
\end{matrix}
\right]
\end{equation}</script><p>其中每一个元素都是一部电影的特征向量。</p>
<p>用户的参数矩阵如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
\Theta=\left[
\begin{matrix}
(\theta^{(1)})^T\\\\
(\theta^{(2)})^T\\\\
┋\\\\
(\theta^{(n_u)})^T
\end{matrix}
\right]
\end{equation}</script><p>其中每个元素都是一个用户的参数向量。</p>
<p>有了所有电影的特征矩阵$X$和所有用户的参数矩阵$\Theta$后，我们就可以得到上面的预测矩阵了：</p>
<script type="math/tex; mode=display">
\begin{equation}
X\Theta^T=\left[
\begin{matrix}
(\theta^{(1)})^T(x^{(1)})&(\theta^{(2)})^T(x^{(1)})&...&(\theta^{(n_u)})^T(x^{(1)})\\\\
(\theta^{(1)})^T(x^{(2)})&(\theta^{(2)})^T(x^{(2)})&...&(\theta^{(n_u)})^T(x^{(2)})\\\\
┋&┋&┋&┋\\\\
(\theta^{(1)})^T(x^{(n_m)})&(\theta^{(2)})^T(x^{(n_m)})&...&(\theta^{(n_u)})^T(x^{(n_m)})
\end{matrix}
\right]
\end{equation}</script><blockquote>
<p>我们的协同过滤算法还有另外一个名字：<strong>低秩矩阵分解（Low Rank Matrix Factorization）</strong>。</p>
</blockquote>
<h3 id="其他功能"><a href="#其他功能" class="headerlink" title="其他功能"></a>其他功能</h3><p>最后，我们还可以通过使用协同过滤算法产生的结果做一些额外的事情。比如说<strong>找到相关的电影</strong>。</p>
<p>如何做到针对用户喜欢的某一部电影，然后推荐给他另一部类似的电影呢？</p>
<p>对于每一部电影，我们都可以学习到他的特征向量$x^{(i)} \in R^n$。</p>
<p>假设我们想找到和电影$i$最类似的一部电影，有一种方式就是求出所有电影的特征向量，然后和电影$i$的特征向量求欧氏距离，距离最小的那个就是最类似的：</p>
<script type="math/tex; mode=display">
Small ||x^{(i)}-x^{(j)}||  → 电影i和电影j最类似</script><p>类似的，如果你想找到与电影$i$最类似的5部电影，你只需求与特征向量$x^{(i)}$欧式距离最小的五部电影即可。</p>
<h2 id="实现细节：均值归一化"><a href="#实现细节：均值归一化" class="headerlink" title="实现细节：均值归一化"></a>实现细节：均值归一化</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Adk8G/implementational-detail-mean-normalization">视频地址</a></p>
<blockquote>
<p>到目前为止，你已经了解到了推荐系统算法或者 协同过滤算法的所有要点。</p>
<p>在本节中，我想分享最后一点实现过程中的细节，这一点就是<strong>均值归一化</strong>。有时它可以让算法运行得更好。</p>
</blockquote>
<h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>为了了解均值归一化这个想法的动机，我们考虑这样一个例子。</p>
<p>有一个用户Eve没有给任何电影评分：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">电影</th>
<th style="text-align:center">Alice(1)</th>
<th style="text-align:center">Bob(2)</th>
<th style="text-align:center">Carol(3)</th>
<th style="text-align:center">Dave(4)</th>
<th style="text-align:center">Eve(5)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">《爱到最后》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">5</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《浪漫永远》</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center">?</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《小爱犬》</td>
<td style="text-align:center">?</td>
<td style="text-align:center">4</td>
<td style="text-align:center">0</td>
<td style="text-align:center">?</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《无尽狂飙》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">4</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
<tr>
<td style="text-align:center">《剑与空手道》</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">5</td>
<td style="text-align:center">?</td>
<td style="text-align:center"><strong>?</strong></td>
</tr>
</tbody>
</table>
</div>
<script type="math/tex; mode=display">
\begin{equation}
Y=\left[
\begin{matrix}
5&5&0&0&?\\\\
5&?&?&0&?\\\\
?&4&0&?&?\\\\
0&0&5&4&?\\\\
0&0&5&0&?
\end{matrix}
\right]
\end{equation}</script><p>我们来看看协同过滤算法会对这个用户做什么。</p>
<p>假设我们的电影只有两个特征$n=2$，用户的参数向量也是2维的：$\theta^{(5)} \in R^2$。</p>
<p><img src="/img/17_05_28/009.gif" alt=""></p>
<p>由于用户没有给任何电影评分，因此代价函数的前半部分没有任何电影满足$r(i,j)=1$的条件，所以可以忽略。所以，真正影响代价函数值变化的只有这一项：</p>
<script type="math/tex; mode=display">
\frac{\lambda}{2}
\sum\_{j=1}^{n\_u}
\sum\_{k=1}^{n}
(\theta\_k^{(j)})^2</script><p>对应到我们的具体的两个特征上之后，就是如下的形式：</p>
<script type="math/tex; mode=display">
\frac{\lambda}{2}
[
(\theta_1^{(5)})^2+(\theta_2^{(5)})^2
]</script><p>因此，为了最小化这一项，我们得到的结果就是：</p>
<script type="math/tex; mode=display">
\begin{equation}
\theta^{(5)}=\left[
\begin{matrix}
0\\\\
0
\end{matrix}
\right]
\end{equation}</script><p>所以，我们可以得出用户对电影评分的预测结果：</p>
<script type="math/tex; mode=display">
(\theta^{(5)})^Tx^{(1)}=0</script><p>都是0颗星。</p>
<p>这个结果看起来并没有什么用，因为其实有的电影是对于某些人来说是有较高的评分的，所以某些电影是值得被推荐的。但是我们得到的预测结果都是0颗星，这个结果在暗示我们不要推荐电影给他，这其实也不够好。</p>
<p>所以，我们需要引入<strong>均值归一化</strong>来解决我们这个问题。</p>
<h3 id="均值归一化处理数据"><a href="#均值归一化处理数据" class="headerlink" title="均值归一化处理数据"></a>均值归一化处理数据</h3><p>对于已知的电影评分数据：</p>
<script type="math/tex; mode=display">
\begin{equation}
Y=\left[
\begin{matrix}
5&5&0&0&?\\\\
5&?&?&0&?\\\\
?&4&0&?&?\\\\
0&0&5&4&?\\\\
0&0&5&0&?
\end{matrix}
\right]
\end{equation}</script><p>我们要做的就是计算每个电影所得评分的均值，用向量$μ$表示：</p>
<script type="math/tex; mode=display">
\begin{equation}
μ=\left[
\begin{matrix}
2.5\\\\
2.5\\\\
2\\\\
2.25\\\\
1.25
\end{matrix}
\right]
\end{equation}</script><p>然后将所有评分减去平均评分，得到新的$Y$：</p>
<script type="math/tex; mode=display">
\begin{equation}
Y=\left[
\begin{matrix}
2.5&2.5&-2.5&-2.5&?\\\\
2.5&?&?&-2.5&?\\\\
?&2&-2&?&?\\\\
-2.25&-2.25&2.75&1.75&?\\\\
-1.25&-1.25&3.75&-1.25&?
\end{matrix}
\right]
\end{equation}</script><p>这样做的目的，<strong>就是把每部电影的评分的平均数都调整为0</strong>。</p>
<p>然后我们对经过归一化处理之后的矩阵使用协同过滤算法，即使用户没有对电影做出过评分，那么我们得到的参数向量是：</p>
<script type="math/tex; mode=display">
\begin{equation}
\theta^{(5)}=\left[
\begin{matrix}
0\\\\
0
\end{matrix}
\right]
\end{equation}</script><p>我们最终预测用户给出的分值是：</p>
<script type="math/tex; mode=display">
(\theta^{(5)})^Tx^{(1)} + μ\_i</script><p>在这个例子中，我们预测Eva对五部电影初始化的评分分别为：2.5，2.5，2，2.25，1.25。</p>
<p>因为：</p>
<script type="math/tex; mode=display">
\begin{equation}
μ=\left[
\begin{matrix}
2.5\\\\
2.5\\\\
2\\\\
2.25\\\\
1.25
\end{matrix}
\right]
\end{equation}</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/05/26/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(3)%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%EF%BC%88%E9%80%89%E5%AD%A6%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/05/26/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(3)%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%EF%BC%88%E9%80%89%E5%AD%A6%EF%BC%89/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第九周 (3)多元高斯分布（选学）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-05-26 21:00:00" itemprop="dateCreated datePublished" datetime="2017-05-26T21:00:00+00:00">2017-05-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="多元高斯分布"><a href="#多元高斯分布" class="headerlink" title="多元高斯分布"></a>多元高斯分布</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Cf8DF/multivariate-gaussian-distribution">视频地址</a></p>
<blockquote>
<p>接下来我将介绍<strong>多元高斯分布 (multivariate Gaussian distribution)</strong>。它是我们目前所学的异常检测算法的延伸，它有一些优势也有一些劣势，它能捕捉到一些之前的算法检测不出来的异常。</p>
</blockquote>
<h3 id="异常检测算法无法解决的问题"><a href="#异常检测算法无法解决的问题" class="headerlink" title="异常检测算法无法解决的问题"></a>异常检测算法无法解决的问题</h3><p>还是现以一个例子来解释：</p>
<p>假设在数据中心监控机器的例子中，我们有如下的内存和CPU使用数据：</p>
<p><img src="/img/17_05_26/001.png" alt=""></p>
<p>其中对于这两个维度的数据，都服从正态分布：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/002.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/003.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>如果现在在我们的测试集中，有一个异常数据点出现在下图的位置中：</p>
<p><img src="/img/17_05_26/004.png" alt=""></p>
<p>那么在这种情况下我们会发现，这一点对应的两个维度下的概率其实都不低，从$p(x)$的结果上，我们无法准确预测这个样本是否属于异常。</p>
<p>产生这个问题的实际原因其实是这样的，从$x_1$和$x_2$这两个维度来看，我们的正常数据及时大多数集中分布在这样一个范围内：</p>
<p><img src="/img/17_05_26/005.png" alt=""></p>
<p>但我们使用之前的异常检测算法，其实是以中心区域向外以正圆的形式扩散的。也就是说距离中心区域距离相等的点，对应的$p(x)$都是一样的，所以我们可能无法检测到这一个异常样本，因为它也处在一个$p(x)$比较大的范围内：</p>
<p><img src="/img/17_05_26/006.png" alt=""></p>
<p>所以，为了解决异常检测算法的这一问题，接下来我解释改良版的异常检测算法，要用到叫做<strong>多元高斯分布（多元正态分布）</strong>的东西。</p>
<h3 id="通过多元高斯分布改良异常检测算法"><a href="#通过多元高斯分布改良异常检测算法" class="headerlink" title="通过多元高斯分布改良异常检测算法"></a>通过多元高斯分布改良异常检测算法</h3><p>在多元高斯分布中，对于n维特征$x \in R^n$，不要把模型$p(x_1)$,$p(x_2)$,…,$p(x_n)$分开，而要建立$p(x)$整体的模型。</p>
<p>多元高斯分布的参数包括向量$µ$和一个$n×n$的矩阵$Σ$。</p>
<script type="math/tex; mode=display">
µ \in R^n  \\\\
Σ \in R^{n×n}</script><p>$Σ$被称为<strong>协方差矩阵</strong>，它类似于我们之前学习PCA的时候所见到的协方差矩阵。</p>
<p>带入之后计算$p(x)$：</p>
<script type="math/tex; mode=display">
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))</script><blockquote>
<p>这个公式不用背，用的时候再去查。</p>
</blockquote>
<p>注意，公式中的$|Σ|$这一项，代表矩阵$Σ$的<strong>行列式</strong>。在Octave中可以用下面的代码来计算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">det(Sigma)</span><br></pre></td></tr></table></figure>
<p>其实这个公式是什么样并不重要，更重要的是$p(x)$到底是什么样。</p>
<h3 id="多元高斯分布的样子"><a href="#多元高斯分布的样子" class="headerlink" title="多元高斯分布的样子"></a>多元高斯分布的样子</h3><p>我们来对比一下不同的$µ$和不同的$Σ$组合后，对应的$p(x)$的形状。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}0.6&amp;0\\0&amp;0.6\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}2&amp;0\\0&amp;2\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/009.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/011.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/010.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/012.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>表格从上到下依次是三种情况对应的参数、三维图像以及俯视图。</p>
<p>$µ$作为均值，象征着中心区域对应的坐标点。$Σ$是协方差矩阵，它衡量的是特征$x_1$和$x_2$的方差。</p>
<p>从图中可以看出来，当缩小协方差$Σ$时，中心区域的凸起就会变得更细长；当扩大协方差$Σ$时，中心区域的凸起就会变得更扁平。因为概率分布的积分必须等于1，所以如果你缩小方差，就会变得细长，反之就会变得扁平。</p>
<p>接下来我们尝试对协方差中使用不同的数值，来观测$p(x)$形状的变化：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}0.6&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}2&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/013.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/015.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/014.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/016.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>在协方差$Σ$中，左上角元素对应的是$x_1$特征，右下角元素对应的是$x_2$特征。</p>
<p>在第二组图中，我们可以看到，当我们缩小$x_1$到原先的0.6倍而$x_2$保持原先的大小时，由于相当于是对特征$x_1$的方差进行了缩小，所以图像在$x_1$的方向上，会显得更细长。</p>
<p>在第三组图中，我们可以看到，当我们放大$x_1$到原先的2倍而$x_2$保持原先的大小时，由于相当于是对特征$x_1$的方差进行了放大，所以图像在$x_1$的方向上，会显得更扁平。</p>
<p>对于多元高斯分布来说，一个很棒的事情就是我们可以用它来<strong>对数据的相关性建模</strong>。也就是说，我们可以用它来给$x_1$和$x_2$高度相关的情况建立模型。具体来说，我们可以通过改变协方差$Σ$非对角线上的元素来得到不同的高斯分布：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0.5\\0.5&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0.8\\0.8&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/017.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/019.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/018.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/020.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>可以看出来，当我改变了非对角线上元素的值时，$p(x)$的图像也变得倾斜了；当我增大了这些元素时，这个倾斜的分布图像变得更细长了。</p>
<p>上面是我们把这些非对角线上元素设置为正数时的样子，那么如果我们把它们设置为负数时，会是什么样呢？</p>
<p>它们的倾斜方向会发生改变：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;-0.5\\-0.5&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;-0.8\\-0.8&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/021.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/023.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/022.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/024.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>如果我们改变$µ$，会对图像$p(x)$产生什么影响呢？</p>
<p>它们会发生平移：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}0\\0.5\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
<th style="text-align:center">$\begin{equation}µ=\left[\begin{matrix}1.5\\-0.5\end{matrix}\right]Σ=\left[\begin{matrix}1&amp;0\\0&amp;1\end{matrix}\right]\end{equation}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/025.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/027.png" alt=""></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_05_26/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/026.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_26/028.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>$µ$有两个元素，第一个元素对应的是图像在$x_1$方向上的位移，第二个元素对应的是图像在$x_2$方向上的位移。当为正数时，是沿着增大的方向平移，反之是沿着缩小的方向平移。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>不同的图片，能够帮助你了解<strong>多元高斯分布</strong>所能描述的概率分布是什么样。它最重要的优势就是能够描述当两个特征变量之间可能存在正相关或者是负相关关系的情况。</p>
<h2 id="通过多元高斯分布来处理异常检测问题"><a href="#通过多元高斯分布来处理异常检测问题" class="headerlink" title="通过多元高斯分布来处理异常检测问题"></a>通过多元高斯分布来处理异常检测问题</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/DnNr9/anomaly-detection-using-the-multivariate-gaussian-distribution">视频地址</a></p>
<blockquote>
<p>在上一节中，我们谈到了多元高斯分布，而且也看到了一些例子通过改变参数$µ$和$Σ$来给不同的概率分布建模。</p>
<p>在这节中，我们使用它们来开发另一种异常检测算法。</p>
</blockquote>
<h3 id="知识回顾"><a href="#知识回顾" class="headerlink" title="知识回顾"></a>知识回顾</h3><p>再回顾一下<strong>多元高斯分布（或者叫多元正态分布）</strong>：</p>
<p>有两个参数：$µ$和$Σ$。</p>
<p>$µ$是一个n维向量，协方差矩阵$Σ$是一个$n×n$的矩阵。</p>
<p>其对应的概率分布公式如下：</p>
<script type="math/tex; mode=display">
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))</script><p>随着你改变$µ$和$Σ$，你可以得到一系列不同的概率分布：</p>
<p><img src="/img/17_05_26/029.png" alt=""></p>
<h4 id="参数拟合"><a href="#参数拟合" class="headerlink" title="参数拟合"></a>参数拟合</h4><p>接下来让我们谈一下<strong>参数拟合问题（参数估计问题）</strong>。</p>
<p>如果我有一组符合高斯分布的样本：</p>
<script type="math/tex; mode=display">
｛x^{(1)},x^{(2)},...,x^{(m)}｝</script><p>我们可以通过公式来得到参数$µ$和$Σ$：</p>
<script type="math/tex; mode=display">
µ=\frac{1}{m}\sum^m\_{i=1}x^{(i)}</script><script type="math/tex; mode=display">
Σ=\frac{1}{m}\sum^m\_{i=1}(x^{(i)}-µ)(x^{(i)}-µ)^T</script><h4 id="具体应用步骤"><a href="#具体应用步骤" class="headerlink" title="具体应用步骤"></a>具体应用步骤</h4><p>有了这两个参数值，我们就可以把他们应用到具体的异常检测算法中了。具体步骤是这样的：</p>
<p>假设我们有如下的训练样本：</p>
<p><img src="/img/17_05_26/030.png" alt=""></p>
<ul>
<li>首先，用我们的训练集来拟合模型$p(x)$，得到参数$µ$和$Σ$：</li>
</ul>
<script type="math/tex; mode=display">
µ=\frac{1}{m}\sum^m\_{i=1}x^{(i)}</script><script type="math/tex; mode=display">
Σ=\frac{1}{m}\sum^m\_{i=1}(x^{(i)}-µ)(x^{(i)}-µ)^T</script><ul>
<li>然后，当你得到一个新的测试样本时，我们用下面的公式来计算其$p(x)$：</li>
</ul>
<script type="math/tex; mode=display">
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))</script><ul>
<li>最后，如果$p(x)&lt;ε$时，就把它标记为是一个异常样本，反之，如果$p(x)&gt;=ε$则不标记为异常样本。</li>
</ul>
<p>所以，如果使用多元高斯分布来解决异常检测问题，你可能会得到这样一个高斯分布的概率模型：</p>
<p><img src="/img/17_05_26/031.png" alt=""></p>
<p>所以，他可以正常的识别出之前用普通的异常检测算法无法正确检测的那个异常样本。</p>
<h3 id="多元高斯模型和原始模型的关系"><a href="#多元高斯模型和原始模型的关系" class="headerlink" title="多元高斯模型和原始模型的关系"></a>多元高斯模型和原始模型的关系</h3><p>最后说一下<strong>多元高斯分布模型</strong>和原来的模型它们之间的关系。</p>
<p>原先的模型是这样的：</p>
<script type="math/tex; mode=display">
p(x)=p(x\_1;µ\_1,σ\_1^2)×p(x\_2;µ\_2,σ\_2^2)×...×p(x\_n;µ\_n,σ\_n^2)</script><p>事实上，你可以证明我们原先的这种模型，是多元高斯模型的一种。它其实是一种等高线都沿着坐标轴方向的多元高斯分布，但这里我不给出证明过程：</p>
<p><img src="/img/17_05_26/032.png" alt=""></p>
<p>所以这三个图像，全都是你可以用原来的模型来拟合的高斯分布的例子。</p>
<p>其实这个模型对应于一种多元高斯分布的特例，具体来说这个特例被定义为约束$p(x)$的分布(也就是多元高斯分布$p(x)$)，使得它的概率密度函数的等高线是沿着轴向的。也就是要求<strong>协方差矩阵$Σ$的非对角线元素都为0</strong>。</p>
<p>所以你可以得到多元高斯分布$p(x)$看起来是上图中这三种样式的。你会发现，在这3个例子中，它们的轴都是沿着$x_1$，$x_2$的轴的。</p>
<p>因此，在<strong>协方差矩阵$Σ$的非对角线元素都为0</strong>的情况下，这两者是相同的：</p>
<script type="math/tex; mode=display">
p(x)=p(x\_1;µ\_1,σ\_1^2)×p(x\_2;µ\_2,σ\_2^2)×...×p(x\_n;µ\_n,σ\_n^2)</script><script type="math/tex; mode=display">
p(x;µ,Σ)=
\frac{1}{(2π)^{\frac{n}{2}}|Σ|^{\frac{1}{2}}}
exp(-\frac{1}{2}(x-µ)^TΣ^{-1}(x-µ))</script><h3 id="何时使用多元高斯模型？何时使用原始模型？"><a href="#何时使用多元高斯模型？何时使用原始模型？" class="headerlink" title="何时使用多元高斯模型？何时使用原始模型？"></a>何时使用多元高斯模型？何时使用原始模型？</h3><p>既然我们知道了原始的模型是多元高斯模型的一个特例，那么应该在什么时候用哪个模型呢？</p>
<p>事实情况是，原始模型比较常用，而多元高斯模型比较少用。</p>
<p>假设在你的样本中，$x_1$和$x_2$是线性相关的特征组合，下面是这两种算法在处理不正常的特征组合时的具体方式对比：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">原始模型</th>
<th style="text-align:center">多元高斯模型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">捕捉到这两个特征，建立一个新的特征$x_3$(比如$x_3=\frac{x_1}{x_2}$)，去尝试手工组合并改变这个新的特征变量，从而使得算法能很好的工作。</td>
<td style="text-align:center">自动捕捉不同特征变量之间的相关性。</td>
</tr>
<tr>
<td style="text-align:center">运算量小(更适用于特征变量个数$n$很大的情况)</td>
<td style="text-align:center">计算更复杂（Σ是$n×n$的矩阵，这里会涉及两个$n×n$的矩阵相乘的逻辑，计算量很大）</td>
</tr>
<tr>
<td style="text-align:center">即使训练样本数$m$很小的情况下，也能工作的很好</td>
<td style="text-align:center">必须满足$m&gt;n$，或者$Σ$不可逆（奇异矩阵）。这种情况下，还可以帮助你省去为了捕捉特征值组合而手动建立额外特征变量所花费的时间。</td>
</tr>
</tbody>
</table>
</div>
<h4 id="一种异常情况的应对"><a href="#一种异常情况的应对" class="headerlink" title="一种异常情况的应对"></a>一种异常情况的应对</h4><p>当你在拟合多元高斯模型时，如果你发现协方差矩阵$Σ$是<strong>奇异的（不可逆的）</strong>，一般只有两种情况：</p>
<ul>
<li>第一种是它没有满足$m&gt;n$的条件</li>
<li>第二种情况是，你有冗余特征变量 <ul>
<li>冗余特征变量的意思是出现了以下两种情况的任意一种：<ul>
<li>出现了两个完全一样的特征变量（你可能不小心把同一个特征变量复制了两份）</li>
<li>如果你有$x_3=x_4+x_5$，这里$x_3$其实并没有包含额外的信息，相对于$x_4$和$x_5$来说，它就是冗余特征变量。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>这是你调试算法时的一个小知识，可能很少会遇到，但是一旦你发现$Σ$不可逆，那么首先需要从这两个方面来考虑解决方案。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/05/25/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(2)%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/05/25/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(2)%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第九周 (2)构建一个异常检测系统</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-05-25 21:14:00" itemprop="dateCreated datePublished" datetime="2017-05-25T21:14:00+00:00">2017-05-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="异常检测系统的开发与评估"><a href="#异常检测系统的开发与评估" class="headerlink" title="异常检测系统的开发与评估"></a>异常检测系统的开发与评估</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Mwrni/developing-and-evaluating-an-anomaly-detection-system">视频地址</a></p>
<blockquote>
<p>在上一节中，我们推导了异常检测算法。在本节，我想介绍一下如何开发一个异常检测的应用来解决一个实际问题。</p>
</blockquote>
<p>在之前，我们已经提到了使用实数评估法的重要性。这样做的想法是，当你在用某个学习算法来开发一个具体的机器学习应用时，你常常需要做出很多决定，比如说选择用什么样的特征等等。而如果你找到某种评估算法的方式，比如直接返回一个数字，来告诉你算法的好坏，那么你做这些决定就显得更容易了。有了这样的数字，你就可以更好的确定某些特征是否需要在构建算法的时候考虑进来了，因为你可以通过对比算法在有这个特征和没这个特征的情况下，算法的具体表现，来决定是否要加入这个特征。</p>
<p>所以对于异常检测系统的评价方式很重要。</p>
<p>为了做到能评价一个异常检测系统，我们先假定已有了一些带标签的数据。</p>
<blockquote>
<p>我们要考虑的异常检测问题是一个非监督问题，使用的是无标签数据。但如果你有一些带标签的数据，能够指明哪些是异常样本，哪些是非异常样本，那么这就是我们要找的能够评价异常检测算法的标准方法。</p>
</blockquote>
<p>还是以飞机发动机的为例，现在假如你有了一些带标签数据，我们用$y=0$表示完全正常的样本，用$y=1$表示有异常的样本。</p>
<p>那么异常检测算法的推导和评价方法如下所示：</p>
<h3 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h3><ul>
<li>我们先考虑训练样本。对于训练集，我们还是需要把数据看成是无标签的，通常来讲我们把这些样本都看成<strong>正常</strong>的，但可能有一些异常的也被分到你的训练集里，这也没关系（毕竟异常的是少数）：</li>
</ul>
<script type="math/tex; mode=display">
x^{(1)},x^{(2)},...,x^{(m)}</script><ul>
<li>接下来我们要定义交叉验证集和测试集。通过这两个集合我们将得到异常检测算法。</li>
</ul>
<script type="math/tex; mode=display">
交叉验证集：(x^{(1)}\_{cv},y^{(1)}\_{cv}),...,(x^{(m)}\_{cv},y^{(m)}\_{cv})</script><script type="math/tex; mode=display">
测试集：(x^{(1)}\_{test},y^{(1)}\_{test}),...,(x^{(m)}\_{test},y^{(m)}\_{test})</script><p>继续回到我们的例子中：</p>
<p>假如说我们有10000制造的引擎作为样本。就我们所知，这些样本都是正常没有问题的飞机引擎。同样地，如果有一小部分有问题的引擎也被混入了这10000个样本，别担心，没有关系，我们假设这10000个样本中大多数都是没有问题的引擎。而且实际上从过去的经验来看，无论是制造了多少年引擎的工厂，在10000个引擎中都会得到大概20个有问题的引擎。对于异常检测的典型应用来说，异常样本的个数(也就是$y=1$的样本)，基本上很多都是20到50个，并且通常我们的正常样本的数量要大得多。</p>
<p>有了这组数据，把数据分为训练集、交叉验证集和测试集的一种典型的分法如下：</p>
<p>我们把这10000个正常的引擎放6000个到无标签的<strong>训练集</strong>中，我叫它“无标签训练集”，但其实所有这些样本都对应$y=0$的情况。</p>
<p>我们要用这6000个训练样本来拟合$p(x)$。</p>
<script type="math/tex; mode=display">
p(x)=p(x\_1;μ\_1, σ\_1^2)...p(x\_n;μ\_n, σ\_n^2)</script><p>因此我们就是要用这6000个样本来计算参数$μ_1,σ_1^2…μ_n,σ_n^2$。</p>
<p>然后我们将剩余的4000个样本一半放入<strong>交叉验证集</strong>，另一半放入<strong>测试集</strong>中。同时我们还有20个异常的发动机样本，同样也把它们进行一个分割：放10个到验证集中，剩下10个放入测试集中。</p>
<blockquote>
<p>注意：不要把交叉验证集和测试集混在一起使用，这样效果并不好。</p>
</blockquote>
<h3 id="异常检测算法的推导和评估方法"><a href="#异常检测算法的推导和评估方法" class="headerlink" title="异常检测算法的推导和评估方法"></a>异常检测算法的推导和评估方法</h3><p>有了训练集、交叉验证集和测试集，异常检测算法的推导和评估方法如下：</p>
<p>首先我们使用训练样本来拟合模型$p(x)$。</p>
<p>然后我们预设一个比较小的$ε$，对于$p(x)&lt;ε$的样本视为异常样本，然后分别在测试集合交叉验证集上进行测试和验证。我们知道在测试集合交叉验证集上是存在$y=1$的异常样本的，只不过量比较少而已。</p>
<p>这里其实我们可以把异常检测算法想象成是对交叉验证集和测试集中的$y$进行预测，这与监督学习有些类似，因为我们在对有标签的数据进行预测。所以我们可以通过对标签预测正确的次数来评价算法的好坏。</p>
<p>当然这些标签会比较偏斜，因为$y=0$(也就是正常的样本)肯定是比出现$y=1$(也就是异常样本)的情况更多。这跟我们在监督学习中用到的评价度量方法非常接近。</p>
<p>那么用什么评价度量好呢？</p>
<p>因为数据是非常偏斜的，所以通过分类准确度来衡量算法并不是一个好的度量法。我们<strong><a target="_blank" rel="noopener" href="http://t.cn/RSh83NE">之前的课程</a></strong>中也有提到过，如果你有一个比较偏斜的数据集，那么总是预测$y=0$它的分类准确度自然会很高。</p>
<p>因此我们应该算出以下数据来更科学的衡量算法的好坏：</p>
<ul>
<li>我们应该算出<strong>真阳性</strong>、<strong>假阳性</strong>、<strong>假阴性</strong>和<strong>真阴性</strong>的比率来作为评价度量值</li>
<li>我们也可以算出<strong>查准率</strong>和<strong>召回率</strong></li>
<li>计算出$F_1-score$，通过一个很简单的数字来总结出查准和召回的大小。</li>
</ul>
<p>通过这些方法，你就可以评价你的异常检测算法在交叉验证和测试集样本中的表现。</p>
<h4 id="ε是怎么得到的呢？"><a href="#ε是怎么得到的呢？" class="headerlink" title="ε是怎么得到的呢？"></a>ε是怎么得到的呢？</h4><p>现在还有一个问题没有说明，那就是参数$p(x)&lt;ε$中的$ε$是如何求得的？</p>
<p>如果你有一组交叉验证集样本，一种选择参数$ε$的方法就是通过尝试多个不同的$ε$，然后选出一个使得$F_1-score$最大的$ε$，这个$ε$就是在交叉验证集上表现最好的。</p>
<p>更一般来说,我们使用训练集、测试集和交叉验证集的方法是当我们需要作出决定时，比如要包括哪些特征或者说要确定参数$ε$取多大合适，我们就可以不断地用交叉验证集来评价这个算法，然后决定我们应该用哪些特征，以及选择哪一个$ε$。</p>
<p>所以就是在交叉验证集中评价算法，然后选出一组特征，或者找到能符合我们要求的ε的值后，我们就能用测试集来最终评价算法的表现了。</p>
<h2 id="异常检测-VS-监督学习"><a href="#异常检测-VS-监督学习" class="headerlink" title="异常检测 VS 监督学习"></a>异常检测 VS 监督学习</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Rkc5x/anomaly-detection-vs-supervised-learning">视频地址</a></p>
<blockquote>
<p>在上一节，我们谈到如何评价一个异常检测算法。我们先是用了一些带标签的数据，以及一些我们知道是异常或者正常的样本(用$y=1$或$y=0$来表示)。</p>
<p>这就引出了这样一个问题：既然我们有了带标签的数据，那么为什么我们不直接用监督学习的方法（比如逻辑回归或者神经网络）呢？</p>
<p>这一节，就来介绍一下什么时候应该用异常检测算法，什么时候用监督学习算法是更有成效的。</p>
</blockquote>
<p>下面这张表格对比了什么时候应该用什么算法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">异常检测</th>
<th style="text-align:center">监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">负样本量很少(通常是在0到20个之间),正样本很多的时候</td>
<td style="text-align:center">正负样本都很多的时候</td>
</tr>
<tr>
<td style="text-align:center">有多种不同的异常类型时（因为对任何算法来说，从大量正样本中去学习到异常具体是什么，都是困难的）；未知的异常与我们已知的异常完全不一样时。</td>
<td style="text-align:center">有足够多的正样本来让你的算法学习到对应的特征，并且在未知的正样本中，也和已知的样本是类似的。</td>
</tr>
</tbody>
</table>
</div>
<p>这就是在遇到具体情况时，要选择异常检查还是监督学习的方式。</p>
<p>其实关键区别就是<strong>在异常检测算法中我们只有一小撮正样本</strong>，因此监督学习算法不能从这些样本中学到太多东西。</p>
<hr>
<p>关于上面表格中，有关<strong>异常检测</strong>中的不同类型的异常情况，我们用之前的垃圾邮件的例子来说明。</p>
<p>在那个例子中，垃圾邮件的类型其实也有很多种。有的是想卖东西给你、有的是想钓出你的密码(这种就叫钓鱼邮件)、还有其他一些类型的垃圾邮件…但对于垃圾邮件的问题，我们能得到绝大多数不同类型的垃圾邮件，因为我们有大量的垃圾邮件样本的集合。因此这也是为什么我们通常把垃圾邮件问题看作是监督学习问题的原因，虽然垃圾邮件的种类通常有太多太多 。</p>
<p>因此，我们可以看看一些异常检测的应用和监督学习应用的比较，我们不难发现对于欺诈检测(fraud detection)，如果你掌握了许多种不同类型的诈骗方法，并且只有相对较小的训练集（只有很少一部分用户有异常行为）那我会使用异常检测算法。当然，有时候欺诈检测的方法也可能会偏向于使用监督学习算法，但是如果你并没有看到许多在你网站上进行异常行为的用户样本，那么欺诈检测通常还是被当做是一个异常检测算法，而不是一个监督学习算法。</p>
<h2 id="选择使用什么特征"><a href="#选择使用什么特征" class="headerlink" title="选择使用什么特征"></a>选择使用什么特征</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/LSpXm/choosing-what-features-to-use">视频地址</a></p>
<blockquote>
<p>前面我们讲到了异常检测算法，并且我们也讨论了如何评估一个异常检测算法。</p>
<p>事实上当你应用异常检测时，对它的效率影响最大的因素之一，是你使用什么特征变量。那么在本节，我将给出一些关于如何设计或选择异常检测算法的特征变量建议。</p>
</blockquote>
<h3 id="对不服从高斯分布的数据进行转换"><a href="#对不服从高斯分布的数据进行转换" class="headerlink" title="对不服从高斯分布的数据进行转换"></a>对不服从高斯分布的数据进行转换</h3><p>在我们的异常检测算法中，我们做的事情之一就是使用正态(高斯)分布来对特征向量建模。通常情况下，我们都需要用直方图来可视化这些数据，如下图：</p>
<p><img src="/img/17_05_25/001.png" alt=""></p>
<p>这么做的原因是为了在使用算法之前，确保我们的数据看起来是服从高斯分布的（当然即使你的数据并不是高斯分布，它也基本上可以良好地运行，但最好转换成高斯分布的样式之后在带入计算）。</p>
<p>如果你的样本的某个特征展示效果完全不像一个正态分布的形状：</p>
<p><img src="/img/17_05_25/002.png" alt=""></p>
<p>那么我们就需要对数据进行一些转换，来确保这些数据能看起来更像高斯分布。这样你的算法才能效果更好。</p>
<p>一般情况下，我们都会对原始数据尝试求对数或者开根号操作进行转换，下图是通过对数来转换的：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x$</th>
<th style="text-align:center">$log(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_25/002.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/003.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>你也可以尝试使用以下的方式来带入：</p>
<script type="math/tex; mode=display">
x←log(x)  \\\\
x←log(x + c) \\\\
x←\sqrt x</script><p>选择哪一个都可以，唯一的原则就是保证转换后的分布看起来更像高斯分布(正态分布)一些。</p>
<hr>
<p>下面是对于一个不服从高斯分布的数据进行转换的过程：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">原始数据</th>
<th style="text-align:center">$x^{0.5}$</th>
<th style="text-align:center">$x^{0.2}$</th>
<th style="text-align:center">$x^{0.1}$</th>
<th style="text-align:center">$x^{0.05}$</th>
<th style="text-align:center">$log(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_25/004.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/005.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/006.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/007.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/008.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_25/009.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>我们对原始数据尝试了不同的转换之后，图像最终趋于了正太分布的样式。在Octive中实现的过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hist(x,50)</span><br><span class="line">hist(x.^0.5,50)</span><br><span class="line">hist(x.^0.2,50)</span><br><span class="line">hist(x.^0.1,50)</span><br><span class="line">hist(x.^0.05,50)</span><br><span class="line">hist(log(x),50)</span><br><span class="line">xNew &#x3D; log(x);</span><br></pre></td></tr></table></figure>
<p>最终我们选择了$log(x)$来代替原来的$x$。</p>
<h3 id="异常检测算法的特征变量的获取"><a href="#异常检测算法的特征变量的获取" class="headerlink" title="异常检测算法的特征变量的获取"></a>异常检测算法的特征变量的获取</h3><p>对于异常检测算法的特征变量的获取的方法，其实和之前学习的误差分析步骤是类似的。</p>
<ul>
<li>首先我们先训练处一个异常检测学习算法。</li>
<li>然后在一组交叉验证集上运行算法。</li>
<li>然后找出那些异常样本。</li>
<li>然后我们尝试其他的特征变量，看是否能让我们的算法变得更好。</li>
</ul>
<h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p>我们用一个具体的例子来说明上面的过程：</p>
<p>在异常检测中，我们希望$p(x)$的值对正常样本来说是比较大的，而对异常样本来说，值是很小的。但一个很常见的问题是$p(x)$是具有可比性的（即对于两者都很大）。</p>
<p>这是我的一组无标签数据：</p>
<p><img src="/img/17_05_25/010.png" alt=""></p>
<p>这里我只有一个特征变量$x_1$，我要用高斯分布来拟合它。</p>
<p>假设我们绘制出它的高斯分布，如下图所示：</p>
<p><img src="/img/17_05_25/011.png" alt=""></p>
<p>假设我们遇到了一个有异常的样本：</p>
<p><img src="/img/17_05_25/012.png" alt=""></p>
<p>但是，从图中我们能看出这个样本在这一特征下的$p(x_1)$并不低，我们无法从这一特征下区分出这一异常样本。</p>
<p>如果我们引入另一个特征$x_2$，图像如下：</p>
<p><img src="/img/17_05_25/013.png" alt=""></p>
<p>再来看看我们的异常样本，出现在了这两个特征所分布的区域的外侧：</p>
<p><img src="/img/17_05_25/014.png" alt=""></p>
<p>这个时候，我们的异常检测算法就会给出很小的值来验证这一点代表的样本属于异常样本。</p>
<blockquote>
<p><strong>总结</strong>:选择异常检测需要考虑的特征时，先找出异常样本，然后尝试通过引入新的特征来验证对异常样本的识别的准确性。如果有所提高，就可以考虑引入这个特征。</p>
</blockquote>
<h4 id="关于特征选择的思考"><a href="#关于特征选择的思考" class="headerlink" title="关于特征选择的思考"></a>关于特征选择的思考</h4><p>最后我想与你分享一些我平时在为异常检查算法选择特征变量时的一些思考。</p>
<p>通常，我会选择那些既不是特别大也不是特别小的特征变量。以数据中心异常计算机的监测的例子为例，我们有以下四个特征：</p>
<script type="math/tex; mode=display">
x\_1=机器内存使用  \\\\
x\_2=硬盘资源使用  \\\\
x\_3=CPU使用  \\\\
x\_4=网络情况</script><p>我假设CPU使用情况和网络情况呈线性关系，正常情况下如果其中一个服务器正在运行时，CPU负载和流量都很大。</p>
<p>现在，假设有一种异常情况，就是流量消耗很小，但CPU负载却很高，因为可能是机器遇到了某个死循环导致CPU负载飙升，因此我们可以定义一个新的特征变量来更好的说明这一情况：</p>
<script type="math/tex; mode=display">
x\_5=\frac{CPU 负载}{流量消耗}</script><p>同样，你也可以尝试使用下面这种特征变量：</p>
<script type="math/tex; mode=display">
x\_6=\frac{(CPU 负载)^2}{流量消耗}</script><p>其实，解决这类问题的思路就是尝试组合新的特征，从而能更好的检测异常情况。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/05/23/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(1)%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/05/23/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B9%9D%E5%91%A8%20(1)%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第九周 (1)密度估计</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-05-23 21:59:00" itemprop="dateCreated datePublished" datetime="2017-05-23T21:59:00+00:00">2017-05-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="动机-异常检测"><a href="#动机-异常检测" class="headerlink" title="动机:异常检测"></a>动机:异常检测</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/V9MNG/problem-motivation">视频地址</a></p>
<blockquote>
<p>在接下来的一系列课程中，我将向大家介绍<strong>异常检测(Anomaly detection)问题</strong>。这是机器学习算法的一个常见应用。这种算法的一个有趣之处在于它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。</p>
</blockquote>
<p>首先，我们举一个例子来解释什么是异常检测。</p>
<p>假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)。而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如你可能测量了引擎运转时产生的热量，或者引擎的振动等等（我有一些朋友很早之前就开始进行这类工作，在实际工作中他们确实会从真实的飞机引擎采集这些特征变量）：</p>
<script type="math/tex; mode=display">
x\_1 = 产生的热量 \\\\
x\_2 = 振动的强度 \\\\
...</script><p>这样一来，如果你生产了m个引擎的话，你就有了一个从$x^{(1)}$到$x^{(m)}$的数据集：</p>
<script type="math/tex; mode=display">
｛x^{(1)}，x^{(2)}，...，x^{(m)}｝</script><p>也许你会将这些数据绘制成图表，看起来就是这个样子：</p>
<p><img src="/img/17_05_23/001.png" alt=""></p>
<p>图中每个叉都是你的无标签数据。</p>
<p>假设有一天，你有一个新的飞机引擎从生产线上流出，这个引擎的数据为$x_{test}$。那么所谓的<strong>异常检测问题</strong>就是检查这个新的飞机引擎是否存在某种异常。</p>
<p>比如说，如果你的新引擎对应的点落在这里：</p>
<p><img src="/img/17_05_23/002.png" alt=""></p>
<p>从数据上来看，它看起来像我们之前见过的引擎，因此我们可以直接认为它是正常的。然而如果你的新飞机引擎的$x_{test}$对应的点在这外面：</p>
<p><img src="/img/17_05_23/003.png" alt=""></p>
<p>那么我们可以认为这是一个异常的引擎。</p>
<h3 id="异常检测问题更正式的定义"><a href="#异常检测问题更正式的定义" class="headerlink" title="异常检测问题更正式的定义"></a>异常检测问题更正式的定义</h3><p><strong>异常检测问题</strong>更正式一些的定义如下：</p>
<p>假设我们有$m$个<strong>正常的</strong>样本数据$｛x^{(1)}，x^{(2)}，…，x^{(m)}｝$，我们需要一个算法来告诉我们一个新的样本数据$x_{test}$是否异常。</p>
<p>我们要采取的方法是：给定无标签的训练集，对数据集$x$建立一个概率分布模型$p(x)$。当我们建立了$x$的概率模型之后，我们就会说，对于新的飞机引擎$x_{test}$，如果概率$p$低于阈值$ε$：</p>
<script type="math/tex; mode=display">
p(x\_{test}) \lt ε</script><p>那么就将其标记为异常。</p>
<p>因此当我们看到一个新的引擎在我们根据训练数据得到的$p(x_{test})$模型中概率非常低时，我们就将其标记为异常；反之如果$p(x_{test})$大于给定的阈值$ε$，我们就认为它是正常的。</p>
<hr>
<p>因此在上面的飞机引擎的例子中，对于给定的训练集，对于图中的中心区域的$p(x)$会很大；而稍微远离中心区域的点概率会小一些；更远的地方的点，它们的概率将更小；在外面的点将成为异常点：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">中心区域$p(x)$很大</th>
<th style="text-align:center">$p(x)$略小</th>
<th style="text-align:center">$p(x)$很小</th>
<th style="text-align:center">异常点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_23/004.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/005.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/006.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/007.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<h3 id="异常检测的应用场景"><a href="#异常检测的应用场景" class="headerlink" title="异常检测的应用场景"></a>异常检测的应用场景</h3><h4 id="欺诈检测"><a href="#欺诈检测" class="headerlink" title="欺诈检测"></a>欺诈检测</h4><p>异常检测最常见的应用就是欺诈检测了。</p>
<p>假设你有很多用户，你的每个用户都在从事不同的活动。你可以对不同的用户活动计算特征变量，然后建立一个用来表示用户行为对应的特征向量出现的概率的模型（用来表示用户表现出各种行为的可能性的模型）。</p>
<p>因此假设你看到某个用户在网站上行为的特征变量是这样的：</p>
<script type="math/tex; mode=display">
\begin{align\*}
x\_1&:是用户登录的频率 \\\\
x\_2&:是用户访问某个页面的次数 \\\\
x\_3&:是用户在论坛上发帖的次数 \\\\
x\_4&:是用户的打字速度（有些网站是可以记录的）
\end{align\*}</script><p>因此你可以根据这些数据建一个模型$p(x)$，然后你可以通过这个模型来发现你网站上的行为奇怪的用户。你只需要看哪些用户的$p(x)\ltε$即可，接下来你就可以对这些用户的档案做进一步筛选，或者要求这些用户 验证他们的身份，从而让你的网站防御异常行为或者欺诈行为。</p>
<p>这种技术将会找到行为不正常的用户，而不仅仅是有欺诈行为的用户。然而这就是许多许多在线购物网站常常用来识别异常用户的技术。</p>
<h4 id="工业领域查找异常产品"><a href="#工业领域查找异常产品" class="headerlink" title="工业领域查找异常产品"></a>工业领域查找异常产品</h4><p>异常检测的另一个例子是在工业生产领域，事实上我们上面已经谈到过飞机引擎的问题，你可以通过异常检测找到异常的飞机引擎，然后要求进一步细查这些引擎的质量。</p>
<h4 id="计算机监控"><a href="#计算机监控" class="headerlink" title="计算机监控"></a>计算机监控</h4><p>第三个应用是数据中心的计算机监控。实际上我有些朋友正在从事这类工作。</p>
<p>如果你正在管理一个计算机集群或者一个数据中心，其中有许多计算机。那么我们可以为每台计算机计算特征变量，例如计算机的内存消耗、硬盘访问量、CPU负载或者一些更加复杂的特征（例如一台计算机的CPU负载与网络流量的比值）。</p>
<p>那么给定正常情况下数据中心中计算机的特征变量，你可以建立$p(x)$模型，通过它来找到运行不正常的计算机。</p>
<p>目前这种技术正在被各大数据中心使用，用来监测大量计算机可能发生的异常。</p>
<h2 id="高斯分布-正态分布"><a href="#高斯分布-正态分布" class="headerlink" title="高斯分布(正态分布)"></a>高斯分布(正态分布)</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/ZYAyC/gaussian-distribution">视频地址</a></p>
<blockquote>
<p>在本节我将介绍<strong>高斯分布</strong>（也称为<strong>正态分布</strong>）。</p>
<p>如果你已经对高斯分布非常熟悉了，那么也许你可以直接跳过这一节。但是如果你不确定或者你已经有段时间没有接触高斯分布了，那么请从头到尾看完这一节。在下一节中，我们将应用高斯分布来推导一套异常检测算法。</p>
</blockquote>
<h3 id="高斯分布的定义"><a href="#高斯分布的定义" class="headerlink" title="高斯分布的定义"></a>高斯分布的定义</h3><p>假设$x$是一个实数随机变量（即：$x \in R$），如果x的概率分布服从高斯分布：其中均值为$μ$，方差为$σ^2$，那么将它记作：</p>
<script type="math/tex; mode=display">
x \sim N(μ,σ^2)</script><blockquote>
<p>这里的$\sim$符号读作：”服从…分布”。大写字母$N$表示Normal (正态)，有两个参数，其中$μ$表示均值，$σ^2$表示方差。</p>
</blockquote>
<p>如果我们将高斯分布的概率密度函数绘制出来，它看起来将是这样一个钟形的曲线：</p>
<p><img src="/img/17_05_23/008.png" alt=""></p>
<p>这个钟形曲线有两个参数，分别是$μ$和$σ$。其中$μ$控制这个钟形曲线的中心位置，$σ$控制这个钟形曲线的宽度。</p>
<p>从图中可以看出来，$x$取中心区域的值的概率相当大，因为高斯分布的概率密度在这里很大；而$x$取远处和更远处数值的概率将逐渐降低，直至消失。</p>
<p>高斯分布的数学公式如下：</p>
<script type="math/tex; mode=display">
p(x;μ,σ^2)=
\frac{1}{\sqrt{2π}σ}exp(-\frac{(x-μ)^2}{2σ^2})</script><p>其实我们并不需要记住这个公式，当我们真的需要用到它时，我们总可以查资料找到它。</p>
<h3 id="高斯分布中，-μ-和-σ-的关系"><a href="#高斯分布中，-μ-和-σ-的关系" class="headerlink" title="高斯分布中，$μ$和$σ$的关系"></a>高斯分布中，$μ$和$σ$的关系</h3><p>我们举例来说明一下高斯分布中$μ$和$σ$这两个参数之间的关系：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$μ=0$，$σ=1$</th>
<th style="text-align:center">$μ=0$，$σ=0.5$</th>
<th style="text-align:center">$μ=0$，$σ=2$</th>
<th style="text-align:center">$μ=3$，$σ=0.5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_23/009.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/010.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/011.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/012.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>值得提醒的是，在高斯分布的图像中，不管曲线的形状如何，曲线围城的总面积都是1。所以如果$σ$很大，就意味着数据的离散化程度越大，中间区域就会变宽，但由于总概率为1，所以高度会降低；反之如果$σ$很小，就意味着数据的离散化程度越小，中间区域就会变窄，但由于总概率为1，所以高度会升高。</p>
<h3 id="参数估计问题"><a href="#参数估计问题" class="headerlink" title="参数估计问题"></a>参数估计问题</h3><p>接下来让我们来看参数估计问题。</p>
<p>假设我们有以下数据集：</p>
<script type="math/tex; mode=display">
｛x^{(1)},x^{(2)},...,x^{(m)}｝ \ \ \ \ \ \ \  x^{(i)} \in R</script><p>其对应的数据在图像中如下：</p>
<p><img src="/img/17_05_23/013.png" alt=""></p>
<p>如果这些数据是服从正态分布的：</p>
<script type="math/tex; mode=display">
x \sim N(μ,σ^2)</script><p>但我们只有数据，并不知道参数$μ$和$σ$的具体值。那么参数估计问题，就是在寻找这些参数具体值的问题。</p>
<h4 id="高斯分布的参数估计公式"><a href="#高斯分布的参数估计公式" class="headerlink" title="高斯分布的参数估计公式"></a>高斯分布的参数估计公式</h4><p>具体来说，高斯分布中的参数估计公式如下：</p>
<script type="math/tex; mode=display">
μ=\frac{1}{m}\sum\_{i=1}^mx^{(i)}</script><script type="math/tex; mode=display">
σ^2=\frac{1}{m}\sum\_{i=1}^m(x^{(i)}-μ)^2</script><p>可以看出来，$μ$是在对所有m个样本求均值，$σ^2$实际上是对所有样本与均值做差再取平方后得到的平均大小。</p>
<blockquote>
<p>再提一下，如果你精通统计学，你可能听过<strong>极大似然估计</strong>，那么这里的估计实际就是对$μ$和$σ^2$的极大似然估计。如果你不知道，也无所谓。</p>
<p>还有一点，如果你在学习统计学时，可能会见到这个式子：$σ^2=\frac{1}{m-1}\sum_{i=1}^m(x^{(i)}-μ)^2$，但在机器学习领域，大家习惯使用$σ^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)}-μ)^2$，其实在实际情况中，具体使用$\frac{1}{m}$还是$\frac{1}{m-1}$其实区别很小，只要你有一个稍大的数据集。这两个版本的公式在理论特性和数学特性上稍有不同，但在实际应用中，他们的区别甚小，几乎可以忽略不计。</p>
</blockquote>
<h2 id="异常检测的具体算法"><a href="#异常检测的具体算法" class="headerlink" title="异常检测的具体算法"></a>异常检测的具体算法</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/C8IJp/algorithm">视频地址</a></p>
<blockquote>
<p>在上节课中，我们谈到了高斯分布。在本节我将应用高斯分布开发异常检测算法。</p>
</blockquote>
<p>假如说我们有一个无标签的训练集，其中共有$m$个训练样本，并且这里的训练集里的每一个样本都是$n$维的特征，因此你的训练集应该是$m$个$n$维的特征构成的样本矩阵：</p>
<script type="math/tex; mode=display">
｛x^{(1)},...,x^{(m)}｝   \\\\ 
x \in R^n</script><p>对于我们的异常检测算法，我们要从数据中建立一个$p(x)$概率模型。由于$x$是一个向量，因此：</p>
<script type="math/tex; mode=display">
p(x)=p(x\_1)p(x\_2)p(x\_3)...p(x\_n)</script><p>我们假定特征$x_1$服从高斯正态分布:</p>
<script type="math/tex; mode=display">
x\_1 \sim N(μ\_1,σ^2\_1)</script><p>根据上节学到的知识，你可以得出对应的$μ_1$和$σ_1$:</p>
<script type="math/tex; mode=display">
μ\_1=\frac{1}{m}\sum\_{i=1}^mx^{(i)}\_1</script><script type="math/tex; mode=display">
σ^2\_1=\frac{1}{m}\sum\_{i=1}^m(x^{(i)}\_1-μ\_1)^2</script><p>这样$p(x_1)$就可以写成这样一个高斯分布:</p>
<script type="math/tex; mode=display">
p(x\_1)=p(x\_1;μ\_1,σ^2\_1)</script><p>同样地，我假设$x_2$也服从高斯分布，可以得出：</p>
<script type="math/tex; mode=display">
p(x\_2)=p(x\_2;μ\_2,σ^2\_2)</script><p>与此类似$x_3$服从另外一个高斯分布:</p>
<script type="math/tex; mode=display">
p(x\_3)=p(x\_3;μ\_3,σ^2\_3)</script><p>直到$x_n$:</p>
<script type="math/tex; mode=display">
p(x\_n)=p(x\_n;μ\_n,σ^2\_n)</script><p>因此可以得出:</p>
<script type="math/tex; mode=display">
\begin{align\*}
p(x) 
&= p(x\_1;μ\_1,σ^2\_1)p(x\_2;μ\_2,σ^2\_2)p(x\_3;μ\_3,σ^2\_3)...p(x\_n;μ\_n,σ^2\_n) \\\\
&= Π\_{j=1}^np(x\_j;μ\_j,σ^2\_j)
\end{align\*}</script><p>其中$Π$（读作pai，是$π$的大写形式）类似$∑$符号，只不过这里将连加换成了连乘。顺便要说的是，估计$p(x)$的分布问题，通常被称为<strong>密度估计</strong>问题。</p>
<blockquote>
<p>注意：对于熟悉统计学的同学来说，上面的式子实际上就对应于一个从$x_1$到$x_n$的独立的假设。但实际应用中，无论这些特征是否相互独立，这些算法的效果都还不错。</p>
</blockquote>
<h3 id="异常检测算法步骤总结"><a href="#异常检测算法步骤总结" class="headerlink" title="异常检测算法步骤总结"></a>异常检测算法步骤总结</h3><p>让我们来总结一下<strong>异常检测</strong>算法的具体步骤：</p>
<ul>
<li>1.从样本中选择一些能体现出异常行为的特征$x_i$。</li>
</ul>
<blockquote>
<p>我们可以尝试找出一些特征，比如在你的系统里，那些能看出用户异常行为或者欺诈行为的特征。</p>
</blockquote>
<ul>
<li>2.分别计算出每个特征的参数$μ_1,…,μ_n,σ^2_1,…,σ^2_n$。</li>
</ul>
<script type="math/tex; mode=display">
μ=
\begin{equation} 
\left[
\begin{matrix}
     μ\_1 \\\\
     μ\_2 \\\\
     ┋     \\\\
     μ\_n
\end{matrix}
\right]
\end{equation}
=
\frac{1}{m}\sum\_{i=1}^mx^{(i)}</script><script type="math/tex; mode=display">
σ^2=
\begin{equation} 
\left[
\begin{matrix}
     σ^2\_1 \\\\
     σ^2\_2 \\\\
     ┋     \\\\
     σ^2\_n
\end{matrix}
\right]
\end{equation}
=
\frac{1}{m}\sum\_{i=1}^m(x^{(i)}-μ)^2</script><p>其中：</p>
<script type="math/tex; mode=display">
μ\_j=\frac{1}{m}\sum\_{i=1}^mx^{(i)}\_j</script><script type="math/tex; mode=display">
σ^2\_j=\frac{1}{m}\sum\_{i=1}^m(x^{(i)}\_j-μ\_j)^2</script><blockquote>
<p>对$m$个无标签数据分别计算出他们每个特征的期望$μ$和方差$σ^2$。<strong>注意，这里$μ$和$σ$都是m维度的向量</strong>，而$μ_j$和$σ_j$都是其中对应的第$j$个元素。</p>
</blockquote>
<ul>
<li>3.给定一个新的样本$x$，计算出它对应的$p(x)$:</li>
</ul>
<script type="math/tex; mode=display">
p(x)=Π\_{j=1}^np(x\_j;μ\_j,σ^2\_j)=Π\_{j=1}^n\frac{1}{\sqrt{2π}σ\_j}exp(-\frac{(x\_j-μ\_j)^2}{2σ^2\_j})</script><p>通过判断$p(x)&lt;ε$，来判断是否有异常发生。</p>
<blockquote>
<p>给定一个用户行为的样本，如何知道用户行为是否异常呢？我们将用户行为数据带入到$p(x)$的计算中来，如果这个结果非常小，那么我们就将这个行为标注为异常行为。</p>
</blockquote>
<h3 id="异常分析例子"><a href="#异常分析例子" class="headerlink" title="异常分析例子"></a>异常分析例子</h3><p>假如说我们有下面这样的数据集：</p>
<p><img src="/img/17_05_23/014.png" alt=""></p>
<p>从图中我们可以看出，数据集有两个特征$x_1$和$x_2$。</p>
<p>其中特征$x_1$对应的是水平方向的数据，它的均值是5，标准差是2；$x_2$对应的是竖直方向上的数据，它的均值是3，标准差是1：</p>
<script type="math/tex; mode=display">
μ\_1=5,σ\_1=2</script><script type="math/tex; mode=display">
μ\_2=3,σ\_2=1</script><p>这两个特征对应的分布如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$p(x_1;μ_1,σ^2_1)$</th>
<th style="text-align:center">$p(x_2;μ_2,σ^2_2)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_05_23/015.png" alt=""></td>
<td style="text-align:center"><img src="/img/17_05_23/016.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>如果绘制出$p(x)$的图像，那么这个图像如下：</p>
<p><img src="/img/17_05_23/017.png" alt=""></p>
<p>通过图像，我们可以得出具体的某一点对应的高度值。</p>
<p>假如$x_1=2$，$x_2=2$那么就是这个点:</p>
<p><img src="/img/17_05_23/018.png" alt=""></p>
<p>在3-D表面图上的高度就代表$p(x)$的值。而这个$p(x)$完整的写出来就是下面的形式：</p>
<script type="math/tex; mode=display">
p(x)=p(x\_1;μ\_1,σ^2\_1)p(x\_2;μ\_2,σ^2\_2)</script><p>那么有了这个表达式，我们如何鉴定新的样本是否异常呢？</p>
<p>要回答这个问题，我们可以先给计算机设某个无穷小的数值$ε$，假如我设置$ε=0.02$(我会在后面讲到如何选取$ε$的值)。</p>
<p>现在我们有两个样本，分别为$x_{test}^{(1)}$和$x_{test}^{(2)}$：</p>
<p><img src="/img/17_05_23/019.png" alt=""></p>
<p>我们用上面的式子来计算出$p(x_{test}^{(1)})$，可以发现这是一个比较大的数，具体大小是大于等于$ε$的，所以对于$x_{test}^{(1)}$的检测结果是不属于异常。同样对于$p(x_{test}^{(2)})$，我们发现这是一个很小的数，具体值是小于$ε$的，所以我们说$x_{test}^{(2)}$属于异常数据。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/05/12/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AB%E5%91%A8%20(2)%E9%99%8D%E7%BB%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/05/12/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AB%E5%91%A8%20(2)%E9%99%8D%E7%BB%B4/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第八周 (2)降维</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-05-12 07:51:58" itemprop="dateCreated datePublished" datetime="2017-05-12T07:51:58+00:00">2017-05-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>在这个模块中，我们将介绍<strong>主成分分析（PCA）</strong>，并显示它可以用于数据压缩，加快学习算法，以及可视化的复杂数据集。</p>
</blockquote>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><h2 id="动机I：数据压缩"><a href="#动机I：数据压缩" class="headerlink" title="动机I：数据压缩"></a>动机I：数据压缩</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/0EJ6A/motivation-i-data-compression">视频地址</a></p>
<blockquote>
<p>本节我将开始介绍第二种无监督学习问题，它叫<strong>降维(dimensionality reduction)</strong>。</p>
<p>我们希望使用降维的一个主要原因是数据压缩。我们会在后几节中看到，数据压缩不仅通过压缩数据使得数据占用更少的计算机内存和硬盘空间，它还能给算法提速。</p>
</blockquote>
<p>首先我们来介绍什么是降维。</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>举一个例子，假如我们有一个有很多很多很多特征变量的数据集：</p>
<p><img src="/img/17_05_12/001.png" alt=""></p>
<p>这里为了方便展示，只画了其中两个。</p>
<p>假设我们不知道这两个特征量。其中$x_1$是某个物体的长度，以厘米为单位；另一个$x_2$是它以英寸为单位的长度。所以这是一个非常冗余的数据，与其用两个特征变量$x_1$和$x_2$，它们都是测量到的长度，或许我们应该把这个数据降到一维，只用一个长度的数据。</p>
<p>这个例子可能看起来好像是我生造的，但这个厘米英寸的例子其实还真不是那么无聊。我在工业界看到的情况也是大同小异。</p>
<blockquote>
<p>如果你有上百或者上千的特征变量，很容易就会忘记你到底有什么特征变量，而且有时候可能有几个不同的工程师团队。一队工程师可能给你200个特征变量，第二队工程师可能再给你300个特征变量，然后第三队工程师给你500个特征变量。所以你一共有1000个特征变量，这样就很难搞清哪个队给了你什么特征变量。实际上得到这样冗余的特征变量并不难。</p>
</blockquote>
<p>所以如果以厘米计的长度被取整到最近的厘米整数，以英寸计的长度被取整到最近的英寸整数。这就是为什么这些样本没有完美地在一条直线上。就是因为取整所造成的误差。</p>
<p><img src="/img/17_05_12/002.png" alt=""></p>
<p>这种情况下，如果我们可以把数据降到一维而不是二维，就可以减少冗余。</p>
<h3 id="降维含义"><a href="#降维含义" class="headerlink" title="降维含义"></a>降维含义</h3><p>让我们再详细讲讲从二维降到一维到底意味着什么。</p>
<h4 id="二维降到一维"><a href="#二维降到一维" class="headerlink" title="二维降到一维"></a>二维降到一维</h4><p>让我给这些样本涂上不同的颜色涂上不同的颜色：</p>
<p><img src="/img/17_05_12/003.png" alt=""></p>
<p>在这个例子中降低维度的意思是：我希望找到一条线，基本所有数据映射到这条线上。这样做之后，我就可以直接测量这条线上每个样本的位置。我想把这个新特征叫做$z_1$。</p>
<p><img src="/img/17_05_12/004.png" alt=""></p>
<p>要确定这条线上的位置，我只需要一个数字。这就是说新特征变量$z_1$能够表示这条绿线上每一个点的位置。</p>
<p>在之前如果想要表示一个样本点，我需要一个二维向量$(x_1,x_2)$，但是现在我可以用一个一维向量$z_1$来表示这个样本点：</p>
<p><img src="/img/17_05_12/005.png" alt=""></p>
<p>总结一下，在把所有训练样本映射到一条线上之后，我就能做到只用一个数字来表示每个训练样本的位置。这是一个对原始训练样本的近似。相对于之前需要用两个数字来表示一个样本而言，现在我只需要一个数字就可以表示了。这样就减少了一半的内存需求或者硬盘需求。</p>
<p>更重要的是，数据压缩还会让我们的学习算法运行地更快。</p>
<h4 id="三维降到二维"><a href="#三维降到二维" class="headerlink" title="三维降到二维"></a>三维降到二维</h4><p>现在，我展示一个把三维数据降到二维的例子。</p>
<p><img src="/img/17_05_12/006.png" alt=""></p>
<blockquote>
<p>顺便说一下，在更典型的降维例子中，我们可能有1000维的数据，我们可能想降低到100维，但是因为我在这里能可视化的展示数据的维度是有限制的，所以我要用的例子是三维到二维的。</p>
</blockquote>
<p>我们有一个图上这样的数据集，我有一个样本$x^{(i)}$的集合，$x^{(i)}$是一个三维实数的点，所以我的样本是三维的：</p>
<script type="math/tex; mode=display">
x^{(i)} \in R^3</script><p>实际上，这些样本点，差不多都处于同一平面上。降维在这里的作用，就是把所有的数据，都投影到一个二维的平面内。所以，我们要对所有的数据进行投影，使得它们落在这个平面上：</p>
<p><img src="/img/17_05_12/007.png" alt=""></p>
<p>最后为了表示一个点在平面上的位置，我们需要两个数来表示平面上一个点的位置。这两个数可能叫做$z_1$和$z_2$：</p>
<p><img src="/img/17_05_12/008.png" alt=""></p>
<p>这也意味着我们现在可以用一个二维向量$z$来表示每一个训练样本了：</p>
<p><img src="/img/17_05_12/009.png" alt=""></p>
<script type="math/tex; mode=display">
z^{(i)} \in R^2</script><hr>
<p>为了更好的理解降维的过程，现在让我们用3D绘图来重现上面的整个过程：</p>
<p><img src="/img/17_05_12/010.png" alt=""></p>
<p>我们走的过程是这样的：左边是原始数据集，中间是投影到2D的数据集，右边是以$z_1$和$z_2$为坐标轴的2D数据集。</p>
<p>我们来更详细地看一下：</p>
<p>原始数据集是这样的：</p>
<p><img src="/img/17_05_12/011.gif" alt=""></p>
<p>可以看出来，大部分数据差不多可能都落在某个2D平面上，或者说距离某个2D平面不远。</p>
<p>所以我们可以把它们投影到2D平面上。下面是投影后的效果：</p>
<p><img src="/img/17_05_12/012.gif" alt=""></p>
<p>你可以看到所有的数据落在一个平面上，因为我们把所有的东西都投影到一个平面上了。所以我们现在只需要两个数:$z_1$和$z_2$来表示点在平面上的位置即可：</p>
<p><img src="/img/17_05_12/013.png" alt=""></p>
<p>这就是把数据从三维降到二维的过程。</p>
<p>这就是降维以及如何使用它来压缩数据的过程。</p>
<h2 id="动机II：可视化数据"><a href="#动机II：可视化数据" class="headerlink" title="动机II：可视化数据"></a>动机II：可视化数据</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/t6pYD/motivation-ii-visualization">视频地址</a></p>
<blockquote>
<p>在上节中，我们讲到一种通过数据降维来进行数据压缩的方法。在本节我将会讲到第二种数据降维的应用，那就是<strong>可视化数据</strong>。</p>
<p>对于大多数的机器学习应用，它真的可以帮助我们来开发高效的学习算法，但前提是我们能更好地理解数据。降维就是数据可视化的一种方法。</p>
</blockquote>
<p>假如我们已经收集了大量的有关全世界不同国家的统计数据集：</p>
<p><img src="/img/17_05_12/014.png" alt=""></p>
<p>第一个特征$x_1$是国家的国内生产总值；第二个特征$x_2$是一个百分比，表示人均占有的GDP；第三个特征$x_3$是人类发展指数；第四个特征$x_4$是预期寿命；…直到$x_{50}$</p>
<p>在这里我们有大量的国家的数据，对于每个国家有50个特征。我们有这样的众多国家的数据集，为了使得我们能更好地来理解数据，我们需要对数据进行可视化展示。这里我们有50个特征，但绘制一幅50维度的图是异常困难的，因此我们需要对数据进行降维，然后再可视化。</p>
<p>具体做法如下：</p>
<p>我们使用特征向量$x^{(i)}$来表示每个国家。$x^{(i)}$有着50个维度。我们需要对这50个特征降维之后，我们可以用另一种方式来代表$x^{(i)}$：使用一个二维的向量$z$来代替之前50维的$x$。</p>
<p><img src="/img/17_05_12/015.png" alt=""></p>
<script type="math/tex; mode=display">
z^{(i)} \in R^2</script><p>我们用$z_1$和$z_2$这两个数来总结50个维度的数据，我们可以使用这两个数来绘制出这些国家的二维图，使用这样的方法尝试去理解二维空间下不同国家在不同特征的差异会变得更容易。</p>
<p>在降维处理时，我们用$z_1$来表示那些象征着国家整体情况的数据，例如”国家总面积”、”国家总体经济水平”等；用$z_2$来表示象征着人均情况的数据，例如”人均GDP”，”人均幸福感”等。</p>
<p>降维处理之后，将数据按照这两个维度展示如下：</p>
<p><img src="/img/17_05_12/016.png" alt=""></p>
<p>在图中，右侧的点，象征着国家整体经济比较好的国家；上方的点，象征着人均经济比较好、人均幸福感较高、人均寿命较长…的国家。</p>
<hr>
<p>那么具体我们要如何去压缩数据达到降维的效果呢？在下一节视频中我们将会开始开发一种特别的算法。简称<strong>PCA</strong>或者<strong>主成分分析 </strong>。这个算法允许我们进行数据可视化，同时可以进行早先我们提到的一些有关数据压缩方面的应用。</p>
<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><h2 id="主成分分析（PCA）相关概念"><a href="#主成分分析（PCA）相关概念" class="headerlink" title="主成分分析（PCA）相关概念"></a>主成分分析（PCA）相关概念</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/GBFTt/principal-component-analysis-problem-formulation">视频地址</a></p>
<blockquote>
<p>对于降维问题来说，目前最流行最常用的算法是<strong>主成分分析法(Principal Componet Analysis, PCA）</strong>。</p>
<p>在本节中，我想首先开始讨论PCA问题的公式描述，也就是说，我们用公式准确地精确地描述：我们想让PCA来做什么。</p>
</blockquote>
<h3 id="PCA的执行过程2D-gt-1D"><a href="#PCA的执行过程2D-gt-1D" class="headerlink" title="PCA的执行过程2D -&gt; 1D"></a>PCA的执行过程2D -&gt; 1D</h3><p>假设我们有这样的一个数据集:</p>
<p><img src="/img/17_05_12/017.png" alt=""></p>
<p>这个数据集含有二维实数空间内的样本X。</p>
<p>假设我想对数据进行降维，从二维降到一维。也就是说我想找到一条直线将数据投影到这条直线上，那怎么找到一条好的直线来投影这些数据呢？ </p>
<p>这样的一条直线也许是个不错的选择：</p>
<p><img src="/img/17_05_12/018.png" alt=""></p>
<p>你认为这是一个不错的选择的原因是：如果你观察投影到直线上的点的位置，我们发现每个点到它们对应的投影到直线上的点之间的距离非常小。（也就是说这些蓝色的线段非常的短）：</p>
<p><img src="/img/17_05_12/019.png" alt=""></p>
<p>所以，正式的说<strong>PCA</strong>所做的就是<strong>寻找一个低维的面(在这个例子中，其实是一条直线）数据投射在上面，使得这些蓝色小线段的平方和达到最小值</strong>。这些蓝色线段的长度被叫做<strong>投影误差</strong>。</p>
<p>所以<strong>PCA</strong>所做的就是寻找一个投影平面，对数据进行投影，使得这个能够最小化。</p>
<p>另外在应用<strong>PCA</strong>之前，通常的做法是先进行<strong>均值归一化</strong>和<strong>特征规范化</strong>，使得特征$x_1$和$x_2$均值为0，数值在可比较的范围之内。</p>
<blockquote>
<p>在这个例子里，我已经这么做了。但是在后面我还将回过来讨论更多有关PCA背景下的特征规范化和均值归一化问题。</p>
</blockquote>
<hr>
<p>我们正式一点地写出<strong>PCA</strong>的目标是这样的：</p>
<p>如果我们将数据从二维降到一维的话，我们需要试着寻找一个向量$u^{(i)}$，该向量属于$n$维空间中的向量（在这个例子中是二维的），我们将寻找一个对数据进行投影的方向，使得<strong>投影误差能够最小</strong>（在这个例子里，我们把PCA寻找到这个向量记做$u^{(1)}$）：</p>
<p><img src="/img/17_05_12/020.png" alt=""></p>
<p>所以当我把数据投影到这条向量所在的直线上时，最后我将得到非常小的重建误差。</p>
<blockquote>
<p>另外需要说明的时无论PCA给出的是这个$u^{(1)}$是正还是负都没关系。因为无论给的是正的还是负的$u^{(1)}$它对应的直线都是同一条，也就是我将投影的方向。</p>
</blockquote>
<p>这就是将二维数据降到一维的例子。</p>
<p>更一般的情况是我们有$n$维的数据想降到$k$维。在这种情况下我们不仅仅只寻找单个的向量（$u^{(1)}$）来对数据进行投影，我们要找到$k$个方向($u^{(k)}$)来对数据进行投影，从而最小化投影误差。</p>
<h3 id="PCA的执行过程3D-gt-2D"><a href="#PCA的执行过程3D-gt-2D" class="headerlink" title="PCA的执行过程3D -&gt; 2D"></a>PCA的执行过程3D -&gt; 2D</h3><p>下面的例子中，假设我有一些三维数据点：</p>
<p><img src="/img/17_05_12/021.png" alt=""></p>
<p>我想要做的是是寻找两个向量$u^{(1)}$和$u^{(2)}$：</p>
<p><img src="/img/17_05_12/022.png" alt=""></p>
<p>这两个向量一起定义了一个二维平面，我将把数据投影到这个二维平面上。</p>
<blockquote>
<p>如果你精通线性代数，那么这里更正式的定义是：我们将寻找一组向量$u^{(1)}$，$u^{(2)}$，…，$u^{(k)}$，我们将要做的是将数据投影到这$k$个向量展开的线性子空间上。</p>
<p>但是如果你不熟悉线性代数，那就想成是寻找$k$个方向（而不是之寻找一个方向）对数据进行投影。</p>
</blockquote>
<p>所以对于3D降维到2D的这个例子来说，寻找一个$k$维的平面，就是在寻找二维的平面。</p>
<hr>
<p>因此<strong>PCA</strong>做的就是：<strong>寻找一组$k$维向量(一条直线、或者平面、或者诸如此类等等)对数据进行投影，来最小化正交投影误差。</strong></p>
<h3 id="PCA和线性回归的关系"><a href="#PCA和线性回归的关系" class="headerlink" title="PCA和线性回归的关系"></a>PCA和线性回归的关系</h3><p>最后一个我有时会被问到的问题是：<strong>PCA和线性回归有怎么样的关系？</strong></p>
<p>因为当我解释<strong>PCA</strong>的时候，我有时候会画出这样看上去有点像线性回归的图：</p>
<p><img src="/img/17_05_12/023.png" alt=""></p>
<p>但是，事实上<strong>PCA不是线性回归</strong>。尽管看上去有一些相似，但是它们确实是两种不同的算法。</p>
<h4 id="不同点-之一"><a href="#不同点-之一" class="headerlink" title="不同点 之一"></a>不同点 之一</h4><p>如果我们做线性回归，我们做的是在给定某个输入特征$x$的情况下预测某个变量$y$的数值。因此对于线性回归，我们想做的是拟合一条直线，来最小化点和直线之间的平方误差：</p>
<p><img src="/img/17_05_12/024.png" alt=""></p>
<p>所以我们要最小化的是，上图中蓝线幅值的平方。注意我画的这些蓝色的垂直线，这是垂直距离。它是某个点与通过假设的得到的其预测值之间的距离。</p>
<p>与此想反，PCA要做的是最小化这些样本点与直线的最短距离(直角距离)：</p>
<p><img src="/img/17_05_12/025.png" alt=""></p>
<p>这是一种非常不同的效果。</p>
<h4 id="不同点-之二"><a href="#不同点-之二" class="headerlink" title="不同点 之二"></a>不同点 之二</h4><p>更更更一般的是，当你做线性回归的时候，有一个特别的变量$y$作为我们即将预测的值，线性回归所要做的就是用$x$的所有的值来预测$y$。然而在PCA中，没有这么一个特殊的变量$y$是我们要预测的。我们所拥有的是特征$x_1$,$x_2$,…,$x_n$，所有的这些特征都是被同样地对待。</p>
<p>在上面那个从3维降到2维的例子中，原先的3个特征$x_1$,$x_2$,$x_3$都是被同样地对待的，没有特殊的变量$y$需要被预测。</p>
<hr>
<p>因此，PCA不是线性回归。尽管有一定程度的相似性，使得它们看上去是有关联的，但它们实际上是非常不同的算法。</p>
<p>因此，希望你们能理解PCA是做什么的：它是寻找到一个低维的平面，对数据进行投影，以便最小化投影误差平方的（最小化每个点与投影后的对应点之间的距离的平方值）。</p>
<h2 id="PCA算法-实现过程"><a href="#PCA算法-实现过程" class="headerlink" title="PCA算法 实现过程"></a>PCA算法 实现过程</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/GBFTt/principal-component-analysis-problem-formulation">视频地址</a></p>
<blockquote>
<p>本节，将介绍<strong>PCA</strong>算法的具体细节，学完本节后，你就应该知道<strong>PCA</strong>的实现过程，并且应用PCA来给你的数据降维了。</p>
</blockquote>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>在使用PCA之前，我们通常会有一个数据预处理的过程。</p>
<p>拿到某组有m个无标签样本的训练集，一般先进行<strong>均值归一化(mean normalization)</strong>。这一步很重要。然后还可以进行<strong>特征缩放(feature scaling)</strong>，这根据你的数据而定。</p>
<blockquote>
<p>这跟我们之前在<strong>监督学习</strong>中提到的<strong>均值归一</strong>和<strong>特征缩放</strong>是一样的。</p>
</blockquote>
<h4 id="数据预处理第一步：均值归一化-mean-normalization"><a href="#数据预处理第一步：均值归一化-mean-normalization" class="headerlink" title="数据预处理第一步：均值归一化(mean normalization)"></a>数据预处理第一步：均值归一化(mean normalization)</h4><p>对于<strong>均值归一</strong>，我们首先应该计算出每个特征的均值$μ$，然后我们用$x-μ$来替换掉$x$。这样就使得所有特征的均值为0。</p>
<p><strong>举例说明：</strong></p>
<p>比如说，如果$x_1$表示房子的面积，$x_2$表示房屋的卧室数量，然后我们可以把每个特征进行缩放，使其处于同一可比的范围内。</p>
<p>同样地，跟之前的监督学习类似，我们可以首先计算出每个特征的均值：</p>
<script type="math/tex; mode=display">
μ\_j=\frac{1}{m}\sum\_{i=1}^{m}x\_j^{(i)}</script><p>然后每个样本值对应的特征减去其对应的均值：</p>
<script type="math/tex; mode=display">
x\_j^{(i)} ← x\_j^{(i)}-μ\_j</script><p>将所有的特征替换为这种形式的结果。这样就保证了所有特征的均值为0。</p>
<h4 id="数据预处理第二步：特征缩放-feature-scaling"><a href="#数据预处理第二步：特征缩放-feature-scaling" class="headerlink" title="数据预处理第二步：特征缩放(feature scaling)"></a>数据预处理第二步：特征缩放(feature scaling)</h4><p>然后，由于不同特征的取值范围都很不一样，我们还需要进行<strong>特征缩放</strong>。</p>
<p>我们需要将每个特征的取值范围都划定在同一范围内，因此对于均值化处理之后的特征值$x_j^{(i)}-μ_j$，我们还需要做进一步处理：</p>
<script type="math/tex; mode=display">
x\_j^{(i)} ← \frac{x\_j^{(i)}-μ\_j}{s\_j}</script><p>这里$s_j$表示特征$j$度量范围，即该特征的最大值减去最小值。</p>
<h3 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h3><p>接下来就正式进入PCA的算法部分。</p>
<p>在之前的视频中，我们已经知道了PCA的原理。PCA是在试图找到一个低维的子空间，然后把原数据投影到子空间上，并且最小化平方投影误差的值（投影误差的平方和，即下图中蓝色线段长度的平方和）：</p>
<p><img src="/img/17_05_12/026.png" alt=""></p>
<p>那么应该怎样来计算这个子空间呢? 实际上这个问题有完整的数学证明来解释如何找到这样的子空间，不过这个数学证明过程是非常复杂的，同时也超出了本课程的范围。但如果你推导一遍这个数学证明过程，你就会发现要找到$u^{(1)}$的值，也不是一件很难的事。但在这里，我不会给出证明，我只是简单描述一下实现PCA所需要进行的步骤。</p>
<p>假如说我们想要把数据从$n$维降低到$k$维，我们首先要做的是计算出下面这个协方差矩阵(通常用$∑$来表示)：</p>
<script type="math/tex; mode=display">
∑=\frac{1}{m}\sum\_{i=1}^n(x^{(i)})(x^{(i)})^T</script><blockquote>
<p>很不幸的是，这个希腊符号$∑$和求和符号重复了。希望你对这里不要产生混淆。</p>
</blockquote>
<p>计算出这个协方差矩阵后，假如我们把它存为Octave中的一个名为<code>Sigma</code>的变量，我们需要做的是计算出<code>Sigma</code>矩阵的<strong>特征向量(eigenvectors)</strong>。</p>
<p>在Octave中，你可以使用如下命令来实现这一功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] &#x3D; svd(Sigma); </span><br></pre></td></tr></table></figure>
<blockquote>
<p>顺便说一下，<code>svd</code>表示<strong>奇异值分解(singular value decomposition)</strong>，这是某种更高级的奇异值分解，这是比较高级的线性代数的内容。你不必掌握这些，但实际上<code>Sigma</code>是一个协方差矩阵。有很多种方法来计算它的特征向量。</p>
<p>如果你线性代数学得很好，或者你之前听说过特征向量的话，那也许知道在Octave中还有另一个<code>eig</code>命令，可以用来计算特征向量。实际上<code>svd</code>命令和<code>eig</code>命令将得到相同的结果。但<code>svd</code>其实要更稳定一些，所以我一般选择用<code>svd</code>，不过我也有一些朋友喜欢用<code>eig</code>函数。</p>
<p>在这里对协方差矩阵<code>Sigma</code>使用<code>eig</code>和<code>svd</code>时，你会得到同样的答案。这是因为协方差均值总满足一个数学性质，称为<strong>对称正定(symmetric positive definite)</strong>，其实你不必细究这个具体是什么意思，只要知道这种情况下，使用<code>eig</code>和<code>svd</code>结果是一样的就可以了。</p>
</blockquote>
<p>好了，这就是你需要了解的一点线性代数知识，如果有任何地方不清楚的话不必在意。你只需要知道上面这行Octave代码就行了。</p>
<p>如果你用除了Octave或者MATLAB之外的其他编程环境，你要做的是找到某个可以计算svd，即奇异值分解的函数库文件。在主流的编程语言中，应该有不少这样的库文件。我们可以用它们来计算出协方差矩阵的$U$ $S$ $V$矩阵。</p>
<hr>
<p>我再提几个细节问题。</p>
<p>这个协方差矩阵<code>Sigma</code>应该是一个$n×n$的矩阵，通过定义可以发现这是一个$n×1$的向量，和它自身的转置（一个$1×n$的向量）相乘得到的结果，这个结果自然是一个$n×n$的矩阵。</p>
<p>然后把这n个$n×n$的矩阵加起来，当然还是$n×n$矩阵。</p>
<p>然后svd将输出三个矩阵，分别是$U$ $S$ $V$。你真正需要的是$U$矩阵。</p>
<p>$U$矩阵也是一个$n×n$矩阵：</p>
<p><img src="/img/17_05_12/027.png" alt=""></p>
<p>实际上$U$矩阵的列元素就是我们需要的$u^{(1)}$,$u^{(1)}$等等。</p>
<p>如果我们想将数据的维度从$n$降低到$k$的话，我们只需要提取前$k$列向量。这样我们就得到了$u^{(1)}$到$u^{(k)}$，也就是我们用来投影数据的$k$个方向。</p>
<p>我们取出$U$矩阵的前$k$列得到一个新的，由$u^{(1)}$到$u^{(k)}$组成的矩阵$U_{reduce}$：</p>
<script type="math/tex; mode=display">
\begin{equation}
U\_{reduce}=\left[
\begin{matrix}
|&|&|&...&|\\\\
|&|&|&...&|\\\\
u^{(1)}&u^{(2)}&u^{(3)}&...&u^{(k)}\\\
|&|&|&...&|\\\\
|&|&|&...&|\\\\
\end{matrix}
\right]
\end{equation}</script><p>这是一个$n × k$维的矩阵。</p>
<p>然后我们用这个$U_{reduce}$来对我的数据进行<strong>降维</strong>。我们定义：</p>
<script type="math/tex; mode=display">
\begin{equation}
z=\left[
\begin{matrix}
|&|&|&...&|\\\\
|&|&|&...&|\\\\
u^{(1)}&u^{(2)}&u^{(3)}&...&u^{(k)}\\\
|&|&|&...&|\\\\
|&|&|&...&|\\\\
\end{matrix}
\right]
^{T}x
\\\\
=
\left[
\begin{matrix}
-&-&u^{(1)}&...&-\\\\
-&-&u^{(2)}&...&-\\\\
-&-&u^{(3)}&...&-\\\
.&.&.&...&.\\\\
-&-&u^{(k)}&...&-\\\\
\end{matrix}
\right]
x
\end{equation}</script><script type="math/tex; mode=display">
z \in R^k</script><p>其中$\left[<br>\begin{matrix}<br>-&amp;-&amp;u^{(1)}&amp;…&amp;-\\<br>-&amp;-&amp;u^{(2)}&amp;…&amp;-\\<br>-&amp;-&amp;u^{(3)}&amp;…&amp;-\\<br>.&amp;.&amp;.&amp;…&amp;.\\<br>-&amp;-&amp;u^{(k)}&amp;…&amp;-\\<br>\end{matrix}<br>\right]$是$k×n$的矩阵，$x$是$n×1$的矩阵，因此$z$是$k×1$的矩阵。</p>
<p>这里的$x$可以是训练集中的样本，也可以是交叉验证集中的样本，也可以是测试集样本。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总结一下，这就是PCA的全过程：</p>
<ul>
<li><p>首先进行均值归一化</p>
<ul>
<li>保证所有的特征量都是均值为0的。</li>
</ul>
</li>
<li><p>然后可以选择进行特征缩放</p>
<ul>
<li>如果不同特征量的范围跨度很大的话，你确实需要进行特征缩放这一步。</li>
</ul>
</li>
<li><p>在以上的预处理之后，我们计算出这个协方差<code>Sigma</code>矩阵：</p>
</li>
</ul>
<script type="math/tex; mode=display">
Sigma = \frac{1}{m}\sum\_{i=1}^m(x^{(i)})(x^{(i)})^T</script><ul>
<li>然后我们可以应用<code>svd</code>函数来计算出<code>U S V</code>矩阵:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] &#x3D; svd(Sigma);</span><br></pre></td></tr></table></figure>
<ul>
<li>然后，我们取出$U$矩阵的前$k$列元素组成新的$U_{reduce}$矩阵：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ureduce &#x3D; U(:,1:k);</span><br></pre></td></tr></table></figure>
<ul>
<li>最后这个式子给出了我们从原来的特征$x$变成降维后的$z$的过程:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z &#x3D; Ureduce&#96;*x;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>另外，跟<strong>k均值算法</strong>类似，如果你使用<strong>PCA</strong>的话，你的$x$应该是$n$维实数。所以没有$x_0 = 1$这一项。</p>
<p>有一件事儿我没做：$u^{(1)},u^{(2)}…u^{(k)}$通过将数据投影到$k$维的子平面上确实使得投影误差的平方和为最小值，但是我并没有证明这一点，因为这已经超出了这门课的范围。</p>
<p>幸运的是PCA算法能够用不多的几行代码就能实现它。</p>
</blockquote>
<h1 id="应用PCA"><a href="#应用PCA" class="headerlink" title="应用PCA"></a>应用PCA</h1><h2 id="对压缩数据的还原"><a href="#对压缩数据的还原" class="headerlink" title="对压缩数据的还原"></a>对压缩数据的还原</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/X8JoQ/reconstruction-from-compressed-representation">视频地址</a></p>
<blockquote>
<p>在前面的视频中我们介绍了<strong>PCA (主成分分析)</strong>作为压缩数据的算法，你会发现它能将高达一千维度的数据压缩到只有一百个维度；或者将三维数据压缩到两个维度的情况。</p>
<p>如果有一个这样的压缩算法，那么也应该有一种方法可以从压缩过的数据近似地回到原始高维度的数据。</p>
<p>假设有一个已经被压缩过的$z^{(i)}$它有100个维度，怎样使它回到其最初的表示$x^{(i)}$也就是压缩前的1000维的数据呢？ </p>
<p>在本节，我将会告诉你如何做到。</p>
</blockquote>
<p>在PCA算法中，我们有下面这些样本：</p>
<p><img src="/img/17_05_12/028.png" alt=""></p>
<p>我们让这些样本投影在一维平面$z_1$上，并且明确地指定其位置：</p>
<p><img src="/img/17_05_12/029.png" alt=""></p>
<p>那么给出一个一维实数点$z$我们能否，让$z$重新变成原来的二维实数点$x$呢？</p>
<p>即做到：</p>
<script type="math/tex; mode=display">
z \in R → x \in R^2</script><hr>
<p>我们知道:</p>
<script type="math/tex; mode=display">
z = U^T\_{reduce}x</script><p>如果想得到相反的情形，方程应这样变化:</p>
<script type="math/tex; mode=display">
x\_{approx} = U\_{reduce}z</script><p>为了检查维度，在这里$U_{reduce}$是一个$n×k$矩阵，$z$就是一个$k×1$维向量。将它们相乘得到的就是$n×1$维。</p>
<p>所以$x_{approx}$是一个$n$维向量。</p>
<p>同时根据PCA的意图，投影的平方误差不能很大。也就是说$x_{approx}$将会与最开始用来导出$z$的原始$x$很接近。用图表示出来就是这样：</p>
<p><img src="/img/17_05_12/030.png" alt=""></p>
<p>这已经与原始数据非常近似了。</p>
<p>这就是用低维度的特征数据$z$还原到未被压缩的特征数据的过程。我们找到一个与原始数据$x$近似的$x_{approx}$。我们也称这一过程为<strong>原始数据的重构(reconstruction)</strong>。</p>
<h2 id="选择主成分的数量k"><a href="#选择主成分的数量k" class="headerlink" title="选择主成分的数量k"></a>选择主成分的数量k</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/S1bq1/choosing-the-number-of-principal-components">视频地址</a></p>
<blockquote>
<p>在PCA算法中，我们把n维特征变量降维到k维特征变量。这个数字k是PCA算法的一个参数。这个数字k也被称作<strong>主成分的数量</strong>。在本节中我会给你们一些参考，告诉你们人们是怎样思考如何选择PCA的参数k的。</p>
</blockquote>
<h3 id="算法原理：最小化平均平方映射误差"><a href="#算法原理：最小化平均平方映射误差" class="headerlink" title="算法原理：最小化平均平方映射误差"></a>算法原理：最小化平均平方映射误差</h3><p>为了选择参数k（也就是要选择<strong>主成分的数量</strong>），这里有几个有用的概念：</p>
<p>PCA所做的是尽量最小化<strong>平均平方映射误差 (Average Squared Projection Error) </strong>。</p>
<p>因此PCA就是要将下面这个量最小化：</p>
<script type="math/tex; mode=display">
\frac{1}{m}\sum\_{i=1}^m||x^{i}-x\_{approx}^{(i)}||^2</script><p>即最小化$x$和其在低维表面上的映射点之间的距离的平方。这就是平均平方映射误差。</p>
<p>同时我们还要定义一下<strong>数据的总变差(Total Variation)</strong>：</p>
<script type="math/tex; mode=display">
\frac{1}{m}\sum\_{1=m}^m||x^{(i)}||^2</script><p>数据的总变差 (Total Variation) 是这些样本的长度的平方的均值。它的意思是 “平均来看，我的训练样本距离零向量（原点）多远？”。</p>
<p>当我们去选择k值的时候，我们通过平均平方映射误差除以数据的总变差来表示数据的变化有多大。我们想要这个比值能够小于1%：</p>
<script type="math/tex; mode=display">
\frac{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}-x\_{approx}^{(i)}||^2
}{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}||^2
}
\le0.01</script><p>大部分人在考虑，选择k的方法时，不是直接选择k值，而是这里的数字应该设置为多少：</p>
<p><img src="/img/17_05_12/031.png" alt=""></p>
<p>它应该是0.01还是其它的数？如果选择了0.01，那么用PCA的语言说就是保留了99%的差异性。</p>
<p>数字0.01是人们经常用的一个值，另一个常用的值是0.05。如果选择了0.05，就意味着95%的差异性被保留了。从95到99是人们最为常用的取值范围。</p>
<p>你可能会惊讶的发现，对于许多数据集，即使保留了99%的差异性，可以大幅地降低数据的维度。因为大部分现实中的数据，许多特征变量都是高度相关的。所以实际上大量压缩数据是可能的，而且仍然会保留99%或95%的差异性。</p>
<h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><p>那么你该如何实现它呢？</p>
<h4 id="原始的算法"><a href="#原始的算法" class="headerlink" title="原始的算法"></a>原始的算法</h4><p>有一种方式是从1开始，依次递增k的值，尝试检查差异性是否达到预设值。</p>
<p>例如：</p>
<ul>
<li>尝试$k=1$时的PCA。</li>
<li>计算出$U_{reduce}，z^{(1)}，z^{(2)}，…，z^{(m)}，x^{(1)}_{approx}，…，x^{(m)}_{approx}$</li>
<li>检查是否满足：</li>
</ul>
<script type="math/tex; mode=display">
\frac{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}-x\_{approx}^{(i)}||^2
}{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}||^2
}
\le0.01</script><p>如果满足条件，我们就用$k=1$；但如果不满足，那么我们接下来尝试$k=2$，然后我们要重新走一遍这整个过程。</p>
<p>以此类推一直试到上面不等式成立为止。</p>
<h4 id="一种更快的算法"><a href="#一种更快的算法" class="headerlink" title="一种更快的算法"></a>一种更快的算法</h4><p>可以想象，上面这种方式非常低效。每次尝试使用新的$k$值带入计算时，整个计算过程都需要重新执行一遍，还好我没有一种更快捷方便的计算方式。</p>
<p>当你调用<code>svd</code>来计算PCA时，你会得到三个矩阵<code>[U,S,V]</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V]&#x3D;svd(Sigma)</span><br></pre></td></tr></table></figure>
<p>除了之前提到的<code>U</code>矩阵之外，当你对协方差的矩阵<code>Sigma</code>调用<code>svd</code>时，我没还会得到中间的这个<code>S</code>矩阵。<code>S</code>矩阵是一个$n×n$的对角矩阵，它只有在对角线上的元素不为0，其余的元素都是0。并且显而易见，它是一个方阵：</p>
<script type="math/tex; mode=display">
\begin{equation}
S=\left[
\begin{matrix}
s\_{11}&0&0&...&0\\\\
0&s\_{22}&0&...&0\\\\
0&0&s\_{33}&...&0\\\
┋&┋&┋&...&┋\\\\
0&0&0&...&s\_{nn}\\\\
\end{matrix}
\right]
\end{equation}</script><p>可以证明的是（我不会在此证明）实际上对于一个给定的k值，可以通过这个$S$矩阵方便的计算出差异性那一项的值：</p>
<script type="math/tex; mode=display">
\frac{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}-x\_{approx}^{(i)}||^2
}{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}||^2
}
=
1-\frac{\sum\_{i=1}^k S\_{ii}}{\sum\_{i=1}^n S\_{ii}}</script><hr>
<p>例如，假设差异性要满足小于$0.01$，那么可以得出：</p>
<script type="math/tex; mode=display">
\frac{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}-x\_{approx}^{(i)}||^2
}{
\frac{1}{m}\sum\_{i=1}^m||x^{(i)}||^2
}
=
1-\frac{\sum\_{i=1}^k S\_{ii}}{\sum\_{i=1}^n S\_{ii}}
\le0.01</script><p>即：</p>
<script type="math/tex; mode=display">
\frac{\sum\_{i=1}^k S\_{ii}}{\sum\_{i=1}^n S\_{ii}}
\ge0.99</script><p>那么你可以从1开始，慢慢增大$k$的值，来计算上面这个不等式，直到满足为止即可（得到满足上面不等式的最小$k$值）。</p>
<hr>
<p>通过这种方式，你只需要<strong>调用一次<code>svd</code>函数</strong>，通过<code>svd</code>给出的<code>S</code>矩阵你就可以通过依次增加$k$值的方式来求解了。这样以来就大幅的提升了计算效率。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>总结一下，使用PCA算法时寻找合适$k$值的方法：</p>
<ul>
<li>首先对协方差矩阵<code>Sigma</code>调用一次<code>svd</code>：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] &#x3D; svd(Sigma)</span><br></pre></td></tr></table></figure>
<ul>
<li>然后使用下面的不等式求得满足条件的最小$k$值：</li>
</ul>
<script type="math/tex; mode=display">
\frac{\sum\_{i=1}^k S\_{ii}}{\sum\_{i=1}^n S\_{ii}}
\ge0.99</script><hr>
<p>顺便说一下，即使你想要手动挑选$k$值，如果你想要向别人解释你实现的PCA的性能具体如何，那么一个好方法就是算出这个值：</p>
<script type="math/tex; mode=display">
\frac{\sum\_{i=1}^k S\_{ii}}{\sum\_{i=1}^n S\_{ii}}</script><p>它会告诉你百分之多少的差异性被保留了下来。</p>
<p>如果你把这个数值展现出来，那么熟悉PCA的人们就可以通过它来更好地理解你用来代表原始数据的压缩后的数据近似得有多好。因为有99%的差异性被保留了。</p>
<p>这就是一个<strong>平方投影误差的测量指标</strong>。它可以带给你对于数据压缩后是否与原始数据相似带来一种很好的直观感受。</p>
<h2 id="应用PCA的建议"><a href="#应用PCA的建议" class="headerlink" title="应用PCA的建议"></a>应用PCA的建议</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/RBqQl/advice-for-applying-pca">视频地址</a></p>
<blockquote>
<p>在之前的课程中，我已经提到过<strong>PCA</strong>有时可以用来提高机器学习算法的速度。在本节，我将讲解如何在实际操作中来实现。同时列举一些例子来说明PCA在具体应用过程中的使用建议。</p>
</blockquote>
<h3 id="PCA应用场景总结"><a href="#PCA应用场景总结" class="headerlink" title="PCA应用场景总结"></a>PCA应用场景总结</h3><p>迄今为止我们讨论过的有关PCA的应用中有如下应用场景：</p>
<ul>
<li>数据压缩<ul>
<li>减少内存或者磁盘空间的使用</li>
<li>提升学习算法的效率（k值的选择是关键）</li>
</ul>
</li>
<li>数据可视化<ul>
<li>将数据降维到二/三维度进行可视化展示</li>
</ul>
</li>
</ul>
<h3 id="通过PCA来提高学习算法的速度"><a href="#通过PCA来提高学习算法的速度" class="headerlink" title="通过PCA来提高学习算法的速度"></a>通过PCA来提高学习算法的速度</h3><p>举例说明，假如你正在用机器学习来处理图片数据。假设每张输入的图片尺寸是$100×100$的，那么对于每张图片来说，都有10000个像素点。假设样本$x^{(i)}$是包含了10000像素强度值的特征向量，即：</p>
<script type="math/tex; mode=display">
x^{(i)}\in R</script><p>那么对于我们的样本数据集来说：</p>
<script type="math/tex; mode=display">
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})</script><p>每个样本中，对应的$x^{(i)}$都是10000维的特征向量。</p>
<p>可想而知，这么高维度的数据带入到逻辑回归、神经网络、支持向量机或者任何别的算法中，学习算法运行的都会很慢。</p>
<p>幸运的是，通过使用PCA，我们能够<strong>降低数据的维数，从而使得算法能够更加高效地运行</strong>。这就是PCA提高算法运算效率的原理。</p>
<h4 id="降维步骤"><a href="#降维步骤" class="headerlink" title="降维步骤"></a>降维步骤</h4><p>首先我们需要检查带标签的训练数据集，并提取出输入数据。我们只需要提取出$x$并暂时把$y$放在一边。这一步我们会得到一组无标签的训练集：</p>
<script type="math/tex; mode=display">
x^{(1)},x^{(2)},...,x^{(m)}\in R^{10000}</script><p>从$x^{(1)}$到$x^{(m)}$，每个样本都是10000维的数据。然后我们应用PCA降维，我们会得到一个降维后的1000维的数据集：</p>
<script type="math/tex; mode=display">
z^{(1)},z^{(2)},...,z^{(m)}\in R^{1000}</script><p>这样我们就得到了一个新的训练集：</p>
<script type="math/tex; mode=display">
(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),...,(z^{(m)},y^{(m)})</script><p>现在，我可以将这个已经降维的数据集输入到学习算法中，来得出假设函数，并把降维后的数据作为输入带入，做出预测。</p>
<p>以<strong>逻辑回归</strong>为例：</p>
<p>逻辑回归中，我们得到的假设函数如下：</p>
<script type="math/tex; mode=display">
h\_{\theta}(z) = \frac{1}{1+e^{-\theta^{T}z}}</script><p>我们将$z$向量作为输入带入，并得出一个预测值。</p>
<p>最后，如果你有一个新的样本$x$，那么你所要做的是将你的测试样本$x$通过同样的PCA降维之后，你会得到这个样本所对应的$z$。然后将这个$z$值带入到这个假设函数中进行预测。</p>
<blockquote>
<p><strong>注意（重要）：</strong></p>
<p>最后要注意一点，PCA定义了从$x$到$z$的对应关系，这种对应关系只可以通过在训练集上运行PCA定义出来。</p>
<p>具体来讲，这种PCA所学习出的对应关系，所做的就是计算出一系列的参数。这些参数这就是<strong>特征缩放</strong>和<strong>均值归一化</strong>以及降维矩阵$U_{reduce}$。但是对于降维矩阵$U_{reduce}$中的数据，我们需要使我们的参数唯一地适应<strong>训练集</strong>，而不是适应交叉验证或者测试集。因此我们通过在训练集中找到了降维矩阵$U_{reduce}$，我们就可以将同样的对应关系应用到其他样本中了，比如交叉验证数集样本，或者用在测试数据集中。</p>
<p>总结一下，当你在运行PCA的时候，只是在训练集那一部分来进行的，而不是在交叉验证的数据集或者测试集上运行。在训练集上运行PCA后，得到了从$x$到$z$的映射，然后你就可以将这个映射应用到交叉验证数据集，和测试数据集中。</p>
</blockquote>
<p>通过这个例子中的这种方式，我们讨论了将数据从上万维降到千维。在实际应用场景中，我们经常发现，将数据降维到原有维度的五分之一或者十分之一，就分类的精确度而言，降维后的数据对学习算法几乎没有什么影响。如果我们将降维用在低维数据上，我们的学习算法会运行得更快。</p>
<h3 id="PCA的错误使用"><a href="#PCA的错误使用" class="headerlink" title="PCA的错误使用"></a>PCA的错误使用</h3><p>有一个值得提醒的频繁被误用的PCA应用场景，那就是使用它来避免过拟合。</p>
<p>具体原因是将高维度数据降维处理后，相较于原先的数据，会更不容易出现过拟合的现象。例如我们将10000维的数据降到了1000维，那么降维后的1000维数据相较于降维前的10000维数据更不容易产生过拟合。</p>
<p>因此有人认为PCA是一种避免过拟合的方法，但在这里，我需要强调一下，<strong>为了解决过拟合问题而使用PCA是不适合的！并且我不建议这么做。</strong></p>
<p>如果你比较担心过拟合问题，那么你应该使用正则化方法，而不是使用PCA来对数据进行降维。</p>
<blockquote>
<p><strong>PCA会丢失信息：</strong>如果你仔细想想PCA的工作原理，你会发现它并不需要使用数据的标签，你只需要设定好输入数据$x^{(i)}$，同时使用这个方法来寻找更低维度的数据近似，在这个过程中，PCA实际上已经把某些信息舍弃掉了。</p>
</blockquote>
<p>舍弃掉一些数据，并在你对数据标签$y$值毫不知情的情况下对数据进行降维，所以这或许是一个使用PCA方法的可行之路。如果保留99%的方差，即保留绝大部分的方差，那也是舍弃了某些有用的信息。事实证明，当你在保留99%或者95%或者其它百分比的方差时，结果表明只使用正则化对于避免过拟合，会带来比较好的效果。</p>
<p>同时对于过拟合问题，正则化效果也会比PCA更好，因为当你使用线性回归或者逻辑回归或其他的方法配合正则化时，这个最小化问题实际就变成了y值是什么，才不至于将有用的信息舍弃掉。然而PCA不需要使用到这些标签，它更容易将有价值信息舍弃。</p>
<p>总之，<strong>使用PCA的目的是加速学习算法，但不应该用它来避免过拟合</strong>。</p>
<h3 id="两个建议"><a href="#两个建议" class="headerlink" title="两个建议"></a>两个建议</h3><h4 id="真的需要PCA吗？"><a href="#真的需要PCA吗？" class="headerlink" title="真的需要PCA吗？"></a>真的需要PCA吗？</h4><p>有时候人们正在设计机器学习系统，或许会写下像这样的计划：</p>
<p><strong>设计一个机器学习系统：</strong></p>
<ul>
<li>收集训练数据集$｛(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})｝$</li>
<li>运行PCA将数据$x^{(i)}$降维到$z^{(i)}$</li>
<li>对降维后的数据训练逻辑回归算法$｛(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),…,(z^{(m)},y^{(m)})｝$</li>
<li>在测试集上测试结果：将$x_{test}^{(i)}$映射到$z_{test}^{(i)}$。对映射后的数据集$｛(z_{test}^{(1)},y_{test}^{(1)}),…,(z_{test}^{(m)},y_{test}^{(m)})｝$运行假设函数$h_{\theta}(z)$</li>
</ul>
<p>通常在一个项目的初期，有些人便直接写出这样的项目计划。</p>
<p>在写下这样一个使用PCA方法的项目计划前，一个非常好的问题是：<strong>如果我们在整个项目中不使用PCA效果会怎样？</strong> </p>
<p>通常人们不会去思考这个问题，尤其是当人们提出一个复杂的项目并且其中使用了PCA或其它方法时，我经常建议大家在使用PCA之前，首先要想清楚你自己做的是什么，以及你想要做什么。这也是你首先需要在原始数据$x^{(i)}$上考虑的问题。并且根据具体情况来分析是否适合使用PCA，还是直接将原始数据带入到学习算法中。</p>
<h4 id="不要一开始就带入PCA"><a href="#不要一开始就带入PCA" class="headerlink" title="不要一开始就带入PCA"></a>不要一开始就带入PCA</h4><p>同时我也建议一开始不要将PCA方法就直接放到算法里，先使用原始数据$x^{(i)}$看看效果。只有一个原因让我们相信算法出现了问题，那就是你的学习算法收敛地非常缓慢，占用内存或者硬盘空间非常大，所以你想来压缩数据。只有当你的$x^{(i)}$效果不好的时候，那么就考虑用PCA来进行压缩数据。</p>
<p>因为我常常看到某些人在项目开始时便将PCA考虑进去，有时他们并没有仔细思考他们做了什么使得结果表现地好，更没有考虑在不用PCA下的情景会是什么样的效果。如果某个数据不使用PCA也可以工作的很好，但我们对于这些数据使用PCA耗费了大量时间，这是不值得的。</p>
<p>然而，尽管有这些需要注意的地方，PCA仍旧是一种不可思议的有用的算法。PCA的使用频率也很高，大部分时候我都用它来加快学习算法。但我认为PCA通常都是被用来压缩数据以减少内存使用或硬盘空间占用的、或者用来可视化数据的。</p>
<p>同时PCA也是一种强有力的无监督学习算法。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="DannyLee"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">DannyLee</p>
  <div class="site-description" itemprop="description">愿你的努力终取得成果</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">136</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
