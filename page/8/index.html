<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="愿你的努力终取得成果">
<meta property="og:type" content="website">
<meta property="og:title" content="圣巢">
<meta property="og:url" content="http://example.com/page/8/index.html">
<meta property="og:site_name" content="圣巢">
<meta property="og:description" content="愿你的努力终取得成果">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="DannyLee">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>圣巢</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8d12c5d1bc83189640335b2363468a74";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">圣巢</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/12/08/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%EF%BC%882%EF%BC%89-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/12/08/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%EF%BC%882%EF%BC%89-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96/" class="post-title-link" itemprop="url">房价预测（2）-数据清洗与持久化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-12-08 23:05:58" itemprop="dateCreated datePublished" datetime="2016-12-08T23:05:58+00:00">2016-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>在<a href="/2016/11/30/房价预测（1）-搜房网数据爬取/">房价预测（1）-搜房网数据爬取</a>中，已经介绍了爬虫的核心代码，但是爬取到的数据是没有经过加工且带有html标签的，很不利于阅读和使用，并且我们没有把数据持久化到本地，那么这篇文章主要介绍的就是这两步工作。</p>
</blockquote>
<h2 id="Item-Pipeline介绍"><a href="#Item-Pipeline介绍" class="headerlink" title="Item Pipeline介绍"></a>Item Pipeline介绍</h2><p>Scrapy中有个很好用的工具<a target="_blank" rel="noopener" href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html">Item Pipeline</a>。</p>
<p>通过阅读文档我们可以看到它的作用是：</p>
<ul>
<li>清理HTML数据</li>
<li>验证爬取的数据(检查item包含某些字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<p>真是完全符合我们的要求。</p>
<blockquote>
<p><strong>Pipeline</strong>的意思是<strong>管道</strong>,如果你玩过Linux的话，对这个名称一定不会陌生。管道是计算机中一个非常常见且重要的概念，大致的作用就是<strong>将管道前一部分的输出结果作为它后一部分的输入</strong>。类似这种设计都可以称为<strong>管道</strong>。</p>
</blockquote>
<p>还记得我们之前写的蜘蛛吗？在<code>ESFListSpider</code>这个蜘蛛中，开始爬取数据时会走到这个方法中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 爬取房源列表信息</span><br><span class="line">def parse(self, response):</span><br><span class="line">        self.log(&#39;A response from %s just arrived!&#39; % response.url)</span><br><span class="line">        infos &#x3D; response.xpath(&quot;&#x2F;&#x2F;a&#x2F;@href&quot;).extract()</span><br><span class="line"></span><br><span class="line">        for i in infos:</span><br><span class="line">            i_str &#x3D; str(i).encode(&quot;utf-8&quot;)</span><br><span class="line">            if &quot;esf&quot; in i_str:</span><br><span class="line">                url &#x3D; i_str.replace(&#39;\\&#39;, &#39;&#39;).strip()</span><br><span class="line">                yield scrapy.Request(url&#x3D;url.replace(&quot;\&quot;&quot;, &quot;&quot;), callback&#x3D;self.parse_details)</span><br></pre></td></tr></table></figure>
<p>在这个方法的内部的循环中会遍历链接地址，再次发起request，之后会自动调用回调函数<code>self.parse_details</code>，自动执行<code>parse_details</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 爬取房源详情信息</span><br><span class="line">def parse_details(self, response):</span><br><span class="line">	...</span><br><span class="line">	# 顺便爬取一下详情页 url</span><br><span class="line">	item &#x3D; ESFItem()</span><br><span class="line">    item[&#39;url&#39;] &#x3D; response.url</span><br><span class="line">    </span><br><span class="line">    # 详情页对应的 城市-一级区域-二级区域 信息</span><br><span class="line">	bread_list &#x3D; response.xpath(&quot;&#x2F;&#x2F;body&#x2F;div[@class&#x3D;&#39;wrap&#39;]&#x2F;div[@class&#x3D;&#39;bread&#39;]&#x2F;p[@class&#x3D;&#39;floatl&#39;]&#x2F;a&quot;).extract()</span><br><span class="line">    for index,bread in enumerate(bread_list[1:]):</span><br><span class="line">        if index &#x3D;&#x3D; 0:</span><br><span class="line">            item[&quot;bread_city&quot;] &#x3D; bread</span><br><span class="line">        elif index &#x3D;&#x3D; 1:</span><br><span class="line">            item[&quot;bread_area&quot;] &#x3D; bread</span><br><span class="line">        elif index &#x3D;&#x3D; 2:</span><br><span class="line">            item[&quot;bread_positon&quot;] &#x3D; bread</span><br><span class="line">	...</span><br><span class="line">	# 这句很重要，保证item可以传递到后面的Pipeline中</span><br><span class="line">	yield item</span><br></pre></td></tr></table></figure>
<p>以上是我们蜘蛛的部分核心代码，如果我们配置了Pipeline，蜘蛛会把塞满信息的item传递到我们定义的Pipeline中。</p>
<h3 id="创建Pipeline"><a href="#创建Pipeline" class="headerlink" title="创建Pipeline"></a>创建Pipeline</h3><p>为了满足<strong>清洗数据</strong>和<strong>存储数据</strong>的功能，我们创建两个Pipeline来分别处理这两类逻辑：</p>
<p>在<code>pipelines.py</code>中添加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class ExtractDataPipeline(object):</span><br><span class="line">	def process_item(self, item, spider):</span><br><span class="line">		...</span><br><span class="line">		return item</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class SaveDataPipline(object):</span><br><span class="line">	def process_item(self, item, spider):</span><br><span class="line">		...</span><br><span class="line">		return item</span><br></pre></td></tr></table></figure>
<p>并在<code>settings.py</code>中进行注册：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">ITEM_PIPELINES &#x3D; &#123;</span><br><span class="line">   &#39;soufang.pipelines.ExtractDataPipeline&#39;: 300,</span><br><span class="line">   &#39;soufang.pipelines.SaveDataPipline&#39;:500,</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>后面的数字范围是0-1000之内的任意整数，代表的是优先级。可以看出来我们是优先清洗数据，然后保存数据。</p>
<h2 id="清洗数据"><a href="#清洗数据" class="headerlink" title="清洗数据"></a>清洗数据</h2><p>清洗数据的代码如下，就是将每一个字段的信息的html标签进行剥离，并且截取掉冗余的字符串：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"># 剔除html代码</span><br><span class="line">def take_out_html(str):</span><br><span class="line">    dr &#x3D; re.compile(r&#39;&lt;[^&gt;]+&gt;&#39;, re.S)</span><br><span class="line">    return dr.sub(&#39;&#39;, str)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ExtractDataPipeline(object):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        print &quot;&gt;&gt;&gt; ExtractDataPipeline &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;</span><br><span class="line">        self.extract_data(item, &quot;url&quot;)</span><br><span class="line">        self.extract_data(item, &quot;id&quot;, f&#x3D;5)</span><br><span class="line">        self.extract_data(item, &quot;publish_time&quot;, f&#x3D;-10)</span><br><span class="line">        self.extract_data(item, &quot;title&quot;)</span><br><span class="line">        self.extract_data(item, &quot;total_price&quot;)</span><br><span class="line">        self.extract_data(item, &quot;house_type&quot;, f&#x3D;3)</span><br><span class="line">        self.extract_data(item, &quot;house_build_area&quot;, f&#x3D;5)</span><br><span class="line">        self.extract_data(item, &quot;house_use_area&quot;, f&#x3D;5)</span><br><span class="line">        self.extract_data(item, &quot;house_age&quot;, f&#x3D;3)</span><br><span class="line">        self.extract_data(item, &quot;orientation&quot;, f&#x3D;3)</span><br><span class="line">        self.extract_data(item, &quot;floor&quot;, f&#x3D;3)</span><br><span class="line">        self.extract_data(item, &quot;structure&quot;, f&#x3D;3)</span><br><span class="line">        self.extract_data(item, &quot;decoration&quot;, f&#x3D;3)</span><br><span class="line">        self.extract_data(item, &quot;residential_category&quot;, f&#x3D;5)</span><br><span class="line">        self.extract_data(item, &quot;building_class&quot;, f&#x3D;5)</span><br><span class="line">        self.extract_data(item, &quot;property_right&quot;, f&#x3D;5)</span><br><span class="line">        self.extract_data(item, &quot;property_name&quot;)</span><br><span class="line">        self.extract_data(item, &quot;school&quot;)</span><br><span class="line">        self.extract_data(item, &quot;supporting_facilities&quot;)</span><br><span class="line">        self.extract_data(item, &quot;bread_city&quot;, t&#x3D;-3)</span><br><span class="line">        self.extract_data(item, &quot;bread_area&quot;, t&#x3D;-3)</span><br><span class="line">        self.extract_data(item, &quot;bread_positon&quot;, t&#x3D;-3)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    # 剔除数据中的多余部分</span><br><span class="line">    def extract_data(self, item, key_bread_positon, f&#x3D;None, t&#x3D;None):</span><br><span class="line">        if self.check_key_exist(item, key_bread_positon):</span><br><span class="line">            item[key_bread_positon] &#x3D; take_out_html(item[key_bread_positon]).strip()[f:t]</span><br><span class="line">        else:</span><br><span class="line">            item[key_bread_positon] &#x3D; &quot;&quot;</span><br><span class="line">        self.print_itme_value(item, key_bread_positon)</span><br><span class="line"></span><br><span class="line">    # 检查key是否存在</span><br><span class="line">    def check_key_exist(self, item, key):</span><br><span class="line">        return key in item.keys()</span><br><span class="line"></span><br><span class="line">    # 输出数据</span><br><span class="line">    def print_itme_value(self, item, key):</span><br><span class="line">        print key + &quot; &gt;&gt;&gt; &quot; + item[key]</span><br></pre></td></tr></table></figure>
<h2 id="保存数据到MongoDB"><a href="#保存数据到MongoDB" class="headerlink" title="保存数据到MongoDB"></a>保存数据到MongoDB</h2><p>保存数据我们使用<a target="_blank" rel="noopener" href="http://www.mongodb.org/">MongoDB</a>，这是一个很简单易用且功能强大的<strong>非关系型数据库</strong>，它可以把数据保存成一种类似Json的格式。</p>
<p>使用教程网上很容易搜到，推荐一个自学网站：<a target="_blank" rel="noopener" href="http://www.runoob.com/mongodb/mongodb-tutorial.html">MongoDB教程</a></p>
<p>mongo的可视化工具也有很多，我使用的是<a target="_blank" rel="noopener" href="https://robomongo.org/">Robomongo</a>。</p>
<p>使用mongo我们需要在<code>settings.py</code>中增添下面的配置信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">MONGO_URI &#x3D; &quot;mongodb:&#x2F;&#x2F;localhost:27017&quot;;</span><br><span class="line">MONGO_DATABASE &#x3D; &quot;soufang&quot;;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>下面是<code>SaveDataPipline</code>的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class SaveDataPipline(object):</span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri &#x3D; mongo_uri</span><br><span class="line">        self.mongo_db &#x3D; mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri&#x3D;crawler.settings.get(&#39;MONGO_URI&#39;),</span><br><span class="line">            mongo_db&#x3D;crawler.settings.get(&#39;MONGO_DATABASE&#39;, &#39;items&#39;)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client &#x3D; pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db &#x3D; self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        print &quot;&gt;&gt;&gt; SaveDataPipline &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;</span><br><span class="line">        collection_name &#x3D; self.mongo_db</span><br><span class="line">        self.db[collection_name].insert(dict(item))</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure>
<h3 id="去除重复项"><a href="#去除重复项" class="headerlink" title="去除重复项"></a>去除重复项</h3><p>为了保证id重复的房源信息不再重复爬取，我们可以对数据库建立<strong>唯一索引</strong>，这样既能够提高查询效率，又能够去除重复数据。</p>
<p>后台建立唯一索引：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.soufang.ensureIndex(&#123;&quot;id&quot;:1&#125;,&#123;&quot;unique&quot;:true&#125;,&#123;&quot;background&quot;:true&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="开始爬取！"><a href="#开始爬取！" class="headerlink" title="开始爬取！"></a>开始爬取！</h2><p>终于可以开心的爬取数据啦~</p>
<p>命令行输入<code>mongod</code>开启mongo服务后，进入我们的爬虫项目，开启我们的爬虫：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl esflist</span><br></pre></td></tr></table></figure>
<p><img src="/img/16_12_07/001.gif" alt=""></p>
<p>在Robomongo中我们执行一条查询语句<code>db.soufang.find()</code>就可以看到我们爬取到的全部数据：</p>
<p><img src="/img/16_12_07/002.png" alt=""></p>
<p>执行<code>db.soufang.findOne()</code>可以查询到一条记录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&gt; db.soufang.findOne()</span><br><span class="line">&#123;</span><br><span class="line">	&quot;_id&quot; : ObjectId(&quot;58497108f26cd3d1ac97004b&quot;),</span><br><span class="line">	&quot;orientation&quot; : &quot;&quot;,</span><br><span class="line">	&quot;bread_area&quot; : &quot;普陀&quot;,</span><br><span class="line">	&quot;publish_time&quot; : &quot;2016-10-20&quot;,</span><br><span class="line">	&quot;id&quot; : &quot;268151561&quot;,</span><br><span class="line">	&quot;property_name&quot; : &quot;甘泉一村&quot;,</span><br><span class="line">	&quot;title&quot; : &quot;甘泉一村 边套全明户型 黄金3楼 得房率高 配套成熟 便利&quot;,</span><br><span class="line">	&quot;building_class&quot; : &quot;板楼&quot;,</span><br><span class="line">	&quot;bread_positon&quot; : &quot;甘泉&quot;,</span><br><span class="line">	&quot;house_build_area&quot; : &quot;55.27㎡&quot;,</span><br><span class="line">	&quot;bread_city&quot; : &quot;上海&quot;,</span><br><span class="line">	&quot;house_use_area&quot; : &quot;&quot;,</span><br><span class="line">	&quot;house_type&quot; : &quot;2室1厅1厨1卫&quot;,</span><br><span class="line">	&quot;structure&quot; : &quot;平层&quot;,</span><br><span class="line">	&quot;decoration&quot; : &quot;简装修&quot;,</span><br><span class="line">	&quot;school&quot; : &quot;&quot;,</span><br><span class="line">	&quot;total_price&quot; : &quot;275&quot;,</span><br><span class="line">	&quot;url&quot; : &quot;http:&#x2F;&#x2F;esf.sh.fang.com&#x2F;chushou&#x2F;3_268151561.htm&quot;,</span><br><span class="line">	&quot;house_age&quot; : &quot;1987年&quot;,</span><br><span class="line">	&quot;floor&quot; : &quot;中层(共6层)&quot;,</span><br><span class="line">	&quot;property_right&quot; : &quot;个人产权&quot;,</span><br><span class="line">	&quot;supporting_facilities&quot; : &quot;&quot;,</span><br><span class="line">	&quot;residential_category&quot; : &quot;普通住宅&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>全上海目前在售的二手房源大概有60到70万套之间，我们可以把速度缩短到1秒中爬取一条，如果24小时爬取的话，预计6天可以爬完全上海。不过我们的目的是对数据进行分析，没必要盲目的采集大量的数据，只要数据够用即可。</p>
<p>如果数据量过大，后面进行训练机器性能也跟不上，所以我准备将总数据量控制在5万条（按照各区域房源数量占比进行组合）。</p>
<h2 id="what-s-next"><a href="#what-s-next" class="headerlink" title="what`s next?"></a>what`s next?</h2><p>接下来我们将构建一个可视化的界面来直观的观察我们爬取到的房源数据。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/12/03/%E5%8F%91%E5%B8%83aar%E5%88%B0jcenter%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/12/03/%E5%8F%91%E5%B8%83aar%E5%88%B0jcenter%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF/" class="post-title-link" itemprop="url">发布aar到jcenter的正确姿势</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-12-03 07:21:58" itemprop="dateCreated datePublished" datetime="2016-12-03T07:21:58+00:00">2016-12-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Android/" itemprop="url" rel="index"><span itemprop="name">Android</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>前段时间开发了一个android测试小插件<a target="_blank" rel="noopener" href="https://github.com/DannyLee1991/ATestKit">ATestKit</a>，准备发布到jcenter库中，可期间碰壁无数，折腾了两天最终终于上传成功。</p>
<p>下面是我的最终整理：</p>
<h2 id="上传aar到jcenter的正确姿势"><a href="#上传aar到jcenter的正确姿势" class="headerlink" title="上传aar到jcenter的正确姿势"></a>上传aar到jcenter的正确姿势</h2><p>打开<a target="_blank" rel="noopener" href="https://bintray.com">https://bintray.com</a>。</p>
<h3 id="第一坑：你需要一个vpn。"><a href="#第一坑：你需要一个vpn。" class="headerlink" title="第一坑：你需要一个vpn。"></a>第一坑：你需要一个vpn。</h3><p>注册时国内的qq邮箱，163邮箱通通用不了，gmail可用。</p>
<p>天朝国情，访问google需要翻墙，不解释。</p>
<h3 id="第二坑：注意注册入口"><a href="#第二坑：注意注册入口" class="headerlink" title="第二坑：注意注册入口"></a>第二坑：注意注册入口</h3><p>首页有一个<strong>START YOUR FREE TRIAL</strong>:</p>
<p><img src="/img/16_12_03/001.png" alt=""></p>
<p>如果你按照网上搜到排名很靠前的的其他教程来进行，比如说这几篇：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/wwj_748/article/details/51913280">Android Library上传到JCenter仓库实践</a></li>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/lmj623565791/article/details/51148825">Android 快速发布开源项目到jcenter</a></li>
<li><a target="_blank" rel="noopener" href="http://www.cnblogs.com/qianxudetianxia/p/4322331.html">使用Gradle发布aar项目到JCenter仓库</a></li>
<li><a target="_blank" rel="noopener" href="http://www.jianshu.com/p/31410d71eaba">Android Studio提交库至Bintray jCenter从入门到放弃</a></li>
</ul>
<p>那么恭喜你，你成功的入坑了，你会发现按照他们的配置一步步进行，但到最后就是不成功，而且错误的提示也很不明确，让你找不到一点头绪。</p>
<p>仔细看看，你看到的jcenter的个人主页似乎和他们的不太一样。</p>
<p>原因可能是因为bintray.com这个网站改版了，导致注册流程和以前不一样了，如果你点击上图的那个注册入口，会引导你创建一个组织，然后你只能在组织下新建自己的仓库，而上面的几篇文章根本就没有组织这一说啊。</p>
<p>在这里有一个关于网站首页的说明：</p>
<p><a target="_blank" rel="noopener" href="https://bintray.com/docs/usermanual/starting/starting_gettingstarted.html#_the_bintray_homepage">https://bintray.com/docs/usermanual/starting/starting_gettingstarted.html#_the_bintray_homepage</a></p>
<p>注意看这里：</p>
<p><img src="/img/16_12_03/003.png" alt=""></p>
<p>好吧，难道说明我们还有另外一个针对于open source plan的注册入口吗？这个入口听起来有点像上面几篇文章描述的那样啊。</p>
<p>果然有！</p>
<p>继续回到首页，拉到页面最底端，有另外一个入口：</p>
<p><img src="/img/16_12_03/004.png" alt=""></p>
<p>从这里注册进入，你就可以不用创建组织了。</p>
<h3 id="第三坑：收费？！"><a href="#第三坑：收费？！" class="headerlink" title="第三坑：收费？！"></a>第三坑：收费？！</h3><p>如果你不幸点击了首页的<strong>START YOUR FREE TRIAL</strong>，那么你的首页上会有这么一个奇怪的标识：</p>
<p><img src="/img/16_12_03/007.png" alt=""></p>
<p>点进来看一看，虽然没有明确说免费版到期后会怎样，但手动终止免费版后会删掉你库里所有的东东：</p>
<p><img src="/img/16_12_03/008.png" alt=""></p>
<p>如果想继续使用，那么150刀一月。</p>
<p><strong>没想到jcenter如此恶毒的把免费试用入口放在最显眼的位置，而且不明确告诉你试用账号到期后的后果，并且把免费使用的社区版入口藏的那么深。</strong></p>
<h3 id="正常配置流程"><a href="#正常配置流程" class="headerlink" title="正常配置流程"></a>正常配置流程</h3><p>通过社区版注册入口进入之后，就没有了奇怪的标识，我们可以创建一个个人仓库了：</p>
<p><img src="/img/16_12_03/009.png" alt=""></p>
<p>配置选择public，类型选择Maven：</p>
<p><img src="/img/16_12_03/010.png" alt=""></p>
<p>然后创建新的Package：</p>
<p><img src="/img/16_12_03/011.png" alt=""></p>
<p>填写相关信息：</p>
<p><img src="/img/16_12_03/012.png" alt=""></p>
<p>在你的编辑你android项目根目录下的<code>build.gradle</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">buildscript &#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        jcenter()</span><br><span class="line">    &#125;</span><br><span class="line">    dependencies &#123;</span><br><span class="line">        classpath &#39;com.android.tools.build:gradle:2.2.0&#39;</span><br><span class="line">        &#x2F;&#x2F; 添加上传到jcenter所需的插件</span><br><span class="line">        classpath &#39;com.github.dcendents:android-maven-gradle-plugin:1.5&#39;</span><br><span class="line">        classpath &#39;com.jfrog.bintray.gradle:gradle-bintray-plugin:1.7.1&#39;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">allprojects &#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        jcenter()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">task clean(type: Delete) &#123;</span><br><span class="line">    delete rootProject.buildDir</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>编辑library下的<code>build.gradle</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">apply plugin: &#39;com.android.library&#39;</span><br><span class="line">apply plugin: &#39;com.github.dcendents.android-maven&#39;</span><br><span class="line">apply plugin: &#39;com.jfrog.bintray&#39;</span><br><span class="line"></span><br><span class="line">version &#x3D; &quot;0.2&quot;	&#x2F;&#x2F;aar的版本号</span><br><span class="line"></span><br><span class="line">android &#123;</span><br><span class="line">    compileSdkVersion 23</span><br><span class="line">    buildToolsVersion &quot;23.0.3&quot;</span><br><span class="line"></span><br><span class="line">    defaultConfig &#123;</span><br><span class="line">        minSdkVersion 14</span><br><span class="line">        targetSdkVersion 23</span><br><span class="line">        versionCode 1</span><br><span class="line">        versionName &quot;1.0&quot;</span><br><span class="line"></span><br><span class="line">        testInstrumentationRunner &quot;android.support.test.runner.AndroidJUnitRunner&quot;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    buildTypes &#123;</span><br><span class="line">        release &#123;</span><br><span class="line">            minifyEnabled false</span><br><span class="line">            proguardFiles getDefaultProguardFile(&#39;proguard-android.txt&#39;), &#39;proguard-rules.pro&#39;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    lintOptions &#123;</span><br><span class="line">        abortOnError false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dependencies &#123;</span><br><span class="line">    compile fileTree(dir: &#39;libs&#39;, include: [&#39;*.jar&#39;])</span><br><span class="line">    compile &#39;com.android.support:appcompat-v7:23.4.0&#39;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def siteUrl &#x3D; &#39;https:&#x2F;&#x2F;github.com&#x2F;DannyLee1991&#x2F;ATestKit&#39;    &#x2F;&#x2F; 项目主页</span><br><span class="line">def gitUrl &#x3D; &#39;https:&#x2F;&#x2F;github.com&#x2F;DannyLee1991&#x2F;ATestKit.git&#39; &#x2F;&#x2F; 项目的git地址</span><br><span class="line">def module_name &#x3D; &#39;ATestKit&#39;	&#x2F;&#x2F; 项目的名称</span><br><span class="line">group &#x3D; &#39;com.dannylee&#39;	&#x2F;&#x2F; 所在组</span><br><span class="line"></span><br><span class="line">install &#123;</span><br><span class="line">    repositories.mavenInstaller &#123;</span><br><span class="line">        &#x2F;&#x2F; This generates POM.xml with proper parameters</span><br><span class="line">        pom &#123;</span><br><span class="line">            project &#123;</span><br><span class="line">                packaging &#39;aar&#39;</span><br><span class="line">                name &#39;ATestKit&#39; &#x2F;&#x2F; 名称</span><br><span class="line">                url siteUrl</span><br><span class="line">                licenses &#123;</span><br><span class="line">                    license &#123;</span><br><span class="line">                        name &#39;The Apache Software License, Version 2.0&#39; &#x2F;&#x2F; 开源协议名称</span><br><span class="line">                        url &#39;http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0.txt&#39; &#x2F;&#x2F; 协议地址</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                developers &#123;</span><br><span class="line">                    developer &#123;</span><br><span class="line">                        id &#39;dannylee&#39;	&#x2F;&#x2F; 账号</span><br><span class="line">                        name &#39;dannylee&#39;	&#x2F;&#x2F; 名称</span><br><span class="line">                        email &#39;leejianan1@gmail.com&#39; &#x2F;&#x2F; 邮箱地址</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                scm &#123;</span><br><span class="line">                    connection gitUrl</span><br><span class="line">                    developerConnection gitUrl</span><br><span class="line">                    url siteUrl</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">task sourcesJar(type: Jar) &#123;</span><br><span class="line">    from android.sourceSets.main.java.srcDirs</span><br><span class="line">    classifier &#x3D; &#39;sources&#39;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">task javadoc(type: Javadoc) &#123;</span><br><span class="line">    source &#x3D; android.sourceSets.main.java.srcDirs</span><br><span class="line">    classpath +&#x3D; project.files(android.getBootClasspath().join(File.pathSeparator))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">task javadocJar(type: Jar, dependsOn: javadoc) &#123;</span><br><span class="line">    classifier &#x3D; &#39;javadoc&#39;</span><br><span class="line">    from javadoc.destinationDir</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">artifacts &#123;</span><br><span class="line">    archives sourcesJar</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Properties properties &#x3D; new Properties()</span><br><span class="line">properties.load(project.rootProject.file(&#39;local.properties&#39;).newDataInputStream())</span><br><span class="line">bintray &#123;</span><br><span class="line">	 &#x2F;&#x2F; 读取配置文件中的用户名和key</span><br><span class="line">    user &#x3D; properties.getProperty(&quot;bintray.user&quot;)</span><br><span class="line">    key &#x3D; properties.getProperty(&quot;bintray.apikey&quot;)</span><br><span class="line">    configurations &#x3D; [&#39;archives&#39;]</span><br><span class="line">    pkg &#123;</span><br><span class="line">        repo &#x3D; &quot;maven&quot;		&#x2F;&#x2F; 你在bintray上创建的库的名称</span><br><span class="line">        name &#x3D; module_name               &#x2F;&#x2F; 在jcenter中的项目名称</span><br><span class="line">        websiteUrl &#x3D; siteUrl</span><br><span class="line">        vcsUrl &#x3D; gitUrl</span><br><span class="line">        licenses &#x3D; [&quot;Apache-2.0&quot;]</span><br><span class="line">        publish &#x3D; true</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>回到bintray，在你的个人信息配置页中，查看APK Key：</p>
<p><img src="/img/16_12_03/013.png" alt=""></p>
<p><img src="/img/16_12_03/014.png" alt=""></p>
<p>在项目根目录下创建<code>local.properties</code>配置文件（如果有就直接打开），写入账号信息：</p>
<p><img src="/img/16_12_03/015.png" alt=""></p>
<p>由于key是比较重要的信息，不能泄露出去，所以需要编辑<code>.gitignore</code>添加过滤规则来把它过滤掉：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Local configuration file (sdk path, etc)</span><br><span class="line">local.properties</span><br></pre></td></tr></table></figure>
<p>然后命令行进入到你的library项目中执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradle build</span><br></pre></td></tr></table></figure>
<p>比较坑的是有的时候可能会失败，多试几次就会成功。</p>
<p>成功之后执行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradle bintrayUpload</span><br></pre></td></tr></table></figure>
<p>如果不出意外的话，会成功上传，如果失败了，很有可能是你的网络问题，换个vpn试试说不定可以成功。如果是其他原因的失败，请自行google。</p>
<p>回到jcenter中项目的管理页面，就可以看到上传的版本信息了，点击<strong>Add to JCenter</strong>就可以正式上传到jcenter中了：</p>
<p><img src="/img/16_12_03/016.png" alt=""></p>
<p>上传后，需要系统审核大概半小时，之后就可以在你的项目中使用这个aar了：</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">    ...</span><br><span class="line">    compile &#39;com.dannylee:atestkit:0.2&#39;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></h2><p>如果你选择的是第一个注册入口，流程也是一样的，唯一不同的就是在library中的<code>build.gradle</code>中要填写组织名称，否则也找不到:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">install &#123;</span><br><span class="line">    repositories.mavenInstaller &#123;</span><br><span class="line">        &#x2F;&#x2F; This generates POM.xml with proper parameters</span><br><span class="line">        pom &#123;</span><br><span class="line">            project &#123;</span><br><span class="line">            	   userOrg &#39;your organisation name&#39; &#x2F;&#x2F; 填写组织名称</span><br><span class="line">                ...</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                ...</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/11/30/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%EF%BC%881%EF%BC%89-%E6%90%9C%E6%88%BF%E7%BD%91%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/11/30/%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%EF%BC%881%EF%BC%89-%E6%90%9C%E6%88%BF%E7%BD%91%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/" class="post-title-link" itemprop="url">房价预测（1）-搜房网数据爬取</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-11-30 22:21:58" itemprop="dateCreated datePublished" datetime="2016-11-30T22:21:58+00:00">2016-11-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="搜房网数据爬取"><a href="#搜房网数据爬取" class="headerlink" title="搜房网数据爬取"></a>搜房网数据爬取</h2><blockquote>
<p>如今<strong>房事</strong>牵动着每家每户的心，所以以搜房网数据为例，用程序员的角度去客观分析一下房价走势，并对房价数据进行预测，也顺便对学到的<strong>机器学习</strong>相关知识练练手。</p>
</blockquote>
<h3 id="爬虫Scrapy"><a href="#爬虫Scrapy" class="headerlink" title="爬虫Scrapy"></a>爬虫Scrapy</h3><p>数据分析的第一步，当然是数据的采集。这里我使用的是<a target="_blank" rel="noopener" href="http://scrapy-chs.readthedocs.io/zh_CN/latest/">Scrpy</a>。这是一个开源的python爬虫框架，可以很方便的处理很多爬虫方面的工作，而且中文的文档写的也比较明确。</p>
<p>安装好python环境后，通过pip即可安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Scrapy</span><br></pre></td></tr></table></figure>
<p>具体使用方式，可以看一下这个<a target="_blank" rel="noopener" href="http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html">Scrapy入门教程</a>，也不难，这里就不做过多的介绍了。</p>
<h3 id="fang-com从何处入手呢？"><a href="#fang-com从何处入手呢？" class="headerlink" title="fang.com从何处入手呢？"></a>fang.com从何处入手呢？</h3><p>大致了解了Scrapy的用法之后，我们就可以实际操作一把了。那么我们面对搜房网这个庞然大物到底要从何处开始爬呢？</p>
<p>首先，先声明一下我的目的：目的是分析房价数据，所以要爬取的数据主要是每个房源信息的数据咯。</p>
<p>想要拿到每条房源信息的url，通常我们是拿到每个列表页的数据。</p>
<p>那么我们第一个要爬取的数据就是房源列表信息了。</p>
<h4 id="列表爬取"><a href="#列表爬取" class="headerlink" title="列表爬取"></a>列表爬取</h4><p>所谓列表页，就是这个页面：<a target="_blank" rel="noopener" href="http://esf.sh.fang.com/housing/">上海二手房</a>，下面“全部小区”部分的列表信息就是我们想要的。</p>
<p>通过观察搜房网，我发现还有另外一个列表页入口，看起来似乎更容易爬取一些，就是：<a target="_blank" rel="noopener" href="http://esf.sh.fang.com/map/">地图页</a>。</p>
<p>在这个页面左侧有个列表：</p>
<p><img src="/img/16_11_30/001.png" alt=""></p>
<p>进一步点击这个列表，最后我们可以得到这个页面：</p>
<p><img src="/img/16_11_30/002.png" alt=""></p>
<p>通过抓包工具，我们可以看到这个列表通过Ajax发送get请求，得到一段json数据：</p>
<p><img src="/img/16_11_30/003.png" alt=""></p>
<p>我们真正想要的数据在这段json中最后的<code>item</code>中：</p>
<p><img src="/img/16_11_30/004.png" alt=""></p>
<p>这里藏的就是这个列表的html代码。</p>
<p>回过头来，再来看看我们请求的地址：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;esf.sh.fang.com&#x2F;map&#x2F;?mapmode&#x3D;y&amp;district&#x3D;996&amp;subwayline&#x3D;&amp;subwaystation&#x3D;&amp;price&#x3D;&amp;room&#x3D;&amp;area&#x3D;&amp;towards&#x3D;&amp;floor&#x3D;&amp;hage&#x3D;&amp;equipment&#x3D;&amp;keyword&#x3D;&amp;comarea&#x3D;21929&amp;orderby&#x3D;30&amp;isyouhui&#x3D;&amp;x1&#x3D;120.798&amp;y1&#x3D;30.926054&amp;x2&#x3D;122.177798&amp;y2&#x3D;31.571157&amp;newCode&#x3D;&amp;houseNum&#x3D;&amp;schoolDist&#x3D;&amp;schoolid&#x3D;&amp;ecshop&#x3D;ecshophouse&amp;PageNo&#x3D;1&amp;zoom&#x3D;16&amp;a&#x3D;ajaxSearch&amp;city&#x3D;sh&amp;searchtype&#x3D;loupan</span><br></pre></td></tr></table></figure>
<p>这里需要凭借直觉来剔除掉一些多余的参数的，至少满足请求到的数据不是局限于<strong>崇明区</strong>的，而是全上海的。</p>
<p>经过一些尝试之后，我发现下面这个url可以请求到全部的列表数据，其中<code>PageNo</code>参数是页数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;esf.sh.fang.com&#x2F;map&#x2F;?mapmode&#x3D;y&amp;orderby&#x3D;30&amp;ecshop&#x3D;ecshophouse&amp;PageNo&#x3D;2&amp;a&#x3D;ajaxSearch&amp;city&#x3D;sh&amp;searchtype&#x3D;loupan</span><br></pre></td></tr></table></figure>
<p>我们可以先通过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http:&#x2F;&#x2F;esf.sh.fang.com&#x2F;map&#x2F;?mapmode&#x3D;y&amp;orderby&#x3D;30&amp;ecshop&#x3D;ecshophouse&amp;PageNo&#x3D;2&amp;a&#x3D;ajaxSearch&amp;city&#x3D;sh&amp;searchtype&#x3D;loupan&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>来找出房源详情页的url。</p>
<p>最终，我发现通过<code>response.xpath(&quot;//a/@href&quot;).extract()</code>可以提取出所有的链接地址，其中如果url中包含<code>&quot;esf&quot;</code>字符串，是二手房房源的url地址。所以<code>parse()</code>的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line"></span><br><span class="line">        infos &#x3D; response.xpath(&quot;&#x2F;&#x2F;a&#x2F;@href&quot;).extract()</span><br><span class="line"></span><br><span class="line">        for i in infos:</span><br><span class="line">            i_str &#x3D; str(i).encode(&quot;utf-8&quot;)</span><br><span class="line">            if &quot;esf&quot; in i_str:</span><br><span class="line">                url &#x3D; i_str.replace(&#39;\\&#39;, &#39;&#39;).strip()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这样我们就得到了所有二手房房源详情页面的url了。</p>
<blockquote>
<p>这里有一个小坑：</p>
<p>这一行代码<code>i_str = str(i).encode(&quot;utf-8&quot;)</code>中，由于i默认没有解码成utf-8的格式，所以不能直接和字符串<code>&quot;esf&quot;</code>进行运算，所以需要一部encode操作。</p>
</blockquote>
<h4 id="详情页爬取"><a href="#详情页爬取" class="headerlink" title="详情页爬取"></a>详情页爬取</h4><p>对于详情页，我主要想爬取的数据如下：</p>
<p><img src="/img/16_11_30/005.png" alt=""></p>
<p>这个页面似乎没有什么取巧的方式了，只能硬着头皮去看他的源码，一层层的抽取了。下面是详情页解析的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">def parse_details(self, response):</span><br><span class="line"></span><br><span class="line">        # path</span><br><span class="line">        xpath &#x3D; &quot;&#x2F;&#x2F;body&#x2F;div[@class&#x3D;&#39;wrap&#39;]&#x2F;div[@class&#x3D;&#39;main clearfix&#39;]&#x2F;div[@class&#x3D;&#39;mainBoxL&#39;]&quot;</span><br><span class="line">        div_title &#x3D; &quot;&#x2F;div[@class&#x3D;&#39;title&#39;]&quot;</span><br><span class="line">        p_gray9 &#x3D; &quot;&#x2F;p[@class&#x3D;&#39;gray9&#39;]&quot;</span><br><span class="line">        h1 &#x3D; &quot;&#x2F;h1&quot;</span><br><span class="line">        span_mr10 &#x3D; &quot;&#x2F;span[@class&#x3D;&#39;mr10&#39;]&quot;</span><br><span class="line">        div_houseInfor_clearfix &#x3D; &quot;&#x2F;div[@class&#x3D;&#39;houseInfor clearfix&#39;]&quot;</span><br><span class="line">        div_inforTxt &#x3D; &quot;&#x2F;div[@class&#x3D;&#39;inforTxt&#39;]&quot;</span><br><span class="line">        dl &#x3D; &quot;&#x2F;dl&quot;</span><br><span class="line">        dt_gray6_zongjia1 &#x3D; &quot;&#x2F;dt[@class&#x3D;&#39;gray6 zongjia1&#39;]&quot;</span><br><span class="line">        span_red20b &#x3D; &quot;&#x2F;span[@class&#x3D;&#39;red20b&#39;]&quot;</span><br><span class="line">        dd_gray6 &#x3D; &quot;&#x2F;dd[@class&#x3D;&#39;gray6&#39;]&quot;</span><br><span class="line">        dd &#x3D; &quot;&#x2F;dd&quot;</span><br><span class="line">        dt &#x3D; &quot;&#x2F;dt&quot;</span><br><span class="line"></span><br><span class="line">        item &#x3D; ESFItem()</span><br><span class="line">        item[&#39;id&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                    div_title +</span><br><span class="line">                                    p_gray9 +</span><br><span class="line">                                    span_mr10).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        item[&#39;publish_time&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                              div_title +</span><br><span class="line">                                              p_gray9).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        item[&#39;title&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                       div_title +</span><br><span class="line">                                       h1).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        item[&#39;total_price&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                             div_houseInfor_clearfix +</span><br><span class="line">                                             div_inforTxt +</span><br><span class="line">                                             dl +</span><br><span class="line">                                             dt_gray6_zongjia1 +</span><br><span class="line">                                             span_red20b).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        dd_infos &#x3D; response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dd).extract()</span><br><span class="line">        huxing_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;户&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;型：&quot;</span><br><span class="line">        jzmj_str &#x3D; &quot;&lt;dd class&#x3D;\&quot;gray6\&quot;&gt;建筑面积：&lt;span class&#x3D;\&quot;black \&quot;&gt;&quot;</span><br><span class="line">        symj_str &#x3D; &quot;&lt;dd class&#x3D;\&quot;gray6\&quot;&gt;使用面积：&lt;span class&#x3D;\&quot;black \&quot;&gt;&quot;</span><br><span class="line">        nd_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;年&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;代：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        cx_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;朝&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;向：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        lc_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;楼&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;层：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        jg_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6 \&quot;&gt;结&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;构：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        zx_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;装&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;修：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        zzlb_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;住宅类别：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        jzlb_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;建筑类别：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        cqxz_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6 \&quot;&gt;产权性质：&lt;&#x2F;span&gt;&quot;</span><br><span class="line"></span><br><span class="line">        for i in dd_infos:</span><br><span class="line">            if huxing_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_type&#39;] &#x3D; i.strip()</span><br><span class="line"></span><br><span class="line">            elif jzmj_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_build_area&#39;] &#x3D; i.strip()</span><br><span class="line"></span><br><span class="line">            elif symj_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_use_area&#39;] &#x3D; i.strip()</span><br><span class="line"></span><br><span class="line">            elif nd_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_age&#39;] &#x3D; i.strip()</span><br><span class="line">               </span><br><span class="line">            elif cx_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;orientation&#39;] &#x3D; i.strip()</span><br><span class="line">                </span><br><span class="line">            elif lc_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;floor&#39;] &#x3D; i.strip()</span><br><span class="line">                </span><br><span class="line">            elif jg_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;structure&#39;] &#x3D; i.strip()</span><br><span class="line">                </span><br><span class="line">            elif zx_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;decoration&#39;] &#x3D; i</span><br><span class="line">                </span><br><span class="line">            elif zzlb_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;residential_category&#39;] &#x3D; i.strip()</span><br><span class="line">                </span><br><span class="line">            elif jzlb_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;building_class&#39;] &#x3D; i.strip()</span><br><span class="line">                </span><br><span class="line">            elif cqxz_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;property_right&#39;] &#x3D; i.strip()</span><br><span class="line">               </span><br><span class="line"></span><br><span class="line">        dt_infos &#x3D; response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dt + &quot;&#x2F;a&quot;).extract()</span><br><span class="line">        for i in dt_infos:</span><br><span class="line">            if &quot;查看此楼盘的更多二手房房源&quot; in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;property_name&#39;] &#x3D; i.strip()</span><br><span class="line">                </span><br><span class="line">            elif &quot;&lt;span class&#x3D;\&quot;gray6 floatl\&quot;&gt;学&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;校：&lt;&#x2F;span&gt;&quot; in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;school&#39;] &#x3D; i.strip()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这样我们就可以得到详情页的信息了。</p>
<p>接下来，把上面两个爬取的操作链接在一起：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def parse(self, response):</span><br><span class="line">       infos &#x3D; response.xpath(&quot;&#x2F;&#x2F;a&#x2F;@href&quot;).extract()</span><br><span class="line"></span><br><span class="line">       for i in infos:</span><br><span class="line">           i_str &#x3D; str(i).encode(&quot;utf-8&quot;)</span><br><span class="line">           if &quot;esf&quot; in i_str:</span><br><span class="line">               url &#x3D; i_str.replace(&#39;\\&#39;, &#39;&#39;).strip()</span><br><span class="line">               </span><br><span class="line">               # 执行下一个request，回去自动调用callback函数，去解析详情页</span><br><span class="line">               yield scrapy.Request(url&#x3D;url.replace(&quot;\&quot;&quot;, &quot;&quot;), callback&#x3D;self.parse_details)</span><br></pre></td></tr></table></figure>
<h4 id="设置起始url"><a href="#设置起始url" class="headerlink" title="设置起始url"></a>设置起始url</h4><p>由于要爬取的不仅仅是一页数据，所以我们的<code>start_urls</code>列表不能只有一个url，我们需要有一个<code>PageNo</code>自增长的url的list。</p>
<p>直接上代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">base_url &#x3D; &quot;http:&#x2F;&#x2F;esf.sh.fang.com&#x2F;map&#x2F;?mapmode&#x3D;y&amp;orderby&#x3D;30&amp;ecshop&#x3D;ecshophouse&amp;PageNo&#x3D;$&amp;a&#x3D;ajaxSearch&amp;city&#x3D;sh&amp;searchtype&#x3D;loupan&quot;</span><br><span class="line">    start_urls &#x3D; []</span><br><span class="line">    # 可以把10设置成一个比较大的数字，由于测试使用，所以写死一个较小的数</span><br><span class="line">    for i in range(2, 10):</span><br><span class="line">        start_urls.append(str(base_url).replace(&quot;$&quot;, str(i)))</span><br></pre></td></tr></table></figure>
<h4 id="可以开始了吗？"><a href="#可以开始了吗？" class="headerlink" title="可以开始了吗？"></a>可以开始了吗？</h4><p>列表爬取和详情爬取，以及起始url都设置ok了，那么我们是不是可以让我们的蜘蛛行动起来了呢？</p>
<p>且慢，这里也有坑。</p>
<p>这里我将页面数设置了200页，开始爬取，结果悲剧就发生了，由于短时间内访问量过大，搜房网的二手房页面似乎把我的ip屏蔽了：</p>
<p><img src="/img/16_11_30/006.png" alt=""></p>
<p>这里我参考了<a target="_blank" rel="noopener" href="http://www.tuicool.com/articles/VRfQR3U">这篇文章</a>，通过</p>
<ul>
<li>设置随机UserAgent</li>
<li>添加代理IP</li>
<li>禁用cookies</li>
<li>设置加载延迟</li>
</ul>
<p>这些方式来防止被屏蔽。</p>
<p>设置完毕后，就可以愉快的开始爬取了~</p>
<h4 id="蜘蛛代码"><a href="#蜘蛛代码" class="headerlink" title="蜘蛛代码"></a>蜘蛛代码</h4><p>最终蜘蛛的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"># coding&#x3D;utf-8</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">import scrapy</span><br><span class="line">from ..items import ESFItem</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(&#39;utf-8&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ESFListSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;esflist&quot;</span><br><span class="line"></span><br><span class="line">    # allowed_domains &#x3D; [&quot;fang.com&quot;]</span><br><span class="line"></span><br><span class="line">    base_url &#x3D; &quot;http:&#x2F;&#x2F;esf.sh.fang.com&#x2F;map&#x2F;?mapmode&#x3D;y&amp;orderby&#x3D;30&amp;ecshop&#x3D;ecshophouse&amp;PageNo&#x3D;$&amp;a&#x3D;ajaxSearch&amp;city&#x3D;sh&amp;searchtype&#x3D;loupan&quot;</span><br><span class="line">    start_urls &#x3D; []</span><br><span class="line">    for i in range(2, 4):</span><br><span class="line">        start_urls.append(str(base_url).replace(&quot;$&quot;, str(i)))</span><br><span class="line"></span><br><span class="line">    def __init__(self, user_agent&#x3D;&#39;&#39;):</span><br><span class="line">        self.user_agent &#x3D; user_agent</span><br><span class="line"></span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        # 这句话用于随机选择user-agent</span><br><span class="line">        ua &#x3D; random.choice(self.user_agent_list)</span><br><span class="line">        if ua:</span><br><span class="line">            request.headers.setdefault(&#39;User-Agent&#39;, ua)</span><br><span class="line"></span><br><span class="line">    user_agent_list &#x3D; [</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;537.1 (KHTML, like Gecko) Chrome&#x2F;22.0.1207.1 Safari&#x2F;537.1&quot; \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (X11; CrOS i686 2268.111.0) AppleWebKit&#x2F;536.11 (KHTML, like Gecko) Chrome&#x2F;20.0.1132.57 Safari&#x2F;536.11&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;536.6 (KHTML, like Gecko) Chrome&#x2F;20.0.1092.0 Safari&#x2F;536.6&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.2) AppleWebKit&#x2F;536.6 (KHTML, like Gecko) Chrome&#x2F;20.0.1090.0 Safari&#x2F;536.6&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.2; WOW64) AppleWebKit&#x2F;537.1 (KHTML, like Gecko) Chrome&#x2F;19.77.34.5 Safari&#x2F;537.1&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (X11; Linux x86_64) AppleWebKit&#x2F;536.5 (KHTML, like Gecko) Chrome&#x2F;19.0.1084.9 Safari&#x2F;536.5&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.0) AppleWebKit&#x2F;536.5 (KHTML, like Gecko) Chrome&#x2F;19.0.1084.36 Safari&#x2F;536.5&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1063.0 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 5.1) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1063.0 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1063.0 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.2) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1062.0 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1062.0 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.2) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1061.1 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1061.1 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.1) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1061.1 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.2) AppleWebKit&#x2F;536.3 (KHTML, like Gecko) Chrome&#x2F;19.0.1061.0 Safari&#x2F;536.3&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (X11; Linux x86_64) AppleWebKit&#x2F;535.24 (KHTML, like Gecko) Chrome&#x2F;19.0.1055.1 Safari&#x2F;535.24&quot;, \</span><br><span class="line">        &quot;Mozilla&#x2F;5.0 (Windows NT 6.2; WOW64) AppleWebKit&#x2F;535.24 (KHTML, like Gecko) Chrome&#x2F;19.0.1055.1 Safari&#x2F;535.24&quot;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        infos &#x3D; response.xpath(&quot;&#x2F;&#x2F;a&#x2F;@href&quot;).extract()</span><br><span class="line"></span><br><span class="line">        for i in infos:</span><br><span class="line">            i_str &#x3D; str(i).encode(&quot;utf-8&quot;)</span><br><span class="line">            if &quot;esf&quot; in i_str:</span><br><span class="line">                url &#x3D; i_str.replace(&#39;\\&#39;, &#39;&#39;).strip()</span><br><span class="line"></span><br><span class="line">                yield scrapy.Request(url&#x3D;url.replace(&quot;\&quot;&quot;, &quot;&quot;), callback&#x3D;self.parse_details)</span><br><span class="line"></span><br><span class="line">    def parse_details(self, response):</span><br><span class="line">        print &quot;+++++++++++++被执行了+++++++++++++++++++&quot;</span><br><span class="line"></span><br><span class="line">        # path</span><br><span class="line">        xpath &#x3D; &quot;&#x2F;&#x2F;body&#x2F;div[@class&#x3D;&#39;wrap&#39;]&#x2F;div[@class&#x3D;&#39;main clearfix&#39;]&#x2F;div[@class&#x3D;&#39;mainBoxL&#39;]&quot;</span><br><span class="line">        div_title &#x3D; &quot;&#x2F;div[@class&#x3D;&#39;title&#39;]&quot;</span><br><span class="line">        p_gray9 &#x3D; &quot;&#x2F;p[@class&#x3D;&#39;gray9&#39;]&quot;</span><br><span class="line">        h1 &#x3D; &quot;&#x2F;h1&quot;</span><br><span class="line">        span_mr10 &#x3D; &quot;&#x2F;span[@class&#x3D;&#39;mr10&#39;]&quot;</span><br><span class="line">        div_houseInfor_clearfix &#x3D; &quot;&#x2F;div[@class&#x3D;&#39;houseInfor clearfix&#39;]&quot;</span><br><span class="line">        div_inforTxt &#x3D; &quot;&#x2F;div[@class&#x3D;&#39;inforTxt&#39;]&quot;</span><br><span class="line">        dl &#x3D; &quot;&#x2F;dl&quot;</span><br><span class="line">        dt_gray6_zongjia1 &#x3D; &quot;&#x2F;dt[@class&#x3D;&#39;gray6 zongjia1&#39;]&quot;</span><br><span class="line">        span_red20b &#x3D; &quot;&#x2F;span[@class&#x3D;&#39;red20b&#39;]&quot;</span><br><span class="line">        dd_gray6 &#x3D; &quot;&#x2F;dd[@class&#x3D;&#39;gray6&#39;]&quot;</span><br><span class="line">        dd &#x3D; &quot;&#x2F;dd&quot;</span><br><span class="line">        dt &#x3D; &quot;&#x2F;dt&quot;</span><br><span class="line"></span><br><span class="line">        item &#x3D; ESFItem()</span><br><span class="line">        item[&#39;id&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                    div_title +</span><br><span class="line">                                    p_gray9 +</span><br><span class="line">                                    span_mr10).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        item[&#39;publish_time&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                              div_title +</span><br><span class="line">                                              p_gray9).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        item[&#39;title&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                       div_title +</span><br><span class="line">                                       h1).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        item[&#39;total_price&#39;] &#x3D; response.xpath(xpath +</span><br><span class="line">                                             div_houseInfor_clearfix +</span><br><span class="line">                                             div_inforTxt +</span><br><span class="line">                                             dl +</span><br><span class="line">                                             dt_gray6_zongjia1 +</span><br><span class="line">                                             span_red20b).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        dd_infos &#x3D; response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dd).extract()</span><br><span class="line">        huxing_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;户&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;型：&quot;</span><br><span class="line">        jzmj_str &#x3D; &quot;&lt;dd class&#x3D;\&quot;gray6\&quot;&gt;建筑面积：&lt;span class&#x3D;\&quot;black \&quot;&gt;&quot;</span><br><span class="line">        symj_str &#x3D; &quot;&lt;dd class&#x3D;\&quot;gray6\&quot;&gt;使用面积：&lt;span class&#x3D;\&quot;black \&quot;&gt;&quot;</span><br><span class="line">        nd_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;年&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;代：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        cx_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;朝&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;向：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        lc_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;楼&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;层：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        jg_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6 \&quot;&gt;结&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;构：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        zx_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;装&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;修：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        zzlb_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;住宅类别：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        jzlb_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6\&quot;&gt;建筑类别：&lt;&#x2F;span&gt;&quot;</span><br><span class="line">        cqxz_str &#x3D; &quot;&lt;span class&#x3D;\&quot;gray6 \&quot;&gt;产权性质：&lt;&#x2F;span&gt;&quot;</span><br><span class="line"></span><br><span class="line">        for i in dd_infos:</span><br><span class="line">            if huxing_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_type&#39;] &#x3D; i.strip()</span><br><span class="line">            elif jzmj_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_build_area&#39;] &#x3D; i.strip()</span><br><span class="line">            elif symj_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_use_area&#39;] &#x3D; i.strip()</span><br><span class="line">            elif nd_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;house_age&#39;] &#x3D; i.strip()</span><br><span class="line">            elif cx_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;orientation&#39;] &#x3D; i.strip()</span><br><span class="line">            elif lc_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;floor&#39;] &#x3D; i.strip()</span><br><span class="line">            elif jg_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;structure&#39;] &#x3D; i.strip()</span><br><span class="line">            elif zx_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;decoration&#39;] &#x3D; i</span><br><span class="line">            elif zzlb_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;residential_category&#39;] &#x3D; i.strip()</span><br><span class="line">            elif jzlb_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;building_class&#39;] &#x3D; i.strip()</span><br><span class="line">            elif cqxz_str in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;property_right&#39;] &#x3D; i.strip()</span><br><span class="line"></span><br><span class="line">        dt_infos &#x3D; response.xpath(xpath + div_houseInfor_clearfix + div_inforTxt + dl + dt + &quot;&#x2F;a&quot;).extract()</span><br><span class="line">        for i in dt_infos:</span><br><span class="line">            if &quot;查看此楼盘的更多二手房房源&quot; in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;property_name&#39;] &#x3D; i.strip()</span><br><span class="line">            elif &quot;&lt;span class&#x3D;\&quot;gray6 floatl\&quot;&gt;学&lt;span class&#x3D;\&quot;padl27\&quot;&gt;&lt;&#x2F;span&gt;校：&lt;&#x2F;span&gt;&quot; in str(i).encode(&quot;utf-8&quot;):</span><br><span class="line">                item[&#39;school&#39;] &#x3D; i.strip()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="what-s-next"><a href="#what-s-next" class="headerlink" title="what`s next?"></a>what`s next?</h3><p>接下来我会将爬取到的数据进行加工后持久化到本地，不过今天就先到这里吧~</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/10/24/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AD%E5%91%A8%20(1)%E8%AF%84%E4%BB%B7%E4%B8%80%E4%B8%AA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/10/24/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AD%E5%91%A8%20(1)%E8%AF%84%E4%BB%B7%E4%B8%80%E4%B8%AA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第六周 (1)评价一个学习算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-10-24 22:28:58" itemprop="dateCreated datePublished" datetime="2016-10-24T22:28:58+00:00">2016-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="如何少走弯路？"><a href="#如何少走弯路？" class="headerlink" title="如何少走弯路？"></a>如何少走弯路？</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/OVM4M/deciding-what-to-try-next">视频地址</a></p>
<h3 id="这些坑可能会耽误你几个月"><a href="#这些坑可能会耽误你几个月" class="headerlink" title="这些坑可能会耽误你几个月"></a>这些坑可能会耽误你几个月</h3><blockquote>
<p>到目前为止，我们已经介绍了许多不同的学习算法。如果你一直跟着这些视频的进度学习，你会发现自己已经不知不觉地成为一个了解许多先进机器学习技术的专家了。</p>
<p>然而，在懂机器学习的人当中，依然存在着很大的差距。一部分人确实掌握了怎样高效有力地运用这些学习算法，而另一些人他们可能对我马上要讲的东西就不那么熟悉了，他们可能没有完全理解怎样运用这些算法，因此总是把时间浪费在毫无意义的尝试上。</p>
<p>我想做的是，确保你在设计机器学习的系统时，你能够明白怎样选择一条最合适、最正确的道路。</p>
<p>因此，在这节视频和之后的几段视频中，我将向你介绍一些实用的建议和指导，帮助你明白怎样进行选择。</p>
<p>具体来讲，我将重点关注的问题是：假如你在开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，你应如何决定接下来应该选择哪条道路？</p>
</blockquote>
<p>以预测房价为例：</p>
<p>假如在预测房价的例子中，你已经完成了正则化线性回归，也就是最小化代价函数$J$的值。</p>
<p><img src="/img/16_10_24/001.png" alt=""></p>
<p>假如在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在的问题是要想改进这个算法接下来该怎么办？</p>
<p>实际上你可以想出很多方法来改进这个算法的性能。</p>
<p>通常情况下我们会使用这几种方法：</p>
<ul>
<li>通过使用更多的训练样本</li>
</ul>
<blockquote>
<p>但有的时候，更多的训练样本并不能解决问题。</p>
</blockquote>
<ul>
<li>尝试选用更少的特征集</li>
</ul>
<blockquote>
<p>你可以从众多的特征集中仔细挑选一小部分来防止<strong>过拟合</strong>。</p>
</blockquote>
<ul>
<li>尝试选用更多的特征集</li>
</ul>
<blockquote>
<p>也许目前的特征集对你来讲并不是很有帮助，你希望获取更多有用的特征数据</p>
</blockquote>
<ul>
<li>也可以尝试增加多项式特征的方法($x_{1}^{2}$,$x_{2}^{2}$,$x_{1}x_{2}$,etc.)</li>
<li>通过增大正则化参数$\lambda$</li>
<li>通过减小正则化参数$\lambda$</li>
</ul>
<p>上面的这些方法，都可以扩展开来，都可能是一个花费6个月甚至更长时间的项目。</p>
<p>遗憾的是，大多数人用来选择这些方法的标准是凭感觉的。而且很多人都会错误的选择其中一种方法花费大量的时间和精力，走上了“不归路”。</p>
<p>幸运的是，有一系列简单的方法，能让你事半功倍，排除掉上面的那个优化清单上的至少一半的方法，留下真正有用的方法。同时也有一种很简单的方法，可以很轻松的排除掉很多选择从而为你节省大量不必要花费的时间。</p>
<h3 id="机器学习诊断法引入"><a href="#机器学习诊断法引入" class="headerlink" title="机器学习诊断法引入"></a>机器学习诊断法引入</h3><p>在接下来的视频中，我首先介绍<strong>怎样评估机器学习算法的性能</strong>，然后在之后的几段视频中，我们将开始讨论这些方法，它们也被称为<strong>“机器学习诊断法(Machine learning diagnostic)”</strong>。</p>
<blockquote>
<p>Diagnostic:A test that you can run to gain insight what is/isn’t working with a learning algorithm, and gain guidance as to how best to improve its performance.</p>
<p>诊断法：这是一种测试法，你通过执行这种测试能够深入了解某种算法到底是否有用，并且也可以告诉你如何改进算法的效果。</p>
</blockquote>
<p>诊断法是一种很有用的方法，可以更有效率地利用好你的时间，但同时也会花费一些时间来实现。</p>
<h2 id="评估假设函数"><a href="#评估假设函数" class="headerlink" title="评估假设函数"></a>评估假设函数</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/yfbJY/evaluating-a-hypothesis">视频地址</a></p>
<p>当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化。有人认为，得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差并不能说明它就一定是一个好的假设函数，而且我们也学习了过拟合假设函数的例子。所以这推广到新的训练集上是不适用的。</p>
<p>那么，你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数$h(x)$进行画图，然后观察图形趋势：</p>
<p><img src="/img/16_10_24/002.png" alt=""></p>
<p>但对于特征变量不止一个的这种一般情况：</p>
<p><img src="/img/16_10_24/003.png" alt=""></p>
<p>想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现的。</p>
<p>因此我们需要另一种方法来评估我们的假设函数。</p>
<h3 id="评估假设函数的方法"><a href="#评估假设函数的方法" class="headerlink" title="评估假设函数的方法"></a>评估假设函数的方法</h3><p>如下给出了一种评估假设函数的标准方法：</p>
<p>假设我们有这样一组数据组：</p>
<p><img src="/img/16_10_24/004.png" alt=""></p>
<p>虽然这里只有十组数据，但通常情况下我们都有成百上千组训练样本。</p>
<p>为了确保我们可以评估我们的假设函数，我们要做的是将这些数据进行三七分：</p>
<p><img src="/img/16_10_24/005.png" alt=""></p>
<p>第一部分（70%）将成为我们的<strong>训练集(Training Set)</strong></p>
<p>第二部分（30%）将成为我们的<strong>测试集(Test Set)</strong></p>
<blockquote>
<p>将所有数据按照7:3的比例划分，是一种常见的划分比例</p>
</blockquote>
<p>现在我们有了一部分训练数据集：</p>
<script type="math/tex; mode=display">
(x^{(1)},y^{(1)})\\\\
(x^{(2)},y^{(2)})\\\\
......\\\\
(x^{(m)},y^{(m)})</script><blockquote>
<p>这里的$m$依然表示训练样本的总数</p>
</blockquote>
<p>剩下的部分数据将被用作测试数据：</p>
<script type="math/tex; mode=display">
(x^{(1)}\_{test},y^{(1)}\_{test})\\\\
(x^{(2)}\_{test},y^{(2)}\_{test})\\\\
......\\\\
(x^{(m\_{test})}\_{test},y^{(m\_{test})}\_{test})</script><blockquote>
<p>这里$m_{test}$表示测试样本的总数，$_{test}$表示这些样本是来自测试集。</p>
<p><strong>注意</strong>：如果说我们的数据是有某种规律的话，那么我们按照7:3的比例取数据时，应该是随机选取的。</p>
</blockquote>
<h3 id="评估步骤详解"><a href="#评估步骤详解" class="headerlink" title="评估步骤详解"></a>评估步骤详解</h3><h4 id="线性回归中"><a href="#线性回归中" class="headerlink" title="线性回归中"></a>线性回归中</h4><p>在线性回归中，的训练/测试流程：</p>
<ul>
<li>1.需要对训练集进行学习，得到参数$\theta$。</li>
</ul>
<blockquote>
<p>具体来讲就是最小化训练误差$J(\theta)$。这里的是使用那70%的数据训练得出的结果。</p>
</blockquote>
<ul>
<li>2.计算出测试误差</li>
</ul>
<blockquote>
<p>使用$J_{test}(\theta)$来表示测试误差，我们要做的是取出之前从训练集中学习得到的参数$\theta$带入到$J_{test}(\theta)$来计算我们的测试误差。可以写成如下形式：</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{align\*}
J\_{test}(\theta)= \frac{1}{2m\_{test}}\sum\_{i=1}^{m\_{test}}(
 h\_{\theta}(x\_{test}^{(i)}) - y\_{test}^{(i)}
 )^{2}
\end{align\*}</script><blockquote>
<p>这实际上是测试集平方误差的平均值。</p>
</blockquote>
<hr>
<h4 id="分类问题中"><a href="#分类问题中" class="headerlink" title="分类问题中"></a>分类问题中</h4><p>当然，这是当我们使用线性回归和平方误差标准时测试误差的定义，那么如果是分类问题，比如说使用逻辑回归的时候呢？</p>
<p>训练和测试逻辑回归与之前所说的非常类似：</p>
<ul>
<li>1.首先我们要从训练数据中（前70%）学习得到参数$\theta$</li>
<li>2.然后用下面的式子计算测试数据的误差值:</li>
</ul>
<p><img src="/img/16_10_24/006.png" alt=""></p>
<p>在分类问题中的测试误差$J_{test}(\theta)$其实也被称作<strong>误分类率</strong>（也被称为<strong>0/1错分率</strong>）。表示你预测到的正确或错误样本的情况。</p>
<p>比如说可以这样定义一次预测的误差：</p>
<ul>
<li><p>当$h_{\theta}\ge0.5$时$y=0$</p>
</li>
<li><p>或者当$h_{\theta}\lt0.5$时$y=1$</p>
</li>
</ul>
<p>这两种情况下我们的假设都对样本进行另外误判，否则其他情况下假设值都能正确的对样本$y$进行分类。</p>
<script type="math/tex; mode=display">
err(h\_{\theta}^{(x)},y)
=\\{
\begin{align\*}
&= 1 \ \ if\ h\_{\theta}(x)\ge0.5,\ y=0 
        \ or\ if\ h\_{\theta}(x)\le0.5,\ y=1 \\\\
&= 0 \ otherwise
\end{align\*}</script><p>然后我们就能应用错分率误差来定义测试误差，也就是：</p>
<script type="math/tex; mode=display">
\begin{align\*}
Test\ error = 
 \frac{1}{m\_{test}}\sum\_{i=1}^{m\_{test}}err(
 h\_{\theta}(x\_{test}^{(i)}), y\_{test}^{(i)}
 )
\end{align\*}</script><blockquote>
<p>以上我们介绍了一套标准技术来评价一个已经学习过的假设，在下一段视频中，我们要应用这些方法来帮助我们进行诸如特征选择一类的问题（比如多项式次数的选择，或者正则化参数的选择）。</p>
</blockquote>
<h2 id="多项式模型的选择以及训练集-验证集-测试集的划分"><a href="#多项式模型的选择以及训练集-验证集-测试集的划分" class="headerlink" title="多项式模型的选择以及训练集/验证集/测试集的划分"></a>多项式模型的选择以及训练集/验证集/测试集的划分</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/QGKbr/model-selection-and-train-validation-test-sets">视频地址</a></p>
<blockquote>
<p>假如你想要确定对于某组数据最合适的多项式次数是几次，怎样选用正确的特征来构造学习算法，这些问题我们称之为<strong>”模型选择问题“</strong>。</p>
<p>在我们对于这一问题的讨论中，我们还将提到如何将数据分为三组：也就是<strong>训练集、验证集和测试集</strong>，而不仅仅是前面提到的两组数据。</p>
<p>这一节中，我们将会介绍这些内容的含义。</p>
</blockquote>
<h3 id="选择合适的模型解决过拟合问题"><a href="#选择合适的模型解决过拟合问题" class="headerlink" title="选择合适的模型解决过拟合问题"></a>选择合适的模型解决过拟合问题</h3><p>在过拟合的情况中，学习算法在适用于训练集时表现非常完美，但这并不代表此时的假设也很完美。</p>
<p>更一般的说，这也是为什么训练集误差通常不能正确预测出该假设是否能很好地拟合新样本的原因。</p>
<p>具体来讲，如果你把这些参数集，比如$\theta_{0}$,$\theta_{1}$,$\theta_{2}$…调整到尽量拟合你的训练集，那么结果就是你的假设会在训练集上表现地很好，但这并不能确定当你的假设推广到训练集之外的新的样本上时，预测结果是怎样的。</p>
<p>而更为普遍的规律是，只要你的参数非常拟合某个数据组，比如说非常拟合训练集（当然也可以是其他数据集）：</p>
<p><img src="/img/16_10_24/007.png" alt=""></p>
<p>那么你的假设对于相同数据组的预测误差（比如说训练误差）是不能够用来推广到一般情况的，或者说，是不能作为实际的泛化误差的（也就是说，不能说明你的假设对于新样本的效果）。</p>
<hr>
<p>接下来具体来说说模型选择问题：</p>
<p>假如说你现在要选择能最好地拟合你数据的多项式次数，那么下面的式子你应该选择哪一个呢？</p>
<p><img src="/img/16_10_24/008.png" alt=""></p>
<p>假设$d$代表我们应该选择的多项式的次数(上面的式子次数依次为从1到10)，我们除了需要确定参数$\theta$之外，还要考虑如何确定这个多项式的次数$d$。</p>
<p>因此，我们需要确定参数$d$最适当的取值。</p>
<p>具体地说，比如你想要选择一个模型，那就从这10个模型中，选择一个最适当的多项式次数，并且用这个模型进行估计，预测你的假设能否很好地推广到新的样本上。</p>
<p>那么你可以这样做：</p>
<ul>
<li>你可以先选择第一个模型，然后求训练误差的最小值。这样你就会得到一个参数向量$\theta$。</li>
<li>然后你再选择第二个模型，进行同样的过程，这样你会得到另一个参数向量$\theta$。</li>
<li>拟合三次函数模型时，同理，也能得到一个参数向量$\theta$。</li>
<li>…</li>
<li>用相同的方式得到第10个模型的向量$\theta$。</li>
</ul>
<p><img src="/img/16_10_24/009.png" alt=""></p>
<p>如果一次模型训练后得到的参数向量为$\theta^{(1)}$,二次模型训练后得到的参数向量为$\theta^{(2)}$，依次类推，十次模型训练后得到的参数向量为$\theta^{(10)}$。</p>
<p>接下来，我们要做的是对所有这些模型求出<strong>测试集误差</strong>。因此，我们可以算出每一个模型的$Jtest(\theta^{(1)})$、$Jtest(\theta^{(2)})$、$Jtest(\theta^{(3)})$…$Jtest(\theta^{(10)})$。</p>
<p>接下来为了确定哪一个模型最好，我们可以找出<strong>测试集误差最小的模型</strong>。</p>
<p>假设我们最终选择了五次多项式模型:</p>
<script type="math/tex; mode=display">
Choose \ \ \theta\_{0} + ... \theta\_{5}x^{5}</script><h3 id="正确的评价某个假设函数的预测能力"><a href="#正确的评价某个假设函数的预测能力" class="headerlink" title="正确的评价某个假设函数的预测能力"></a>正确的评价某个假设函数的预测能力</h3><blockquote>
<p>由于我们使用的是<strong>测试结果集</strong>来衡量的代价函数，从而得到多项式次数$d$这个参数的值为5，所以这样任然不能公平地说明这个假设可以推广到一般情况。</p>
<p>也就是说我们选择了一个能最好地拟合测试集的参数$d$的值，因此我们的参数向量$\theta^{(5)}$在拟合测试集时的结果很可能导致一个比实际泛化误差更完美的预测结果。（因为我们找到的是一个最能拟合测试集的参数$d$）。因此我们再用测试集来评价我们的假设就显得不公平了。</p>
<p>而我们其实更关心的是对新样本的拟合效果，所以我们之前说到的如果我们用训练集来拟合参数$\theta_{0}$,$\theta_{1}$等参数时，拟合后的模型在作用于训练集上的效果是不能预测出我们将这个假设推广到新样板上时效果如何的。这是因为这些参数能够很好地拟合训练集，因此它们很有可能在对训练集的预测中表现的很好，但对其他的新样本来说就不一定那么好了。</p>
</blockquote>
<p>我们要做的实际上是用测试集来拟合参数$d$，但是这同样也意味着这并不能较为公平地预测出假设函数在遇到新样本时的表现。</p>
<h4 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h4><p>为了解决这一问题在模型选择中，如果我们要评价某个假设，我们通常采用以下的方法：</p>
<p>给定某个数据集，这里我们要将其分为三段：训练集、<strong>交叉验证集（cross validation set）</strong>、测试集。</p>
<p><img src="/img/16_10_24/010.png" alt=""></p>
<p>一种典型的分割比例是：训练集60%，交叉验证集20%，测试集20%。（这个比例可稍作调整）</p>
<p><img src="/img/16_10_24/011.png" alt=""></p>
<blockquote>
<p>$m$是训练集个数，$m_{cv}$是交叉验证集个数，$m_{test}$是测试集个数。</p>
</blockquote>
<p>我们可以得到 训练集/交叉验证集/测试集 的误差：</p>
<p><img src="/img/16_10_24/012.png" alt=""></p>
<p>现在我们的模型选择问题是这样的：我们现在要使用交叉验证集来选择合适的模型：</p>
<p><img src="/img/16_10_24/013.png" alt=""></p>
<p>即通过使用<strong>训练集</strong>对每一个假设函数依次去求最小化的代价函数$minJ(\theta)$，并求得对应的参数向量$\theta^{(d)}$。</p>
<p>然后我们要做的是在<strong>交叉验证集</strong>中测试这些假设的表现，测出$J_{cv}$来看看这些假设在交叉验证集中表现如何。</p>
<p>然后我们要选择<strong>交叉验证集</strong>误差最小的那个假设模型，假如这个模型是$J_{cv}(\theta^{(4)})$对应的那个模型，因此我们就选择这个四次多项式模型：</p>
<script type="math/tex; mode=display">
\theta\_{0} + \theta\_{1}x^{1} + ... + \theta\_{4}x^{4}</script><p>我们得到了拟合出最好的系数$d=4$。</p>
<p>这个过程中，我们没有使用<strong>测试集</strong>进行拟合，这样我们就回避了<strong>测试集</strong>的嫌疑。这样我们就可以光明正大的使用<strong>测试集</strong>来估计所选模型的泛化误差了。</p>
<p>最后，我们可以使用<strong>测试集</strong>来评价模型的表现。</p>
<p>但最后还是要提醒大家的一点是，在如今的机器学习应用中，确实也有很多人是像我之前介绍的那样做的（用测试集来选择模型，然后用同样的测试集来评价模型的表现，报告测试误差，看起来好像还能得到比较不错的泛化误差），我说过这并不是一个好的方法，但不幸的是，现在还有很多人这样做。如果测试集足够多的话，这也许还能行得通，但大多数的机器学习开发人员是不会这么做的，因为最佳做法还是把数据分成 训练集、验证集、测试集。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/10/24/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AD%E5%91%A8%20(2)%E5%81%8F%E5%B7%AEVS%E6%96%B9%E5%B7%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/10/24/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AD%E5%91%A8%20(2)%E5%81%8F%E5%B7%AEVS%E6%96%B9%E5%B7%AE/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第六周 (2)偏差VS方差</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-10-24 22:28:58" itemprop="dateCreated datePublished" datetime="2016-10-24T22:28:58+00:00">2016-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="偏差-VS-方差"><a href="#偏差-VS-方差" class="headerlink" title="偏差 VS 方差"></a>偏差 VS 方差</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/yCAup/diagnosing-bias-vs-variance">视频地址</a></p>
<blockquote>
<p>当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大、要么是方差比较大。换句话说，出现的情况要么是欠拟合、要么是过拟合问题。</p>
<p>对于这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关，搞清楚这一点非常重要。因为这两种情况其实是一个很有效的指示器，指示着改进算法最有效的方法和途径。</p>
<p>在这一节中，我将更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题。</p>
</blockquote>
<p>下面这几幅图，可能你已经见过很多次了：</p>
<p><img src="/img/16_10_24/014.png" alt=""></p>
<p>如果你用两个很简单的假设来拟合数据，那么不足以拟合这组数据（欠拟合）：</p>
<p><img src="/img/16_10_24/015.png" alt=""></p>
<p>而如果你用两个很复杂的假设来拟合时，那么对训练集来说，则会拟合的很好，但又过于完美（过拟合）：</p>
<p><img src="/img/16_10_24/016.png" alt=""></p>
<p>而像这样的中等复杂度的假设，对数据拟合得刚刚好：</p>
<p><img src="/img/16_10_24/017.png" alt=""></p>
<p>此时对应的泛华误差，也是三种情况中最小的。</p>
<p>现在我们已经掌握了“训练集”、“验证集”和“测试集”的概念，我们就能更好地理解偏差和方差的问题。</p>
<p>具体来说，我们沿用之前所使用的训练集误差和验证集误差的定义（也就是平方误差）即对训练集或验证集数据进行预测，所产生的平均平方误差。</p>
<p>下面我们来画出如下这个示意图，其中横坐标代表多项式的次数，纵坐标表示训练误差。其中<strong>蓝色线表示训练集误差的变化情况</strong>，<strong>红色线表示验证集误差的变化情况</strong>：</p>
<p><img src="/img/16_10_24/018.png" alt=""></p>
<p>可以看到随着多项式次数的增多，训练集误差呈下降趋势，验证集误差呈先降后升的趋势。</p>
<h3 id="如何分辨算法处于偏差还是方差？"><a href="#如何分辨算法处于偏差还是方差？" class="headerlink" title="如何分辨算法处于偏差还是方差？"></a>如何分辨算法处于偏差还是方差？</h3><p>假设你得出了一个学习算法，而这个算法并没有表现地如你期望那么好，所以你的交叉验证误差或者测试集误差都很大，我们应该如何判断此时的学习算法正处于高偏差的问题还是高方差的问题呢？</p>
<p><img src="/img/16_10_24/019.png" alt=""></p>
<p>图中左边点表示偏差(bias)情况，右侧点表示方差(variance)情况。</p>
<h4 id="偏差情况"><a href="#偏差情况" class="headerlink" title="偏差情况"></a>偏差情况</h4><p>可以看到偏差情况下，测试集误差和验证集误差都很大，两者误差可能很接近：</p>
<p><img src="/img/16_10_24/020.png" alt=""></p>
<p>那么当你遇到这种情况，就说明你的算法正处于高偏差的问题。</p>
<h4 id="方差情况"><a href="#方差情况" class="headerlink" title="方差情况"></a>方差情况</h4><p>而反过来，如果你的交叉验证集误差远远大于训练集误差：</p>
<p><img src="/img/16_10_24/021.png" alt=""></p>
<p>这就预示着你的算法正处于高方差和过拟合的情况。</p>
<h2 id="正则化和偏差-方差"><a href="#正则化和偏差-方差" class="headerlink" title="正则化和偏差/方差"></a>正则化和偏差/方差</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/4VDlf/regularization-and-bias-variance">视频地址</a></p>
<blockquote>
<p>你现在应该已经知道算法正则化可以有效地防止过拟合，但正则化跟算法的偏差和方差又有什么关系呢？在这一节中，我想更深入地探讨一下偏差和方差的问题。讨论一下两者之间是如何互相影响的，以及和算法的正则化之间的相互关系。</p>
</blockquote>
<p>假设我们要对这样一个高阶多项式进行拟合：</p>
<p><img src="/img/16_10_24/022.png" alt=""></p>
<p>为了防止过拟合现象，我们要使用一个正则化项，因此我们试图通过这样一个正则化项：</p>
<p><img src="/img/16_10_24/023.png" alt=""></p>
<p>来让参数的值尽可能的小。正则化项的求和范围照例取为$j=1$到$j=m$，而非$j=0$到$j=m$。</p>
<h3 id="正则化参数-λ-对假设函数的影响"><a href="#正则化参数-λ-对假设函数的影响" class="headerlink" title="正则化参数$λ$对假设函数的影响"></a>正则化参数$λ$对假设函数的影响</h3><p>然后我们来分析以下三种情形：</p>
<ul>
<li><strong>第一种情形是正则化参数$λ$取一个比较大的值（比如$λ$的值取为10000甚至更大）</strong></li>
</ul>
<blockquote>
<p>在这种情况下，所有这些参数$\theta_{1},\theta_{2},\theta_{3}$等等，将被大大惩罚。其结果是这些参数的值将近似于等于0，并且假设模型$h(x)$的值将等于或者近似等于$\theta_{0}$。因此我们最后得到的假设函数应该是这个样子的：<br><img src="/img/16_10_24/024.png" width = "300" height = "200" align=center /><br>这对于数据集来说，不是一个好的假设</p>
</blockquote>
<ul>
<li><strong>与之对应的另一种情况是$λ$值很小（比如说$λ$的值等于0）</strong></li>
</ul>
<blockquote>
<p>在这种情况下，如果我们要拟合一个高阶多项式的话，那么我们通常会处于过拟合（overfitting）的情况。</p>
<p><img src="/img/16_10_24/025.png" width = "300" height = "200" align=center /></p>
<p>在拟合一个高阶多项式时，如果没有进行正则化或者正则化程度很微小的话，我们通常会得到高方差和过拟合的结果。因为$λ$的值等于0，相当于没有正则化项，因此会对假设过拟合。</p>
</blockquote>
<ul>
<li><strong>只有当我们取一个中间大小的，既不大也不小的$λ$值时，我们才会得到一组合理的对数据刚好拟合的$\theta$参数值。</strong></li>
</ul>
<blockquote>
<p><img src="/img/16_10_24/026.png" width = "300" height = "200" align=center /></p>
</blockquote>
<p>那么我们应该怎样自动地选择出一个最合适的正则化参数$λ$呢？</p>
<p>重申一下，我们的模型和学习参数以及最优化目标是这样的:</p>
<p>让我们假设在使用正则化的情形中，定义$Jtrain(\theta)$为另一种不同的形式，同样定义为最优化目标，但不使用正则化项:</p>
<p><img src="/img/16_10_24/027.png" alt=""></p>
<p>在先前的授课视频中，当我们没有使用正则化时，我们定义的$JTrain(\theta)$就是代价函数$J(\theta)$。</p>
<p>但当我们使用正则化，多出这个$λ$项时，我们就将训练集误差($Jtrain$)定义为<strong>训练集数据预测误差的平方求和</strong>。或者说是训练集的评价误差平方和，但不考虑正则化项。</p>
<p>与此类似，我们来定义交叉验证集误差和测试集误差，和之前一样定义为对交叉验证集合测试集进行预测的平均误差平方和。</p>
<p><img src="/img/16_10_24/028.png" alt=""></p>
<p>总结一下，<strong>我们对于训练误差$Jtrain$,$Jcv$,$Jtest$的定义，都是平均误差平方和</strong>。</p>
<hr>
<h3 id="选择一个正确的-λ"><a href="#选择一个正确的-λ" class="headerlink" title="选择一个正确的$λ$"></a>选择一个正确的$λ$</h3><p>下面就是我们自动选取正则化参数$λ$的方法：</p>
<p>通常我的做法是选取一系列我想要尝试的$λ$值。因此首先我可能考虑不使用正则化的情形，以及一系列我可能会试的值，比如说我可能从0.01,0.02,0.04开始，一直试下去，通常我会将步长设为2倍的速度增长，直到一个比较大的值。在本例中，我们最终取值为10.24（实际上我们取的值是10，但已经非常接近了）。</p>
<p><img src="/img/16_10_24/029.png" alt=""></p>
<p>因此，这样我就得到了12个不同的正则化参数$λ$，对应的12个不同的模型，当然了，你也可以试小于0.01的值或者大于10的值。但在这里，我就不讨论这些情况了。</p>
<p>得到这12组模型后，接下来我们要做的事情是选用第一个模型，也就是$λ=0$，然后最小化我们的代价函数$J(\theta)$，这样我们就得到了某个参数向量$\theta$。</p>
<p>与之前视频的做法类似，我使用$\theta^{(1)}$来表示第一个参数向量$\theta$然后我再取第二个模型$λ=0.01$的模型，最小化代价方差，当然现在$λ=0.01$，那么会得到一个完全不同的参数向量$\theta$，用$\theta^{(2)}$来表示。</p>
<p>同理，接下来，我会得到$\theta^{(3)}$对应于我的第三个模型，以此类推，一直到最后一个$λ=10$或$λ=10.24$的模型对应的$\theta^{(12)}$。</p>
<p><img src="/img/16_10_24/030.png" alt=""></p>
<p>接下来我就可以用交叉验证集来评价这些假设参数了。</p>
<p>因此，我可以从第一个模型开始，然后是第二个模型，对每一个不同的正这化参数$λ$进行拟合，然后用交叉验证集来评价每一个模型:</p>
<p><img src="/img/16_10_24/031.png" alt=""></p>
<p>测出每一个参数$\theta$在交叉验证集上的评价误差平方和，然后我就选取这12个模型中交叉验证集误差最小的那个模型作为最终选择。</p>
<p>对于本例而言，假如说最终我选择了$\theta^{(5)}$，因为此时的交叉验证集误差最小，做完这些最后，如果我想看看该模型在测试集上的表现，我可以用经过学习得到的模型$\theta^{(5)}$来测出它对测试集的预测效果是如何。（再次重申，这里我们依然使用交叉验证集来拟合模型，这也是为什么我之前预留了一部分数据作为测试集的原因。）</p>
<p>这样我就可以用这部分测试集比较准确地估算出我的参数向量$\theta$对于新样本的泛化能力，这就是模型选择在选取正则化参数$λ$时的应用。</p>
<h3 id="正则化参数-λ-对交叉验证集误差和训练集误差产生的影响"><a href="#正则化参数-λ-对交叉验证集误差和训练集误差产生的影响" class="headerlink" title="正则化参数$λ$对交叉验证集误差和训练集误差产生的影响"></a>正则化参数$λ$对交叉验证集误差和训练集误差产生的影响</h3><p>在这一节中，我想讲的最后一个问题是，当我们改变正则化参数$λ$的值时，交叉验证集误差和训练集误差会随之发生怎样的变化。</p>
<p>我想提醒一下，我们最初的代价函数$J(\theta)$是包含正则化项的，但在这里我们把训练误差和交叉验证集误差定义为不包括正则化项。</p>
<p><img src="/img/16_10_24/033.png" alt=""></p>
<p>我要做的是绘制出$Jtrain(\theta)$和$Jcv(\theta)$的曲线，表达的是随着增大正则化项参数$λ$，看看假设在训练集上的是如何变化的，以及在交叉验证集上表现如何变化。</p>
<p>就像我们之前看到的，如果$λ$的值很小，那也就是说我们几乎没有使用正则化，因此我们有很大可能处于过拟合；而如果$λ$的值取的很大的时候，我们很有可能处于高偏差的情况。</p>
<h3 id="λ-在训练集上的变化"><a href="#λ-在训练集上的变化" class="headerlink" title="$λ$在训练集上的变化"></a>$λ$在训练集上的变化</h3><p>所以，如果你画出$Jtrain(\theta)$和$Jcv(\theta)$的曲线，你就会发现当$λ$的取值很小时，对训练集的拟合相对较好，因为没有使用正则化。而如果$λ$的值很大时，你将处于高偏差问题，不能对训练集很好地拟合，训练集误差$Jtrain(\theta)$的值会趋于上升。</p>
<p><img src="/img/16_10_24/032.png" alt=""></p>
<p>此时，你练训练集都不能很好地拟合。反过来，当$λ$的值取得很小的时候，你的数据能随意地与高次多项式很好地拟合。</p>
<h3 id="λ-在交叉验证集上的变化"><a href="#λ-在交叉验证集上的变化" class="headerlink" title="$λ$在交叉验证集上的变化"></a>$λ$在交叉验证集上的变化</h3><p>在曲线的右端，当$λ$的值取得很大时，我们会处于欠拟合问题。这对应着偏差问题，那么此时交叉验证集误差将会很大。我们的假设不能在交叉验证集上表现地比较好。</p>
<p><img src="/img/16_10_24/034.png" alt=""></p>
<p>而在曲线的左端，对应的是高方差问题，此时我们的$λ$值取得很小很小，因此我们会对数据过度拟合，所以交叉验证集误差也会很大。</p>
<p>这就是当我们改变正则化参数$λ$的值时，交叉验证集误差和训练集误差随之发生的变化。当然，在中间取的某个$λ$的值，表现得刚好合适，这种情况下表现最好，交叉验证集误差或者测试集误差都很小。</p>
<blockquote>
<p>当然由于我在这里画的图显得太卡通，也太理想化了，对于真实的数据，你得到的曲线可能比这个看起来更凌乱，会有很多的噪声，对某个实际的数据集，你或多或少能看出像这样一个趋势。通过绘出这条曲线，通过交叉验证集误差的变化趋势，你可以用自己选择出，或者编写程序自动得出能使交叉验证集误差最小的那个点，然后选出那个与之对应的参数$λ$的值。</p>
<p>当我在尝试为学习算法选择正则化参数$λ$的时候，我通常都会画出像这样一个图，帮助我更好地理解各种情况，同时也帮助我确认我选择的正则化参数值到底好不好。</p>
<p>希望这节课的内容让你更深入地理解了正则化以及它对学习算法的偏差和方差的影响，到目前为止你已经从不同角度认识了方差和偏差问题 在下一节视频中我要做的是基于我们已经介绍过的所有这些概念，将它们结合起来，建立我们的诊断法。也称为<strong>学习曲线</strong>这种方法通常被用来诊断一个学习算法到底是处于偏差问题还是方差问题，还是两者都有。</p>
</blockquote>
<h2 id="学习曲线-Learning-Curves"><a href="#学习曲线-Learning-Curves" class="headerlink" title="学习曲线(Learning Curves)"></a>学习曲线(Learning Curves)</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Kont7/learning-curves">视频地址</a></p>
<blockquote>
<p>绘制学习曲线非常有用，也许你想检查你的学习算法运行是否正常，或者你希望改进算法的表现或效果，那么学习曲线就是一种很好的工具。</p>
<p>我经常使用学习曲线来判断某一个学习算法是否处于偏差方差问题，或者二者皆有。</p>
</blockquote>
<p>为了绘制一条学习曲线，我通常先绘制出$Jtrain(\theta)$（也就是训练集数据的平均误差平方和）或者$Jcv(\theta)$（也就是交叉验证集数据的平均误差平方和）。</p>
<p><img src="/img/16_10_24/035.png" alt=""></p>
<p>我要将其绘制成一个关于参数m的函数（也就是一个关于<strong>训练集样本总数</strong>的函数）。</p>
<p><img src="/img/16_10_24/036.png" alt=""></p>
<p>所以m一般都是一个常数，比如$m=100$，表示100组训练样本。但我要自己取一些m的值，比如说我取10，20，30或者40组训练集，然后绘制出训练集误差，以及交叉验证集误差。</p>
<p>那么我们来看看，这条曲线绘制出来是什么样子。</p>
<p>假设我使用二次函数来拟合模型：</p>
<script type="math/tex; mode=display">
h\_{\theta}(x) = \theta\_{0} + \theta\_{1}x + \theta\_{2}x^{2}</script><p>当只有一组训练样本，即$m=1$，假设函数对数据的拟合情况正如下图所示：</p>
<p><img src="/img/16_10_24/037.png" width = "300" height = "200" align=center /></p>
<p>由于我只有一个训练样本，拟合的结果很明显会很好。对这一个训练样本拟合，其误差一定为0。</p>
<p>如果有两组训练样本，二次函数也能很好地拟合：</p>
<p><img src="/img/16_10_24/038.png" width = "300" height = "200" align=center /></p>
<p>即使是使用正则化，拟合结果也会很好。而如果不使用正则化的话，那么拟合效果绝对棒极了。</p>
<p>如果我用三组训练样本的话，看起来依然能很好地用二次函数拟合：</p>
<p><img src="/img/16_10_24/039.png" width = "300" height = "200" align=center /></p>
<p>也就是说，当$m=1$、$m=2$或$m=3$时，对训练集数据进行预测，得到的训练集误差都将等于0（这里假设我不使用正则化，当然如果使用正则化那么误差就稍大于0）。</p>
<p>好的，总结一下，我们现在已经看到，当训练样本容量m很小的时候，训练误差也会很小，因为很显然，如果我们训练集很小，那么很容易就能把训练集拟合到很好，甚至完全拟合。</p>
<p>现在我们来看看当$m=4$的时候，二次函数似乎也能对数据拟合得很好：</p>
<p><img src="/img/16_10_24/040.png" width = "300" height = "200" align=center /></p>
<p>那我们再看当$m=5$的情况，这时候再用二次函数来拟合，好像效果有下降，但还是差强人意：</p>
<p><img src="/img/16_10_24/041.png" width = "300" height = "200" align=center /></p>
<p>而当我的训练集越来越大的时候，你不难发现，要保证使用二次函数的拟合效果依然很好，就显得越来越困难了：</p>
<p><img src="/img/16_10_24/042.png" width = "300" height = "200" align=center /></p>
<p>因此，事实上随着训练集容量的增大，我们不难发现我们的平均训练误差是逐渐增大的，因此如果你画出这条曲线，你就会发现，训练集误差对假设进行预测的误差平均值随着m的增大而增大。</p>
<p><img src="/img/16_10_24/043.png" width = "300" height = "200" align=center /></p>
<p>那么对于交叉验证误差的情况如何呢？</p>
<p>好的，交叉验证集误差是对完全陌生的交叉验证集数据进行预测得到的误差，那么我们知道，当训练集很小的时候，泛化程度不会很好（意思是不能很好的适应新样本）。因此这个假设就不是一个理想的假设，只有当我使用一个更大的训练集时，我才有可能得到一个能够更好拟合数据的可能的假设。</p>
<p>因此你的验证集误差和测试集误差都会随着训练集样本容量m的增加而减小，因为你是用的数据越多，你能获得的泛化能力就越强，或者说对新样本的适应能力就越强。因此数据越多，越能拟合出合适的假设。</p>
<p>所以如果你把$Jtrain(\theta)$和$Jcv(\theta)$绘制出来，就应该得到这样的曲线：</p>
<p><img src="/img/16_10_24/044.png" width = "300" height = "200" align=center /></p>
<hr>
<h3 id="高偏差-高方差下的学习曲线"><a href="#高偏差-高方差下的学习曲线" class="headerlink" title="高偏差/高方差下的学习曲线"></a>高偏差/高方差下的学习曲线</h3><p>现在我们来看看当处于高偏差或者高方差的情况时，这些学习曲线又会变成什么样子。</p>
<h4 id="高偏差下的学习曲线"><a href="#高偏差下的学习曲线" class="headerlink" title="高偏差下的学习曲线"></a>高偏差下的学习曲线</h4><p>假如你的假设处于高偏差问题，为了更清楚地解释这个问题，我要用一个简单的例子来说明，也就是用一条直线来拟合数据的例子（很显然，一条直线不能很好地拟合数据）：</p>
<script type="math/tex; mode=display">
h\_{\theta}(x)=\theta\_{0} + \theta\_{1}x</script><p><img src="/img/16_10_24/045.png" width = "300" height = "200" align=center /></p>
<p>现在我们来想一想，如果增大样本容量m会发生什么情况呢？</p>
<p><img src="/img/16_10_24/046.png" width = "300" height = "200" align=center /></p>
<p>当样本数量增多的时候，你不难发现用来拟合这些数据的直线相较于之前不会变化太大，因为这条直线是对这组数据最接近的拟合，但一条直线再怎么接近，也不可能对这组数据进行很好的拟合。</p>
<p>所以，如果你绘制出交叉验证集误差，应该是这个样子：</p>
<p><img src="/img/16_10_24/047.png" width = "300" height = "200" align=center /></p>
<p>最左边表示训练集样本容量很小，比如说只有一组样本，那么表现当然很不好；当达到某一个容量值的时候，你就会找到那条最有可能拟合数据的那条直线，并且此时即便你继续增大训练集的样本容量m，你基本上还是会得到一条差不多的直线。因此交叉验证集误差将会很快变为水平而不再变化。</p>
<p>那么训练误差又如何呢？</p>
<p>同样，训练误差一开始也是很小的，而在高偏差的情形中，你会发现训练集误差会逐渐增大，一直趋于接近交叉验证集误差，这是因为你的参数很少。但当m很大的时候，数据太多，此时训练集和交叉验证集的预测结果将会非常接近：</p>
<p><img src="/img/16_10_24/048.png" width = "300" height = "200" align=center /></p>
<p>这就是当你的学习算法处于高偏差情形时学习曲线的大致走向。</p>
<p>最后补充一点，高偏差的情形反映出的问题是交叉验证集和训练集误差都很大，也就是说，你最终会得到一个值比较大的$Jcv(\theta)$和$Jtrain(\theta)$。</p>
<p>这也得出一个很有意思的结论，那就是如果一个学习算法有很大的偏差，那么当我们选用更多的训练样本时，我们发现交叉验证集误差的值不会表现出明显的下降，实际上是变为水平了。</p>
<p><strong>所以如果学习算法正处于高偏差的情形，那么选用更多的训练集数据对于改善算法表现无益。</strong></p>
<p>所以能够看清你的算法正处于高偏差的情形，是一件很有意义的事情，因为这样可以让你避免把时间浪费在想收集更多的训练样本上，因为再多的数据也是无意义的。</p>
<hr>
<h4 id="高方差下的学习曲线"><a href="#高方差下的学习曲线" class="headerlink" title="高方差下的学习曲线"></a>高方差下的学习曲线</h4><p>接下来我们再来看看正处于高方差的时候，学习曲线应该是什么样子的。</p>
<p>首先我们看看训练集误差，如果你的训练集样本容量很小，如下图：</p>
<p><img src="/img/16_10_24/049.png" width = "300" height = "200" align=center /></p>
<p>如果我们用很高阶次的多项式来拟合：</p>
<script type="math/tex; mode=display">
h\_{\theta}(x) = \theta\_{0} + \theta\_{1}x + ... + \theta\_{100}x^{100}</script><blockquote>
<p>当然不会有人用这样的多项式，这里只是为了演示使用。</p>
</blockquote>
<p>假设我们使用一个很小的$\lambda$，那么很显然，我们会对这组数据拟合的非常好：</p>
<p><img src="/img/16_10_24/050.png" width = "300" height = "200" align=center /></p>
<p>所以，如果训练集样本容量很小时，训练集误差$Jtrain(\theta)$将会很小，随着训练集样本容量的增加，可能这个假设函数任然会对数据或多或少有一点过拟合，但很明显此时要对数据很好地拟合显得更加困难和吃力了：</p>
<p><img src="/img/16_10_24/051.png" width = "300" height = "200" align=center /></p>
<p>所以随着训练集样本容量的增大，我们会发现$Jtrain(\theta)$的值会随之增大，因为当训练样本越来越多的时候，我们就越难将训练集数据拟合得很好，但总体来说，训练集误差还是很小：</p>
<p><img src="/img/16_10_24/052.png" width = "300" height = "200" align=center /></p>
<p>那么交叉验证集误差又如何呢？</p>
<p>在高方差的情形中，假设函数对数据过拟合，因此交叉验证集误差将会一直都很大，即便我们选择一个比较合适恰当的训练集样本数：</p>
<p><img src="/img/16_10_24/053.png" width = "300" height = "200" align=center /></p>
<p><strong>所以算法处于高方差情形最明显的一个特点是在训练集误差和交叉验证集误差之间以一段很大的差距：</strong></p>
<p><img src="/img/16_10_24/054.png" width = "300" height = "200" align=center /></p>
<p>而这个曲线也反映出如果我们要考虑增大训练集的样本数，这两条学习曲线会逐渐靠近，<strong>高方差情形下使用更多的数量级对改进算法的表现事实上是有效果的。</strong></p>
<hr>
<blockquote>
<p>以上画出的学习曲线都是相当理想化的形式，针对一个实际的学习算法，如果你画出学习曲线的话，你会看到基本类似的结果。虽然有的时候数据会带有一点噪声或干扰的曲线，但总的来说像这样画出学习曲线确实能帮助你来看清你的学习算法是否处于高偏差、高方差、或者二者皆有的情形。</p>
<p>所以在改进一个学习算法的时候，通常要先画出这些学习曲线。这项工作会让你更轻松地看出偏差或方差的问题。</p>
</blockquote>
<h2 id="重新审视决定下一步做什么"><a href="#重新审视决定下一步做什么" class="headerlink" title="重新审视决定下一步做什么"></a>重新审视决定下一步做什么</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/zJTzp/deciding-what-to-do-next-revisited">视频地址</a></p>
<blockquote>
<p>我们讨论了模型选择问题，偏差和方差的问题，那么这些诊断法则怎样帮助我们判断哪些方法可能有助于改进学习算法的效果，而哪些可能是徒劳的呢？让我们再次回到最开始的例子，在那里寻找答案。</p>
</blockquote>
<p>这就是我们之前的例子：</p>
<blockquote>
<p>Debugging 一个学习算法：</p>
<p>假设你用正则化线性回归来预测房价。但当你尝试在一组新的数据上使用你的假设函数时，你发现它出现了很大的无法容忍的误差。你接下来要怎么做来改进这个算法呢？</p>
</blockquote>
<p>我们使用正则化的线性回归拟合模型，却发现该算法没有达到预期效果。我们提到我们有如下选择方案：</p>
<ul>
<li>通过使用更多的训练样本</li>
<li>尝试选用更少的特征集</li>
<li>尝试选用更多的特征集</li>
<li>也可以尝试增加多项式特征的方法$(x_{1}^{2},x_{2}^{2},x_{1}x_{2},etc.)$</li>
<li>通过增大正则化参数$λ$</li>
<li>通过减小正则化参数$λ$</li>
</ul>
<p>那么如何判断哪些方法更可能是有效的呢？</p>
<p>第一种可供选择的方法是使用更多的训练集数据，这种方法对于<strong>高方差问题</strong>是有帮助的。</p>
<p>第二种方法的情况同样是对<strong>高方差问题</strong>时有效。</p>
<blockquote>
<p>换句话说如果你通过绘制学习曲线或者别的什么方法看出你的模型处于高偏差问题，那么切记千万不要浪费时间视图从已有的特征中挑出一小部分来使用。</p>
<p>如果你发现你的算法处于高方差的情形，那么你需要花一点时间来挑选出一小部分合适的特征，这是把时间用在了刀刃上的。</p>
</blockquote>
<p>第三种方法选用更多的特征集，通常来讲尽管不是所有时候都适用，但增加特征数<strong>一般可以帮助解决高偏差问题</strong>。</p>
<blockquote>
<p>如果你需要增加更多的特征时，一般是由于你现有的假设函数太简单，因此我们才决定增加一些别的特征来让假设函数更好地拟合训练集。</p>
</blockquote>
<p>第四种方式，增加多项式特征与第三种方式类似，也是用于修正<strong>高偏差问题</strong>。</p>
<blockquote>
<p>具体来说，如果你画出的学习曲线告诉你你还是处于高方差问题，那么采取这种方法就是浪费时间。</p>
</blockquote>
<p>第五种和第六种方式，增大和减小正则化参数$λ$，是很方便的方法，不至于花费太多时间。减小$λ$可以修正<strong>高偏差问题</strong>，增大$λ$可以修正<strong>高方差问题</strong>。</p>
<hr>
<h3 id="和神经网络的联系"><a href="#和神经网络的联系" class="headerlink" title="和神经网络的联系"></a>和神经网络的联系</h3><p>最后我们回顾一下这几节课介绍的这些内容，并且看看它们和神经网络的联系。</p>
<p>我想介绍一些很实用的经验或建议，这些也是我平时为神经网络模型选择结构或者链接形式的一些技巧。</p>
<p>当你在用一个相对比较简单的神经网络模型进行神经网络拟合的时候：</p>
<ul>
<li>其中有一种选择是选择相对简单的隐藏层结构，比如说只有一个隐藏层，或者相对来讲比较少的隐藏层单元：</li>
</ul>
<p><img src="/img/16_10_24/055.png" width = "300" height = "200" align=center /></p>
<blockquote>
<p>这种结构简单的神经网络参数就不会很多，很容易出现<strong>欠拟合</strong>。这种比较小型的神经网络其最大优势在于计算量较小。</p>
</blockquote>
<ul>
<li>与之相对的另一种情况是相对较大型的神经网络结构：要么隐藏层单元比较多，要么隐藏层单元比较多：</li>
</ul>
<p><img src="/img/16_10_24/056.png" width = "300" height = "200" align=center /></p>
<blockquote>
<p>这种比较复杂的神经网络参数一般比较多，也更容易出现<strong>过拟合</strong>。这种结构的一大劣势也许不是主要的，但还是需要考虑，那就是当网络中的神经元数量很多的时候，这种结构会显得计算量较大，虽然有这个情况，但通常来讲这不是大问题。</p>
<p>这种大型网络最主要的问题还是它更容易出现过拟合现象。</p>
<p><strong>事实上，如果你经常应用神经网络，特别是大型神经网络的话，你就会发现越大型的网络性能越好，但如果发生了过拟合，你可以使用正则化的方法来修正过拟合。</strong></p>
<p>一般来说，使用一个大型的神经网络并使用正则化来修正过拟合问题，通常比使用一个小型的神经网络效果更好。但主要问题可能出现在计算量相对较大。</p>
<p><strong>最后，你还要选择隐藏层的层数，你是该用一个隐藏层呢，还是用三个呢？通常来说，默认的情况是使用一个隐藏层。但是如果你确实想要选择多个隐藏层你也可以试试把数据分割为训练集、验证集和测试集，然后使用交叉验证的方法比较一个隐藏层的神经网络然后试试两个三个隐藏层以此类推，然后看看哪个神经网络在交叉验证集上表现得最理想，然后你对每一个模型都用交叉验证集数据进行测试，算出三种情况下（隐藏层分别为一层、两层、三层）的交叉验证集误差$Jcv(\theta)$，然后选出你认为最好的神经网络结构。</strong></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/10/16/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%94%E5%91%A8%20(4)%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/10/16/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%94%E5%91%A8%20(4)%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第五周 (4)神经网络实现自动驾驶</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-10-16 00:18:58" itemprop="dateCreated datePublished" datetime="2016-10-16T00:18:58+00:00">2016-10-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="使用神经网络实现自动驾驶"><a href="#使用神经网络实现自动驾驶" class="headerlink" title="使用神经网络实现自动驾驶"></a>使用神经网络实现自动驾驶</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/zYS8T/autonomous-driving">视频地址</a></p>
<p>这一节将介绍一个具有历史意义的神经网络学习的例子，那就是使用神经网络来实现自动驾驶。也就是说汽车通过学习来自己驾驶。</p>
<p>这是Dean Pomerleau那里得到的一段视频，Dean Pomerleau任职于美国东海岸的卡耐基梅隆大学，下面的视频中你会明白可视化技术到底是什么。</p>
<p><img src="/img/16_10_16/001.png" alt=""></p>
<p>在上面这张图中，左下方就是汽车所看到的前方路况图像，是汽车前方摄像头每两秒采集的并且进过压缩处理之后得到的一张30 × 32的图像。这张图片中，你依稀可以看到一条道路。</p>
<p><img src="/img/16_10_16/002.png" alt=""></p>
<p>左上角第一个水平的进度条显示的是驾驶员所选择的方向，就是那条白亮的区段显示的就是人类驾驶者选择的方向，白色区段偏左，就是向左转，反之，则是向右转。</p>
<p><img src="/img/16_10_16/003.png" alt=""></p>
<p>第二个水平的进度条则是学习算法选出的行驶方向，代表的含义和第一个条一样，白色区段偏左，就是向左转，反之，则是向右转。</p>
<p><img src="/img/16_10_16/004.png" alt=""></p>
<p>实际上神经网络在开始学习之前，你会看到网络的输出是一条灰色的区段，覆盖着整个区域，只有在学习算法运行足够长的时间之后，亮白色的区段才能逐渐显现。</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ilP4aPDTBPE" frameborder="0" allowfullscreen></iframe>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/10/13/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%94%E5%91%A8%20(2)BP%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/10/13/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%94%E5%91%A8%20(2)BP%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第五周 (2)BP算法练习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-10-13 00:08:00" itemprop="dateCreated datePublished" datetime="2016-10-13T00:08:00+00:00">2016-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="将参数从矩阵展开成向量"><a href="#将参数从矩阵展开成向量" class="headerlink" title="将参数从矩阵展开成向量"></a>将参数从矩阵展开成向量</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/60Uxp/implementation-note-unrolling-parameters">视频地址</a></p>
<blockquote>
<p>在这一节中，我想快速地向你介绍一个细节的实现过程，怎样把你的参数从矩阵展开成向量，以便我们在高级最优化步骤中的使用需要。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">function [jVal, gradient] &#x3D; costFunction(theta)</span><br><span class="line">	...</span><br><span class="line">optTheta &#x3D; fminunc(@costFunction, initialTheta, options)</span><br></pre></td></tr></table></figure>
<p>具体来讲，对代价函数<code>costFunction(theta)</code>传入参数<code>theta</code>，函数返回值是代价函数<code>jVal</code>以及导数值<code>gradient</code>，然后你可以将返回值传递给高级最优化算法<code>fminunc</code>。</p>
<blockquote>
<p>顺便提一下，<code>fminunc</code>并不是唯一的算法，你也可以使用别的优化算法，</p>
</blockquote>
<p>其中<code>costFunction</code>中的参数、返回值<code>gradient</code>以及<code>fiminunc</code>的参数<code>initialTheta</code>都是一个$R^{n+1}$阶的向量。</p>
<p><img src="/img/16_10_13/001.png" alt=""></p>
<p>这部分在我们使用逻辑回归的时候运行顺利，但现在对于神经网络，我们的参数将不再是向量，而是矩阵了。</p>
<p>以一个拥有4层的完整的神经网络为例：</p>
<p>其参数<code>theta</code>所代表的参数矩阵为矩阵为$Θ^{(1)}$,$Θ^{(2)}$,$Θ^{(3)}$，在Octave中，我们可以设为<code>Theta1</code>,<code>Theta2</code>,<code>Theta3</code>。</p>
<p><img src="/img/16_10_13/003.png" alt=""></p>
<p><img src="/img/16_10_13/002.png" alt=""></p>
<p>类似的，这些梯度项<code>gradient</code>也是<code>costFunction</code>的返回值之一，在之前的视频中我们演示了如何计算这些梯度矩阵，它们的计算结果是$D^{(1)}$,$D^{(2)}$,$D^{(3)}$，在Octave中用<code>D1</code>,<code>D2</code>,<code>D3</code>来表示。</p>
<p><img src="/img/16_10_13/005.png" alt=""></p>
<p><img src="/img/16_10_13/004.png" alt=""></p>
<p>在这一节中，我想很快地向你介绍怎样取出这些矩阵，并将他们展开成向量，以便它们最终成为恰当的格式，能够传入这里的<code>initialTheta</code>:</p>
<p><img src="/img/16_10_13/006.png" alt=""></p>
<p>并且得到正确的梯度返回值<code>gradient</code>。</p>
<hr>
<p>具体来说，假设我们有这样一个神经网络：</p>
<p><img src="/img/16_10_13/007.png" alt=""></p>
<p>其输入层有10个输入单元($s_{1}=10$)，隐藏层有10个单元($s_{2}=10$)，最后的输出层只有一个输出单元($s_{3}=1$)。</p>
<p>在这种情况下矩阵$Θ$的维度，和矩阵$D$的维度将有这个神经网络的结构所决定，比如$Θ^{(1)}$是一个$10 × 11$的矩阵。</p>
<p>因此，在Octave中，如果你想讲这些矩阵向量化，那么你要做的是取出你的$Θ^{(1)}$、$Θ^{(2)}$、$Θ^{(3)}$，然后使用下面这段代码，这段代码将取出三个$Θ$矩阵中的所有元素，然后把他们全部展开，成为一个很长的向量，也就是<code>thetaVec</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thetaVec &#x3D; [Theta1(:);Theta2(:);Theta3(:)];</span><br></pre></td></tr></table></figure>
<p>同样的，下面这段代码将取出$D$矩阵的所有元素，然后展开成一个长向量<code>DVec</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DVec &#x3D; [D1(:);D2(:);D3(:)];</span><br></pre></td></tr></table></figure>
<p>最后，如果你想要从向量表达式返回到矩阵表达式的话，你要做的就是使用<code>reshape</code>函数，传入向量的区间以及矩阵的行数和列数，即可得到对应的矩阵：</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 &#x3D; reshape(thetaVec(1:110),10,11);</span><br><span class="line">Theta2 &#x3D; reshape(thetaVec(111:220),10,11);</span><br><span class="line">Theta3 &#x3D; reshape(thetaVec(221:231),10,11);</span><br></pre></td></tr></table></figure></h2><p>下面用一个Octave例子来展示上面的计算过程：</p>
<p>首先，让我们假设<code>Theta1</code>是一个10行11列的单位矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; Theta1 &#x3D; ones(10,11);</span><br><span class="line">&gt;&gt; Theta1</span><br><span class="line">Theta1 &#x3D;</span><br><span class="line"></span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br></pre></td></tr></table></figure>
<p><code>Theta2</code>是一个元素都为2的10行11列的矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; Theta2 &#x3D; 2*ones(10,11);</span><br><span class="line">&gt;&gt; Theta2</span><br><span class="line">Theta2 &#x3D;</span><br><span class="line"></span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br></pre></td></tr></table></figure>
<p>然后假设<code>Theta3</code>是一个<code>1×11</code>的元素均为3的矩阵：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; Theta3 &#x3D; 3*ones(1,11);</span><br><span class="line">&gt;&gt; Theta3</span><br><span class="line">Theta3 &#x3D;</span><br><span class="line"></span><br><span class="line">   3   3   3   3   3   3   3   3   3   3   3</span><br></pre></td></tr></table></figure>
<p>现在，我们想把这些所有的矩阵变成一个向量<code>thetaVec</code> ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; thetaVec &#x3D; [ Theta1(:); Theta2(:); Theta3(:)];</span><br><span class="line">&gt;&gt; thetaVec</span><br><span class="line"></span><br><span class="line">thetaVec &#x3D;</span><br><span class="line"></span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   1</span><br><span class="line">   ...</span><br></pre></td></tr></table></figure>
<p>我们可以看到<code>thetaVec</code>是一个$231×1$的向量，这里包含了所有矩阵的元素：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; size(thetaVec)</span><br><span class="line">ans &#x3D;</span><br><span class="line"></span><br><span class="line">   231     1</span><br></pre></td></tr></table></figure>
<p>如果我想重新得到我最初的三个矩阵，我可以对<code>thetaVec</code>使用<code>reshape</code>命令。</p>
<p>比如，我们可以抽出前110个元素，来重组一个$10×11$的矩阵，即<code>Theta1</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; reshape(thetaVec(1:110),10,11)</span><br><span class="line">ans &#x3D;</span><br><span class="line"></span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br><span class="line">   1   1   1   1   1   1   1   1   1   1   1</span><br></pre></td></tr></table></figure>
<p>我们也可以用同样的方式来取接下来的110个元素，来重组<code>Theta2</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; reshape(thetaVec(111:220),10,11)</span><br><span class="line">ans &#x3D;</span><br><span class="line"></span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br><span class="line">   2   2   2   2   2   2   2   2   2   2   2</span><br></pre></td></tr></table></figure>
<p>最后再抽出221到231的元素，重组<code>Theta3</code>：</p>
<h2 id="-1"><a href="#-1" class="headerlink" title=""></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; reshape(thetaVec(221:231),1,11)</span><br><span class="line">ans &#x3D;</span><br><span class="line"></span><br><span class="line">   3   3   3   3   3   3   3   3   3   3   3</span><br></pre></td></tr></table></figure></h2><p>为了使这个过程更形象，下面我们来看怎样将这一方法应用于我们的学习算法：</p>
<p>假设你有一些初始参数值:$Θ^{(1)}$，$Θ^{(2)}$，$Θ^{(3)}$。</p>
<p>我们要做的是取出这些参数，并且将它们展开为一个长向量作为<code>initialTheta</code>，带入<code>fminunc</code>函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fminunc(@costFunction, initialTheta, options)</span><br></pre></td></tr></table></figure>
<p>我们要做的另一件事是执行代价函数<code>@costFunction</code>，实现算法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function [jval, gradientVec] &#x3D; costFunction(thetaVec)</span><br></pre></td></tr></table></figure>
<p>代价函数<code>costFunction</code>传入参数<code>thetaVec</code>，这也是我所有参数的向量，是将所有的参数展开成一个向量的形式。</p>
<p>因此，我要做的第一件事就是通过向量<code>thetaVec</code>使用重组函数<code>reshape</code>来得到$Θ^{(1)}$,$Θ^{(2)}$,$Θ^{(3)}$。</p>
<p>这样我就能执行<strong>向前传播</strong>和<strong>反向传播</strong>来计算出导数$D^{(1)}$,$D^{(2)}$,$D^{(3)}$和代价函数$J(Θ)$。</p>
<p>最后，我可以取出这些导数值，然后让它们保存和我展开的$Θ$值相同的顺序来展开它们(按照$D^{(1)}$,$D^{(2)}$,$D^{(3)}$的顺序)，得到<code>gradientVec</code>，这个值由我的代价函数返回，它可以以一个向量的形式返回这些导数值。</p>
<blockquote>
<p>现在，你应该对怎样进行参数的矩阵表达式和向量表达式之间的转换，有了一个更清晰的认识。</p>
<p>使用矩阵表达式的好处是：当你的参数以矩阵的形式存储时，你在进行正向传播和反向传播时，你会觉得更加方便。当你将参数存储为矩阵时，一个大好处是充分利用了向量化的实现过程。</p>
<p>相反地，向量表达式的优点是如果你有像thetaVec或者DVec这样的矩阵，当你使用一些高级的优化算法时，这些算法通常要求你所有的参数都展开成一个长向量的形式。</p>
</blockquote>
<h2 id="梯度检验（Gradient-Checking）"><a href="#梯度检验（Gradient-Checking）" class="headerlink" title="梯度检验（Gradient Checking）"></a>梯度检验（Gradient Checking）</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking">视频地址</a></p>
<blockquote>
<p>在之前的视频中，我们讨论了如何使用向前传播和反向传播计算神经网络中的导数，但反向传播作为一个有很多细节的算法，在实现的时候会有点复杂，而且有一个不好的方面是在实现反向传播时，会遇到很多细小的错误。所以如果你把它和梯度下降算法或者其他优化算法一起运行时，可能看起来它运行正常，并且你的代价函数$J(Θ)$最后可能在每次梯度下降法迭代时，都会减小，即使在实现反向传播时有一些小错误，可能也会检查不出来。</p>
<p>所以它看起来是$J(Θ)$在减小，但是可能你最后得到的神经网络误差比没有错误的要高，而且你很可能就不知道你的这些结果是这些小错误导致的。那你应该怎么办呢？</p>
<p>有一个想法叫做<strong>梯度检验(Gradient Checking)</strong>可以解决基本所有的问题。我现在每次实现神经网络的反向传播或者类似的梯度下降算法或者其他比较复杂的模型，我都会使用梯度检验，如果你这么做，它会帮你确定并且能很确信你实现的向前传播和反向传播或者其他的什么算法是100%正确的。</p>
<p>在之前的视频中，我一般是让你相信我给出的那些计算就是代价函数的梯度，但一旦你们实现数值梯度检验，也就是这节视频的主题，你就能自己验证你写的代码确实是在计算代价函数$J(Θ)$了。</p>
</blockquote>
<h3 id="梯度检验原理"><a href="#梯度检验原理" class="headerlink" title="梯度检验原理"></a>梯度检验原理</h3><p>看下面这样的例子：</p>
<p><img src="/img/16_10_13/008.png" alt=""></p>
<p>加入我有一个代价函数$J(\theta)$，并且我有一个实数值$\theta$：</p>
<p><img src="/img/16_10_13/009.png" alt=""></p>
<p>假如说我想估计这个函数在这一点的导数，这个导数就等于这样一条直线的斜率：</p>
<p><img src="/img/16_10_13/010.png" alt=""></p>
<p>下面我要用数值计算的方法来计算近似的导数，这个是用数值方法计算近似导数的过程：</p>
<p>我要找到$\theta+ε$和$\theta-ε$这两个点，然后用一条直线把这两点连起来：</p>
<p><img src="/img/16_10_13/011.png" alt=""></p>
<p>然后用这条红线的斜率来作为导数的近似值。在数学上，这条红线的斜率等于两点之间垂直方向的差值除以水平方向的差值：</p>
<p><img src="/img/16_10_13/012.png" alt=""></p>
<p>所以垂直方向上的差值为：</p>
<script type="math/tex; mode=display">
J(\theta+ε) - J(\theta-ε)</script><p>水平方向的差值为：</p>
<script type="math/tex; mode=display">
2ε</script><p>那么我们的近似是这样的：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta}J(\theta)≈
\frac{J(\theta+ε) - J(\theta-ε)}{2ε}</script><blockquote>
<p>通常，我给$ε$取很小的值，比如可能取$ε=10^{-4}$，$ε$的取值在一个很大的范围内都是可行的，实际上，如果你让$ε$非常小，那么数学上上面的式子实际上就是导数。只是我们不用想非常非常小的$ε$，因为可能会产生数值问题，所以我通常让$ε$取$ε=10^{-4}$。</p>
<p>顺便说一下，可能你们见过这种估计导数的公式：$\frac{J(\theta+ε) - J(\theta)}{ε}$，这种方式称为<strong>单侧差分</strong>，而上面的那种方式叫做<strong>双侧差分</strong>。双侧差分给我们了一个稍微精确些的估计，所以，我通常用双侧差分，而不用单侧差分估计。</p>
</blockquote>
<h3 id="Octave中实现梯度检验"><a href="#Octave中实现梯度检验" class="headerlink" title="Octave中实现梯度检验"></a>Octave中实现梯度检验</h3><p>具体地说，你在Octave中实现时，要使用下面这个代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradApprox &#x3D; (J(theta + EPSILON) - J(theta - EPSILON))&#x2F;(2*EPSILON)</span><br></pre></td></tr></table></figure>
<p>你的程序要调用<code>gradApprox</code>来计算这个函数。这个函数会通过这个公式：$\frac{J(\theta+ε) - J(\theta-ε)}{2ε}$，它会给出这点导数的数值估计。</p>
<h3 id="当-theta-是向量时"><a href="#当-theta-是向量时" class="headerlink" title="当$\theta$是向量时"></a>当$\theta$是向量时</h3><p>在之前的例子中，$\theta$是一个实数，接下来让我们来讨论更普遍的一种情况：当$\theta$是一个向量参数的情况。</p>
<p>假如说$\theta$是一个$n$维向量（它可能是我们的神经网络参数$Θ^{1}$，$Θ^{2}$，$Θ^{3}$的展开形式）</p>
<p>所以$\theta$是一个有n个元素的向量。</p>
<script type="math/tex; mode=display">
\theta = [\theta\_{1},\theta\_{2},\theta\_{3},...,\theta\_{n}]</script><hr>
<p>我们可以用类似的想法来估计所有的偏导数项：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta\_{1}}J(\theta)
≈
\frac{
J(\theta\_{1} + ε,\theta\_{2},\theta\_{3},...,\theta\_{n})
-
J(\theta\_{1} - ε,\theta\_{2},\theta\_{3},...,\theta\_{n})
}
{2ε}</script><script type="math/tex; mode=display">
\frac{\partial}{\partial \theta\_{2}}J(\theta)
≈
\frac{
J(\theta\_{1},\theta\_{2} + ε,\theta\_{3},...,\theta\_{n})
-
J(\theta\_{1},\theta\_{2} - ε,\theta\_{3},...,\theta\_{n})
}
{2ε}</script><script type="math/tex; mode=display">...</script><script type="math/tex; mode=display">
\frac{\partial}{\partial \theta\_{n}}J(\theta)
≈
\frac{
J(\theta\_{1},\theta\_{2},\theta\_{3},...,\theta\_{n} + ε)
-
J(\theta\_{1},\theta\_{2},\theta\_{3},...,\theta\_{n} - ε)
}
{2ε}</script><p>分别对$\theta$向量的每个元素使用<strong>双侧差分</strong>来计算导数。</p>
<p>上面的这些公式给出了一个队任意参数求近似偏导数的方法。具体地说，你要实现的是下面这个程序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i &#x3D; 1:n,</span><br><span class="line">	thetaPlus &#x3D; theta;</span><br><span class="line">	thetaPlus(i) &#x3D; thetaPlus(i) + EPSILON;</span><br><span class="line">	thetaMinus &#x3D; theta;</span><br><span class="line">	thetaMinus(i) &#x3D; thetaMinus(i) - EPSILON;</span><br><span class="line">	gradApprox(i) &#x3D; (J(thetaPlus) - J(thetaMinus))&#x2F;(2*EPSILON);</span><br><span class="line">end;</span><br></pre></td></tr></table></figure>
<p>我们把这个用在Octave里，来计算数值导数。</p>
<p>我们实现神经网络时，我们用<code>for</code>循环来计算代价函数对每个网络中的参数的偏导数<code>gradApprox</code>，然后和我们从反向传播得到的导数<code>DVec</code>进行对比，看是否相等或近似于<code>DVec</code>。</p>
<script type="math/tex; mode=display">
Check\ that\ gradApprox≈DVec</script><p>如果这两种计算导数的方法给了你相同的结果，或者非常接近的结果，那么我就非常确信我实现的反向传播是正确的。然后我把这些<code>DVec</code>向量用在梯度下降法，或者其他高级优化算法里。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>最后，我想把所有的东西放在一起，然后告诉你怎样实现这个数值梯度检验。</p>
<p><strong>实现步骤:</strong></p>
<ul>
<li>实现反向传播来计算<code>DVec</code>($D^{(1)}$，$D^{(2)}$，$D^{(3)}$)。</li>
<li>用<code>gradApprox</code>实现数值梯度检验</li>
<li>然后确定<code>DVec</code>和<code>gradApprox</code>给出的结果非常相近</li>
<li>在使用你的代码去学习训练你的网络之前，重要的是要关掉梯度检验，不在使用<code>gradApprox</code>这个数值导数公式（这么做的原因是，这个梯度检验的计算量非常大，它是一个非常慢的计算近似导数的方法。而相对的反向传播算法是一个在计算导数上效率更高的方法。）</li>
</ul>
<p>再次重申一下，在为了训练分类器运行你的算法，做很多次梯度下降或高级优化算法的迭代之前，要确定你不再使用梯度检验的程序，具体来说，如果你在每次的梯度下降法迭代时，都运行数值梯度检验，你的程序会变得非常慢，因为数值检验程序比反向传播算法要慢得多。</p>
<h2 id="随机初始化-Θ"><a href="#随机初始化-Θ" class="headerlink" title="随机初始化$Θ$"></a>随机初始化$Θ$</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/ND5G5/random-initialization">视频地址</a></p>
<blockquote>
<p>这一节将介绍神经网络训练中的最后一个知识点：<strong>随机初始化$Θ$</strong></p>
</blockquote>
<p>当你运行一个算法（例如梯度下降算法，或者其他高级优化算法）时，我们需要给变量$\theta$一些初始值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optTheta &#x3D; fminunc(@costFunction, initialTheta, options)</span><br></pre></td></tr></table></figure>
<p>现在考虑一下梯度下降算法，同样我们需给定$\theta$一些初始值，接下来使用梯度下降方法慢慢地执行这些步骤使其下降，使$J(\theta)$下降到最小。</p>
<p>那么$\theta$的初始值该设置为多少呢？是否可以设置为一个0向量呢？</p>
<script type="math/tex; mode=display">
Set\ \ initialTheta = zeros(n,1)\ ?</script><p>虽然说在逻辑回归时，初始化所有变量为0是可行的，但在训练神经网络时，这样做是不可行的。</p>
<p>以训练下面这个神经网络为例：</p>
<p><img src="/img/16_10_13/013.png" alt=""></p>
<p>照之前所说，将所有变量初始化为0：</p>
<script type="math/tex; mode=display">
Θ\_{ij}^{(l)}=0 \ \ for\ all\ i,j,l.</script><p>如果是这样的话，当初始化下面这些颜色两两相同的权重时，这些权重都被赋予相同的初始值0：</p>
<p><img src="/img/16_10_13/014.png" alt=""></p>
<p>那么这就意味着经过计算后，这两个隐藏单元$a_{1}$，$a_{2}$的值是相同的：</p>
<script type="math/tex; mode=display">
a\_{1}^{(2)}=a\_{2}^{(2)}</script><p>同样的原因，由于权重相同，也可以证明：</p>
<script type="math/tex; mode=display">
δ\_{1}^{(2)}=δ\_{2}^{(2)}</script><p>同时，如果你更深入地挖掘一下，你不难得出这些变量对参数的偏导数满足以下条件：</p>
<p>以这两条红色的权重为例：</p>
<p><img src="/img/16_10_13/015.png" alt=""></p>
<p>即代价函数的关于这两个权重的偏导数是相等的：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial Θ\_{01}^{(1)}}J(Θ)
=
\frac{\partial}{\partial Θ\_{02}^{(1)}}J(Θ)</script><p>这也意味着，一旦更新梯度下降方法，第一个红色权重也会更新，等于学习率乘以这个式子:$\frac{\partial}{\partial Θ_{01}^{(1)}}J(Θ)$</p>
<p>第二条红色权重更新为学习率乘以这个式子：$\frac{\partial}{\partial Θ_{02}^{(1)}}J(Θ)$。</p>
<p>这也就意味着，一旦更新梯度下降，这两条红色权重的值，在最后将互为相等：</p>
<script type="math/tex; mode=display">
Θ\_{01}^{(1)}=Θ\_{02}^{(1)}</script><p>因此，即使权重现在不都为0，但参数的值最后也互为相等。</p>
<p>同样地，即使更新一个梯度下降，下面这两条红色的权重也会互为相等：</p>
<p><img src="/img/16_10_13/016.png" alt=""></p>
<p>同理，最下面的两个权重也会互为相等。</p>
<p>所以每次更新后，两个隐藏单元的输入的对应的参数将是相同的。这就意味着即使经过一次梯度下降的循环后，你会发现两个隐藏单元任然是两个完全相同的输入函数：</p>
<script type="math/tex; mode=display">
a\_{1}^{(2)}=a\_{2}^{(2)}</script><p>这也意味着，这个神经网络并不能计算出什么更有价值的东西。</p>
<p>想象一下，不止有两个隐藏单元，而是有很多的隐藏单元，这就是会导致所有的隐藏单元都在计算相同的特征，这是完全多余的表达，因为这意味着最后的逻辑回归单元只会得到一种特征。这样便阻止了神经网络学习出更有价值的信息。</p>
<h3 id="随机初始化-Θ-引入"><a href="#随机初始化-Θ-引入" class="headerlink" title="随机初始化$Θ$引入"></a>随机初始化$Θ$引入</h3><p>为了解决这个神经网络变量初始化的问题，我们采用<strong>随机初始化</strong>的方法。</p>
<p>具体的说，上面我们说到的所有权重相同的问题，有时被我们也称为<strong>对称权重</strong>。所以随机初始化解决的就是如何<strong>打破这种对称性(Symmetry breaking)</strong>。</p>
<p>所以我们需要做的是对$Θ_{ij}^{(l)}$的每个值进行初始化，范围在$[-ε,ε]$之间（$-ε\le Θ_{ij}^{(l)} \le ε$）。</p>
<p>在Octave中初始化$Θ$：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Theta1 &#x3D; rand(10,11)*(2*INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 &#x3D; rand(1,11)*(2*INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure>
<p>其中<code>rand(10,11)</code>代表一个$10×11$的随机矩阵，这个<code>rand()</code>函数就是用来得到一个任意的随机矩阵的方法，并且所有的值都是介于0到1之间的实数。</p>
<p>因此，如果取0到1之间的一个数和$2ε$相乘再减去$ε$，然后得到的结果就是一个在$[-ε,ε]$之间的数。</p>
<blockquote>
<p>顺便说一句，这里的这个$ε$和在进行梯度检查中用的那个$ε$不是一回事，这也是为什么上面这段代码要使用<code>INIT_EPSILON</code>而不是<code>EPSILON</code>的原因。(在梯度检查中用到的是<code>EPSILON</code>)</p>
</blockquote>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>总的来说，为了训练神经网络，应该对权重进行随机初始化为$[-ε,ε]$之间的值。$ε$是接近于0的小数，然后进行反向传播，执行梯度检查，使用梯度下降或者高级的优化算法，试着使代价函数$J(Θ)$达到最小，从某个随机选取的参数$Θ$开始。通过打破对称性的过程，我们希望梯度下降或者其他高级优化算法可以找到$Θ$的最优值。</p>
<h2 id="神经网络总体回顾：所有算法合体吧！"><a href="#神经网络总体回顾：所有算法合体吧！" class="headerlink" title="神经网络总体回顾：所有算法合体吧！"></a>神经网络总体回顾：所有算法合体吧！</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Wh6s3/putting-it-together">视频地址</a></p>
<blockquote>
<p>这一节中，我们将对神经网络的所有内容进行一个整体回顾，看看这些零散的内容相互之间有怎样的联系，以及神经网络学习算法的总体实现过程。</p>
</blockquote>
<h3 id="第一步，选择一个合适的神经网络结构"><a href="#第一步，选择一个合适的神经网络结构" class="headerlink" title="第一步，选择一个合适的神经网络结构"></a>第一步，选择一个合适的神经网络结构</h3><p>当我们在训练一个神经网络的时候，我们要做的第一件事就是搭建网络的大体框架，这里我说的框架意思是神经元之间的链接模式。我们可能会从以下几种结构中选择：</p>
<p><img src="/img/16_10_13/017.png" alt=""></p>
<ul>
<li>第一种网络结构包含三个输入单元、五个隐藏单元、和四个输出单元。</li>
<li>第二种包含三个输入单元，两组五个隐藏单元作为隐藏层，四个输出单元。</li>
<li>第三种组合是三个输入单元，三组五个隐藏单元作为隐藏层，四个输出单元。</li>
</ul>
<p>这些就是可能选择的结构，每一层可以选择多少个隐藏单元，以及可以选择多少个隐藏层。这些都是你构建时的选择，那么我们该如何做出选择呢？</p>
<p>首先，我们知道我们已经定义了输入单元的数量，一旦你确定了特征集$x^{(i)}$对应的输入单元数目，也就确定了特征$x^{(i)}$的维度，输入单元的数目将会由此确定。</p>
<p>其次如果你正在进行多类别分类，那么输出层的单元数目将会由你分类问题中所要区分的类别个数确定。</p>
<blockquote>
<p>值得一提的是，在多类别分类问题中，若$y$的取值范围是在${1,2,3,..,10}$之间，那么你就有10个可能的分类，别忘了把你的y重新写成向量的形式，例如：</p>
</blockquote>
<script type="math/tex; mode=display">
y=
\begin{bmatrix}
   1 \\\\
   0 \\\\
   0 \\\\
   ... \\\\
   0
  \end{bmatrix}</script><blockquote>
<p>假设现在要表示第5个分类，也就是说$y=5$，那么在你的神经网络中，就不能直接使用数值5来表达，因为这种情况下，神经网络将有10个输出单元，你应该用一个向量来表示：</p>
</blockquote>
<script type="math/tex; mode=display">
y=
\begin{bmatrix}
      0 \\\\
    0 \\\\
    0 \\\\
    0 \\\\
       1 \\\\
       0 \\\\
       0 \\\\
    0 \\\\
    0 \\\\
       0
  \end{bmatrix}</script><p>所以对于输入和输出单元的数目的选择，是比较容易理解的。而对于隐藏层单元的个数，以及隐藏层的数目，我们有一个默认的规则，那就是<strong>只使用单个隐藏层</strong>，所以第一种类型的神经网络架构是最常见的：</p>
<p><img src="/img/16_10_13/018.png" alt=""></p>
<p>或者如果你使用超过一层的隐藏层的话，同样我们也有一个默认规则，那就是<strong>每一个隐藏层通常都应该拥有相同的单元数</strong>。所以后面的两种神经网络结构的隐藏层都拥有相同的单元数：</p>
<p><img src="/img/16_10_13/019.png" alt=""></p>
<p>但实际上通常来说，只使用一层隐藏层的结构是较为合理的默认结构。</p>
<p>而对于隐藏单元的个数，通常情况下<strong>隐藏单元越多越好</strong>，不过我们需要注意的是，如果有大量的隐藏单元，计算量一般会比较大。并且，一般来说，每个隐藏层所包含的单元数量还应该和输入$x$的维度相匹配，也要和特征的数目相匹配。可能隐藏单元的数目和输入特征的数量相同，或者是它的二倍或者三倍、四倍。因此，隐藏单元的数目需要和其他参数相匹配。</p>
<p>一般来说隐藏单元的数目取稍大于输入特征数目都是可以接受的。</p>
<h3 id="训练神经网络的步骤"><a href="#训练神经网络的步骤" class="headerlink" title="训练神经网络的步骤"></a>训练神经网络的步骤</h3><p>接下来，我们就来具体介绍如何实现神经网络的训练过程，下面是训练神经网络的六个步骤：</p>
<ul>
<li>1.构建一个神经网络并且随机初始化权值（<strong>Randomly initialize Weight</strong>）</li>
</ul>
<blockquote>
<p>我们通常把权值初始化为很小的值，接近于0</p>
</blockquote>
<ul>
<li>2.执行向前传播算法，也就是对于神经网络的任意一个输入$x^{(i)}$计算出对应的$h_{Θ}(x^{(i)})$。</li>
<li>3.通过代码计算出代价函数$J(Θ)$</li>
<li>4.执行反向传播算法(<strong>Backprop</strong>)来算出这些偏导数：$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$</li>
</ul>
<blockquote>
<p>具体来说，我们要对所有训练集数据使用一个<code>for</code>循环进行遍历没一个样本（实际上有更复杂的方式来替代<code>for</code>循环来实现，但对于第一次实现神经网络的训练过程，我非常不建议使用<code>for</code>循环以为的方式，因为这种方式更有助于第一次使用时的理解）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i &#x3D; 1:m</span><br></pre></td></tr></table></figure>
<p>注意：下面这部分是在for循环内，但markdown的代码片段不支持显示Letex所以写在了外部：</p>
<p>—-for循环内部开始—-</p>
<p>对样本$(x^{(i)},y^{(i)})$使用向前传播和反向传播算法<br>     (具体来说，就是把输入项带入后，得出每个节点的激励值$a^{(l)}$和delta项$δ^{(l)}$)</p>
<script type="math/tex; mode=display">
△^{(l)} := △^{(l)} + δ^{(l+1)}(a^{(l)})^{T}</script><p>—-for循环内部结束—-</p>
<p>计算出这些$△$的累加值之后，我们将用别的程序来计算出偏导数项：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial Θ\_{jk}^{(l)}}J(Θ)</script></blockquote>
<ul>
<li>5.使用梯度检查来校验结果。用梯度检查来比较这些已经用反向传播算法得到的偏导数值$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$与用数值方法得到的估计值进行比较，来检查，确保这两种方法得到值是基本相近的。</li>
</ul>
<blockquote>
<p>通过梯度检查，我们能确保我们的反向传播算法得到的结果是正确的，但必须要说明的一点是，检查结束后我们需要去掉梯度检查的代码，因为梯度检查计算非常慢。</p>
</blockquote>
<ul>
<li>6.使用一个最优化算法（比如说梯度下降算法或者其他更加高级的优化方法，比如说BFGS算法，共轭梯度法，或者其他一些已经内置到<code>fminunc</code>函数中的方法），将所有这些优化方法和反向传播算法相结合，这样我们就能计算出这些偏导数项的值$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$。</li>
</ul>
<blockquote>
<p>到现在，我们已经知道了如何计算代价函数$J(Θ)$，我们知道了如何使用反向传播算法来计算偏导数$\frac{\partial}{\partial Θ_{jk}^{(l)}}J(Θ)$，那么我们就能使用某个最优化方法来最小化$J(Θ)$关于$Θ$的函数值。</p>
</blockquote>
<p>顺便提一下，对于神经网络代价函数$J(Θ)$是一个非凸函数，因此理论上是能够停留在局部最小值的位置。实际上，梯度下降算法和其他一些高级优化方法理论上都能收敛于局部最小值，但一般来讲这个问题其实并不是什么要紧的事，尽管我们不能保证这些优化算法一定会得到全局最优值，但通常来讲，像梯度下降这类的算法在最小化代价函数$J(Θ)$的过程中，还是表现的很不错的，通常能够得到一个很小的局部最小值，尽管这可能不一定是全局最优值。</p>
<h3 id="梯度下降法在神经网络中的直观理解"><a href="#梯度下降法在神经网络中的直观理解" class="headerlink" title="梯度下降法在神经网络中的直观理解"></a>梯度下降法在神经网络中的直观理解</h3><p>最后，梯度下降算法，似乎对于神经网络来说还是比较神秘的，希望下面这幅图能让你对梯度下降法在神经网络中的应用产生一个更直观的理解：</p>
<p><img src="/img/16_10_13/020.png" alt=""></p>
<p>这实际上有点类似我们早先时候解释梯度下降时的思路：我们有一个代价函数$J(Θ)$，并且在我们的神经网络中有一系列参数值，这里我之写下了两个参数值$Θ_{12}^{(1)}$,$Θ_{11}^{(1)}$，当然实际上在神经网络里，我们可以有很多很多的参数值：$Θ^{(1)}$,$Θ^{(2)}$…。因此我们参数的维度就会很高了，也无法绘制成直观的图像。所以这里我们假设这个神经网络中只有两个参数值：$Θ_{12}^{(1)}$,$Θ_{11}^{(1)}$。</p>
<p>那么代价函数$J(Θ)$度量的就是这个神经网络对训练数据的拟合情况。</p>
<p>所以，如果你取某个参数，比如说在这样一个局部最优值：</p>
<p><img src="/img/16_10_13/021.png" alt=""></p>
<p>这一点的位置所对应的参数$Θ$的情况是对于大部分的训练数据，我的假设函数的输出会非常接近于$y^{(i)}$:</p>
<script type="math/tex; mode=display">
h\_{Θ}(x^{(i)})≈y^{(i)}</script><p>那么如果是这样的话，那么我们的代价函数$J(Θ)$值就会很小。</p>
<p>而反过来，如果我们取这个值：</p>
<p><img src="/img/16_10_13/022.png" alt=""></p>
<p>我们的代价函数$J(Θ)$值就会很大。</p>
<p>因此<strong>梯度下降的原理是我们从某个随机的初始点开始，它将会不停的下降，那么反向传播算法的目的就是算出梯度下降的方向，而梯度下降的过程就是沿着这个方向一点点的下降，一直到我们希望得到的点，这一点就是我们希望找到的局部最优点</strong>。</p>
<p>所以，当你在执行反向传播算法并使用梯度下降或者更高级的优化方法时，上面的图片很好地帮你解释了基本的原理，也就是视图找到某个最优的参数值。这个值使得我们的神经网络的输出值与$y^{(i)}$的实际值（也就是训练集的输出观测值）尽可能的接近</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/09/14/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%94%E5%91%A8%20(1)%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/14/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%BA%94%E5%91%A8%20(1)%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第五周 (1)训练神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-14 01:14:00" itemprop="dateCreated datePublished" datetime="2016-09-14T01:14:00+00:00">2016-09-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/na28E/cost-function">视频地址</a></p>
<blockquote>
<p>在本节课中和后面的几节课中，我将开始讲述一种在给定训练集下为神经网络拟合参数的学习算法。正如我们讨论大多数学习算法一样，我们准备从拟合神经网络参数的代价函数开始讲起。</p>
</blockquote>
<p>以神经网络在分类问题中的应用为例：</p>
<p><img src="/img/16_09_18/001.png" alt=""> </p>
<p>假设我们有一个如上图所示的神经网络结构，然后假设我们有一个这样的训练集：</p>
<script type="math/tex; mode=display">
\begin{aligned}
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})
\end{aligned}</script><p>一共有m个训练样本$(x^{(i)},y^{(i)})$。</p>
<p>用大写字母$L$表示这个神经网络结构的总层数：</p>
<script type="math/tex; mode=display">
L = total\ no.\ of\ layers\ in\ network(神经网络总层数)</script><p>所以，对于左边的网络结构，我们得到$L=4$。</p>
<p>然后，我们准备用$S_{l}$表示第$l$层的神经元的数量，<strong>这其中不包括L层的偏置单元</strong>：</p>
<script type="math/tex; mode=display">
S\_{l}= no.\ of\ units(not\ counting\ bias\ unit)\ in\ layer\ l</script><p>比如说：</p>
<ul>
<li>$S_{1}=3$(也就是输入层)</li>
<li>$S_{2}=5$</li>
<li>$S_{3}=5$</li>
<li>$S_{4}=S_{L}=4$ (因为$L=4$)</li>
</ul>
<p>我们接下来讨论两种分类问题，分别是<strong>二元分类</strong>和<strong>多类别分类</strong>。</p>
<h3 id="二元分类-Binary-classification"><a href="#二元分类-Binary-classification" class="headerlink" title="二元分类(Binary classification)"></a>二元分类(Binary classification)</h3><p>在二元分类中$y$只能是0或者1：</p>
<script type="math/tex; mode=display">
y= 0\ or\ 1</script><p>在这个例子中，我们有一个输出单元（不同于上面的神经网络有4个输出单元）。神经网络的输出会是一个实数：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)\in \mathbb{R}</script><p>输出单元的个数:</p>
<script type="math/tex; mode=display">
S\_{L} = 1</script><blockquote>
<p>在这类问题里，为了简化记法，我会把$K$设为1，这样<strong>你可以把$K$看作输出层的单元数目</strong>。</p>
</blockquote>
<h3 id="多类别分类（Multi-class-classification）"><a href="#多类别分类（Multi-class-classification）" class="headerlink" title="多类别分类（Multi-class classification）"></a>多类别分类（Multi-class classification）</h3><p>在多类别分类问题中，会有$K$个不同的类，比如说如果我们有四类的话，我们就用下面这种表达形式来代表$y$。在这类问题里，我们就会有$K$个输出单元。</p>
<p><img src="/img/16_09_18/002.png" alt=""> </p>
<p>我们的输出假设就是一个$K$维向量：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)\in \mathbb{R}^{K}</script><p>输出单元的个数就是$K$：</p>
<script type="math/tex; mode=display">
S\_{L} = K</script><blockquote>
<p>通常这类问题中，我们都有$K\ge3$，因为如果我们只有两类的话，我们直接使用二元分类法就可以了。因此只有在$K\ge3$的情况下，我们才会使用这种<strong>多类别分类</strong>。</p>
</blockquote>
<h3 id="定义代价函数"><a href="#定义代价函数" class="headerlink" title="定义代价函数"></a>定义代价函数</h3><p>我们在神经网络里，使用的代价函数，应该是逻辑回归里使用的代价函数的一般形式。</p>
<p><strong>逻辑回归的代价函数:</strong></p>
<script type="math/tex; mode=display">
\begin{equation\*}
J(\theta)=-\frac{1}{m}
[
\sum\_{i=1}^m
y^{(i)}logh\_{\theta}(x^{(i)})
+(1-y^{(i)})log(1-h\_{\theta}(x^{(i)}))
]+
\frac{\lambda}{2m}
\sum\_{j=1}^n
\theta^{2}\_{j}
\end{equation*}</script><p>其中$\begin{equation*}<br>\frac{\lambda}{2m}<br>\sum_{j=1}^n<br>\theta^{2}_{j}<br>\end{equation*}$这一项是个额外的正则化项，是一个$j$从1到$n$的求和形式。因为我们并没有把偏置项0正则化。</p>
<p>对于神经网络，我们使用的代价函数是这个式子的一般化形式。</p>
<script type="math/tex; mode=display">
\begin{equation\*}
J(Θ)=-\frac{1}{m}
[
\sum\_{i=1}^m
\sum\_{k=1}^K
y^{(i)}\_{k}log(h\_{Θ}(x^{(i)}))\_{k}
+(1-y^{(i)})\_{k})log(1-(h\_{Θ}(x^{(i)}))\_{k})
]+
\frac{\lambda}{2m}
\sum\_{l=1}^{L-1}
\sum\_{i=1}^{S\_{l}}
\sum\_{j=1}^{S\_{l+1}}
(Θ^{(l)}\_{ji})^{2}
\end{equation*}</script><p>神经网络现在输出了在$K$维的向量$h_{Θ}(x)$：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)\in \mathbb{R}^{K}</script><p>用$(h_{Θ}(x))_{i}$来表示第$i$个输出。</p>
<p>其中$\begin{equation*}\sum_{k=1}^K\end{equation*}$这个求和项是$K$个输出单元的求和，比如你有4个输出单元在神经网络的最后一层，那么这个求和项就是$k$从1到4所对应的每一个逻辑回归算法的代价函数之和。</p>
<p>最后式子中的这一项$\begin{equation*}<br>\frac{\lambda}{2m}<br>\sum_{l=1}^{L-1}<br>\sum_{i=1}^{S_{l}}<br>\sum_{j=1}^{S_{l+1}}<br>(Θ^{(l)}_{ji})^{2}<br>\end{equation*}$类似于我们在逻辑回归里所用的正则化项，这个求和项看起来确实非常复杂，它所做的就是把这些项全部加起来，也就是对所有的$Θ_{ji}^{(l)}$的值都相加。正如我们在逻辑回归里的一样，这里要除去那些对应于偏差值的项。具体来说，我们不把$Θ_{j0}^{(l)}$这些项加进去，这是因为当我们计算神经元的激励值时，我们会有这些项。这些带0的项，类似于偏置单元的项。类比于我们在做逻辑回归的时候，我们就不应该把这些项加入到正规化项里去，因为我们并不想正规化这些项，并把这些项设定为0。 （这里我表示没看懂）</p>
<p>即使我们真的把他们加进去了，也就是说$i$从0加到$S_{l}$依然成立，并且不会有大的差异，但是这个“不把偏差项正规化”的规定可能只是会更常见一些。</p>
<h2 id="反向传播-B-P"><a href="#反向传播-B-P" class="headerlink" title="反向传播(B-P)"></a>反向传播(B-P)</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm">视频地址</a></p>
<blockquote>
<p>这个视频中，我们来讨论一下让代价函数最小化的算法，具体来说，我们将主要讲解<strong>反向传播算法（BP算法）</strong></p>
</blockquote>
<p>下面是我们上一节写好的代价函数：</p>
<p><img src="/img/16_09_18/003.png" alt=""> </p>
<p>我们要做的就是试图找到使得代价函数$J(Θ)$最小的$Θ$值：</p>
<p><img src="/img/16_09_18/004.png" alt=""> </p>
<p>为了使用梯度下降法，我们需要做的就是写好一个通过输入参数$Θ$，然后计算：</p>
<p><img src="/img/16_09_18/005.png" alt=""> </p>
<p>这一节的大部分内容也都是在讲解如何计算真两项的。</p>
<hr>
<p><strong>梯度下降计算</strong></p>
<p><img src="/img/16_09_18/006.png" alt=""> </p>
<p>首先，我们从只有一个训练样本的情况说起，假设我们整个训练集只包含一个训练样本：</p>
<script type="math/tex; mode=display">
(x,y)</script><p>让我们粗看一下，使用这样一个训练样本来计算的顺序。</p>
<p>首先我们用向前传播方法来计算一下在给定输入的时候，假设函数的输出结果：</p>
<script type="math/tex; mode=display">
a^{(1)} = x</script><blockquote>
<p>$a^{(1)}$就是第一层的激励值，也就是输入层</p>
</blockquote>
<script type="math/tex; mode=display">
z^{(2)} = Θ^{(1)}a^{(1)}</script><script type="math/tex; mode=display">
a^{(2)} = g(z^{(2)})</script><blockquote>
<p>这里记得添加偏差项$a_{0}^{(2)}$</p>
</blockquote>
<script type="math/tex; mode=display">
z^{(3)} = Θ^{(2)}a^{(2)}</script><script type="math/tex; mode=display">
a^{(3)} = g(z^{(3)})</script><blockquote>
<p>这里记得添加偏差项$a_{0}^{(3)}$</p>
</blockquote>
<script type="math/tex; mode=display">
z^{(4)} = Θ^{(3)}a^{(3)}</script><script type="math/tex; mode=display">
a^{(4)} = h\_{Θ}(x) = g(z^{(4)})</script><p>通过上面的步骤计算，我们就可以得出假设函数的输出结果了：</p>
<p><img src="/img/16_09_18/007.png" alt=""> </p>
<p>接下来，为了计算导数项，我们将采用一种叫做<strong>反向传播(Backpropagation)</strong>的算法。</p>
<p>反向传播算法从直观上说就是对每一个节点求下面这一个误差项:</p>
<p><img src="/img/16_09_18/008.png" alt=""> </p>
<blockquote>
<p>$δ_{j}^{(l)}$这种形式代表了第$l$层的第$j$个结点的<strong>误差</strong></p>
<p>我们还记得我们使用$a_{j}^{(l)}$来表示第$l$层的第$j$个结点的<strong>激励值</strong>，所以这个$δ$项，在某种程度上就捕捉到了我们在这个神经结点的激励值的误差。所以我们可能希望这个结点的激励值稍微不一样。</p>
</blockquote>
<p>具体来讲，我们用上面的那个四层的神经网络结构做例子：</p>
<p>每一项的输出单元(layer L = 4)</p>
<script type="math/tex; mode=display">
δ\_{j}^{(4)} = a\_{j}^{(4)} - y\_j</script><blockquote>
<p>对于每一个输出单元，我们准备计算$δ$项，所以第四层的第$j$个单元的$δ$就等于<strong>这个单元的激励值减去训练样本里的真实值</strong>。所以$a_{j}^{(4)}$这一项同样可以写成$h_{Θ}(x)_{j}$：</p>
</blockquote>
<script type="math/tex; mode=display">
δ\_{j}^{(4)} = h\_{Θ}(x)\_{j} - y\_j</script><p>顺便说一下，如果你把$δ$、$a$和$y$这三项都看作向量的话，那么上面的式子你也可以写出向量化的实现：</p>
<script type="math/tex; mode=display">
δ^{(4)} = a^{(4)} - y</script><p>这里的$δ^{(4)}$、$a^{(4)}$和$y$都是一个向量，并且向量维数等于输出单元的数目。</p>
<p>所以现在我们计算出网络结构的误差项$δ^{(4)}$，我们下一步就是计算网络中前面几层的误差项$δ$。</p>
<p>这就是$δ^{(3)}$的计算公式：</p>
<script type="math/tex; mode=display">
δ^{(3)} = (Θ^{(3)})^{T}δ^{(4)}.*g'(z^{(3)})</script><blockquote>
<p>这里的点乘$.*$是我们从MATLAB里知道的对y元素的乘法操作，指的是两个向量中元素间对应相乘。</p>
</blockquote>
<p>其中$g’(z^{(3)})$这一项其实是对激励函数$g$在输入值为$z(3)$的时候所求的导数。</p>
<p>如果你稍微会一些微积分的知识，你可以很容易的求得$g’(z^{(3)})$这一项的值是：</p>
<script type="math/tex; mode=display">
a^{(3)}.*(1-a^{(3)})</script><p>这里的$1$是元素都为1的向量。</p>
<p>接下来，你可以应用一个相似的公式来求得$δ^{(2)}$:</p>
<script type="math/tex; mode=display">
δ^{(2)} = (Θ^{(2)})^{T}δ^{(3)}.*g'(z^{(2)})</script><p>值得注意的是，这里我们没有$δ^{(1)}$项，因为第一层是输入层，不存在误差。所以这个例子中，我们的$δ$项就只有第2层和第3层。</p>
<hr>
<p>反向传播法这个名字源于我们从输出层开始计算$δ$项，然后我们返回到上一层计算第三隐藏层的$δ$项，接着我们再往前一步来计算$δ^{(2)}$。</p>
<p>所以说我们是类似于把输出层的误差反向传播给了第3层，然后再传到第二层。这就是反向传播的意思。</p>
<p>最后，这个推导过程是出奇的复杂，但是如果你按照这样几个步骤来计算，就有可能简单直接地完成复杂的数学证明。</p>
<p>如果你忽略标准化所产生的项，我们可以证明我们想要的偏导项，恰好就是下面这个表达式：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial Θ\_{ij}^{(l)}}J(Θ)=
a\_{j}^{(l)}δ\_{i}^{(l+1)}</script><script type="math/tex; mode=display">
(ignore\ λ; if\ λ = 0)</script><blockquote>
<p>这里我们忽略了$λ$，我们将在之后完善这一个关于正则化项。</p>
</blockquote>
<p>所以到现在，我们通过反向传播计算这些$δ$项，可以非常快速的计算出所有参数的偏导数项。</p>
<hr>
<p>好，现在让我们把上面所讲的所有内容整合在一起，然后说说如何实现反向传播算法：</p>
<p>当我们有一个很大的训练样本的时候，而不是像我们例子里这样的一个训练样本。我们是这样做的：</p>
<p>假设我们有m个样本的训练集：</p>
<p>$Training\ set\ {(x^{(1)},y^{(1)}),…,(x^{(m)},y^{(m)})}$</p>
<p>我们要做的第一件事就是设置这些值：</p>
<p><img src="/img/16_09_18/009.png" alt=""> </p>
<blockquote>
<p>这里的$△$其实是大写的$δ$，实际上这些$△_{ij}^{(l)}$将被用来计算偏导数项$\frac{\partial}{\partial Θ_{ij}^{(l)}}J(Θ)$</p>
<p>所以，正如我们接下来看到的，这些$△_{ij}^{(l)}$将被作为累加项，慢慢地增加，以算出这些偏导数。</p>
</blockquote>
<p>下图是我们接下来要执行的一些操作：</p>
<p><img src="/img/16_09_18/010.png" alt=""> </p>
<p>在这里我们将遍历我们的训练集。</p>
<p>我们要做的第一件事就是设定$a^{(1)}$，也就是输入层的激励函数：$a^{(1)} = x^{(i)}$</p>
<p>接下来我们运用正向传播，来计算第2，3，4，…，L层的激励值$a^{l}$。</p>
<p>接下来，我们将用$y^{(i)}$来计算$δ^{(L)}=a^{(L)}-y^{(i)}$</p>
<p>接下来，我们使用反向传播算法来计算$δ^{(L-1)},δ^{(L-2)},…,δ^{(2)}$</p>
<p>最终，我们将用$△{ij}^{(l)}$来积累我们在前面写好的偏导数项：</p>
<script type="math/tex; mode=display">
△\{ij}^{(l)}:=△\{ij}^{(l)} + a\_{j}^{(l)}δ\_{i}^{(l+1)}</script><p>如果你再看一下上面这个表达式，你可以把它写成向量形式：</p>
<p>具体来说，如果你把$△$看做一个矩阵，$ij$代表矩阵中的位置，那么上面的式子我们就可以写成：</p>
<script type="math/tex; mode=display">
△^{(l)}:=△^{(l)} + δ^{(l+1)}(a^{(l)})^{T}</script><p>最后，执行这个for循环体之后我们挑出这个for循环，然后计算下面这些式子：</p>
<script type="math/tex; mode=display">
D\_{ij}^{(l)} := \frac{1}{m}△\{ij}^{(l)} + λΘ\_{ij}^{(l)}    
\ \ \ \ if\ j ≠ 0</script><script type="math/tex; mode=display">
D\_{ij}^{(l)} := \frac{1}{m}△\{ij}^{(l)}
\ \ \ \ if\ j = 0</script><blockquote>
<p>这里我们对$j ≠ 0$ 和 $j = 0$分两种情况来讨论，在$j=0$的情况下对应的是偏差项，所以这也是为什么在$j=0$的情况下没有写额外的标准化项的原因。</p>
</blockquote>
<p>最后，尽管严格的证明对于你来说太复杂，你现在可以说明的是一旦你计算出来了这些，这就正好是代价函数关于每一个参数的偏导数：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial Θ\_{ij}^{(l)}}J(Θ)
= D\_{ij}^{(l)}</script><p>所以，你可以把它用在梯度下降算法，或者其他更高级的算法中。</p>
<h2 id="反向传播算法的直观介绍"><a href="#反向传播算法的直观介绍" class="headerlink" title="反向传播算法的直观介绍"></a>反向传播算法的直观介绍</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/du981/backpropagation-intuition">视频地址</a></p>
<blockquote>
<p>这一节中，将要更加深入的讨论一下反向传播算法的这些复杂的步骤，并且希望给你一个更加直观的感受，理解这些步骤究竟是在做什么。也希望通过这一节，你能理解它至少还是一个合理的算法。</p>
<p>但可能你即使看了这段视频你还是觉得反向传播依然很复杂，这也没关系，其实即使是我（吴恩达）接触了反向传播这么多年了，有时候任然觉得这是一个难以理解的算法，但还是希望这段视频能有些许帮助。</p>
</blockquote>
<h3 id="距离说明神经网络计算过程"><a href="#距离说明神经网络计算过程" class="headerlink" title="距离说明神经网络计算过程"></a>距离说明神经网络计算过程</h3><p>为了更好地理解反向传播算法，我们再来仔细研究一下向前传播的原理。</p>
<p><img src="/img/16_09_18/011.png" alt=""></p>
<p>这里有一个包含两个输入单元（不包括偏差单元）的神经网络，在第二层有两个隐藏单元（不包括偏差单元），第三层也有两个隐藏单元（不包括偏差单元），最后的输出层有一个输出单元。</p>
<h3 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h3><p>为了更清楚的展示向前传播，下图展示了这个神经网络的<strong>向前传播</strong>的运算过程：</p>
<p><img src="/img/16_09_18/012.png" alt=""></p>
<p>事实上，<strong>反向传播</strong>算法的运算过程非常类似于此，只有计算的方向不同而已。</p>
<h3 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h3><p>为了更好的理解反向传播算法的原理，我们把目光转向代价函数：</p>
<p><img src="/img/16_09_18/013.png" alt=""></p>
<p>这个代价函数对应的情况是只有一个输出单元，如果我们有不止一个输出单元的话，只需要对所有的输出单元进行一次求和运算。</p>
<p>请注意这组训练样本$x^{(i)}$,$y^{(i)}$，注意这种只有一个输出单元的情况，如果不考虑正则化即$λ=0$，因此最后的正则化项就没有了。</p>
<p><img src="/img/16_09_18/014.png" alt=""></p>
<p>这个求和运算括号里面与第i个训练样本对应的代价项，也就是说$(x^{(i)},y^{(i)})$对应的代价项，将有下面这个式子决定：</p>
<script type="math/tex; mode=display">
cost(i) = y^{(i)}log h\_{Θ}(x^{(i)}) + (1 - y^{(i)})logh\_{Θ}(x^{(i)})</script><p>而这个代价函数所扮演的角色可以看做是平方误差，当然，如果你愿意，你可以把$cost(i)$想象成：</p>
<script type="math/tex; mode=display">
cost(i)≈(h\_{Θ}(x^{(i)})-y^{(i)})^{2}</script><p>因此，这里的$cost(i)$表征了该神经网络是否能准确地预测样本i的值，也就是输出值，和实际观测值$y^{(i)}$的接近程度。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>现在我们来看看反向传播是怎么做的。</p>
<p>一种直观的理解是反向传播算法就是在计算所有这些$δ$项：</p>
<script type="math/tex; mode=display">
δ\_{j}^{(l)} = "error" \ of \ cost \ for \ a\_{j}^{(l)} \ (unit \ j \ in \ layer \ l).</script><p>并且我们可以把它们看作是这些激励值的“<strong>误差</strong>”(注意这些激励值是第l层中的第j项)。</p>
<p>更正式一点的说法是$δ$项实际上是关于$z_{j}^{(l)}$的偏微分，也就是cost函数关于我们计算出的输入项的加权和，也就是$z$项的偏微分:</p>
<script type="math/tex; mode=display">
δ\_{j}^{(l)} = \frac{\partial}{\partial z\_{j}^{(l)}}cost(i) \ \ \ \ (for \ j \ge 0 )</script><p>其中：</p>
<script type="math/tex; mode=display">
cost(i) = y^{(i)}log h\_{Θ}(x^{(i)}) + (1 - y^{(i)})logh\_{Θ}(x^{(i)})</script><p>如果我们观察该神经网络内部的话，把这些$z_{j}^{(l)}$项稍微改一点点，那就将影响到神经网络的输出，并且最终会改变代价函数的值。</p>
<p>因此，它们度量着我们对神经网络的权值做多少的改变，对中间的计算量影响是多少，进一步对整个神经网络的输出$h(x)$影响多少，以及对整个的代价影响多少。</p>
<p>可能刚才讲的偏微分的这种理解不太容易理解，没关系，不用偏微分的思想，我们同样也可以理解。</p>
<p>我们再深入一点，研究一下反向传播的过程，对于输入层，如果我们设置$δ$项，假设我们进行第i个训练样本，那么：</p>
<script type="math/tex; mode=display">
δ\_{1}^{(4)}=y^{(i)}-a^{(4)}\_{1}</script><p>接下来我们要对这些值进行反向传播，算出$δ_{1}^{(3)}$、$δ_{2}^{(3)}$，然后同样的再进行下一层的反向传播，算出$δ_{1}^{(2)}$、$δ_{2}^{(2)}$。</p>
<p>举个例子:</p>
<p>接下来，我们来看看如何计算$δ_{2}^{(2)}$。</p>
<p>我要对一些权值进行标记:</p>
<p><img src="/img/16_09_18/016.png" alt=""> </p>
<p>实际上，我们要做的是我们要用下一层的$δ$值和权值相乘，然后加上另一个$δ$值和权值相乘的结果。也就是说，它其实是$δ$值的加权和。权值是这些对应边的强度。</p>
<p><img src="/img/16_09_18/017.png" alt=""> </p>
<p>计算过程是：</p>
<script type="math/tex; mode=display">
δ\_{2}^{(2)}=Θ\_{12}^{(2)}δ\_{1}^{(3)} + Θ\_{22}^{(2)}δ\_{2}^{(3)}</script><p>再看看另一个例子：</p>
<p>如果想要计算$δ_{2}^{(3)}$的值，计算过程也是类似的：</p>
<script type="math/tex; mode=display">
δ\_{2}^{(3)}=Θ\_{12}^{(3)}δ\_{1}^{(4)}</script><p>另外顺便提一下，目前为止我写的$δ$值仅仅是隐藏层中的没有包括偏差单元:”+1”的。包不包括偏差单元取决于你如何实现这个反向传播算法，你也可以对这些偏差单元计算$δ$的值，这些偏差单元总是取为”+1”的值。</p>
<p>通常来说，我在执行反向传播的时候，我是算出了这些偏差单元的$δ$值，但我通常忽略掉它们，而不是把它们带入计算，因为它们其实并不是计算那些微积分的必要部分，</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/09/11/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%9B%9B%E5%91%A8%20(3)%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/11/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%9B%9B%E5%91%A8%20(3)%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第四周 (3)神经网络应用实例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-11 19:22:00" itemprop="dateCreated datePublished" datetime="2016-09-11T19:22:00+00:00">2016-09-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="神经网络应用实例"><a href="#神经网络应用实例" class="headerlink" title="神经网络应用实例"></a>神经网络应用实例</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/rBZmG/examples-and-intuitions-i">视频地址 part1</a></p>
<p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/solUx/examples-and-intuitions-ii">视频地址 part2</a></p>
<blockquote>
<p>接下来将通过一个具体的例子来解释神经网络是如何计算关于输入的复杂的非线性函数的。</p>
</blockquote>
<h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h3><p>考虑下面的问题：</p>
<p>我们有二进制的输入特征$x_{1}$和$x_{2}$，它们的取值要么是0，要么是1。这个例子中，我画出了两个正样本和两个负样本。</p>
<p><img src="/img/16_09_11/001.png" alt=""></p>
<p>但你可以认为这是更复杂的学习问题的简化版本。在这个复杂问题中，我们可能在右上角有一堆正样本，在右下方有一堆用圆圈来表示的负样本。</p>
<p><img src="/img/16_09_11/002.png" alt=""></p>
<p>我们想要做到的就是有一个非线性的决策边界来区分正负样本：</p>
<p><img src="/img/16_09_11/003.png" alt=""></p>
<p>那么，神经网络是如何做到的呢？为了描述方便，我继续使用上面二进制输入特征的例子。</p>
<p>具体来讲，我们要计算的目标函数:</p>
<script type="math/tex; mode=display">y=x\_{1} XNOR x\_{2}</script><blockquote>
<p>求同或(都为真或都为假时，结果为真，否则结果为假)</p>
</blockquote>
<p>或者也可以写作：</p>
<script type="math/tex; mode=display">NOT(y=x\_{1} XOR x\_{2})</script><blockquote>
<p>求异或(都为真或都为假时，结果为假，否则结果为真)再取反</p>
</blockquote>
<h3 id="AND、OR、NOT的实现"><a href="#AND、OR、NOT的实现" class="headerlink" title="AND、OR、NOT的实现"></a>AND、OR、NOT的实现</h3><h4 id="AND"><a href="#AND" class="headerlink" title="AND"></a>AND</h4><p>为了解释神经网络模型如何来拟合这种训练集。我们先讲解一个稍微简单一些的神经网络，它拟合了<strong>“且运算”(AND)</strong>：</p>
<p><img src="/img/16_09_11/004.png" alt=""></p>
<p>假设我们有二进制输入$x_{1}$和$x_{2}$，目标函数是$y=x_{1}ANDx_{2}$，那么我们怎样得到一个具有单个神经元的神经网络来计算这个<strong>逻辑与</strong>呢？为了做到这一点，我们也需要画出偏置单元（即下图中+1的单元）：</p>
<p><img src="/img/16_09_11/005.png" alt=""></p>
<p>接下来让我给这个网络分配一些权重(参数)：</p>
<p><img src="/img/16_09_11/006.png" alt=""></p>
<p>所以我的假设函数是：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)=g(-30 + 20x\_{1} + 20x\_{2})</script><p>这里$Θ^{(1)}_{10}$就是$-30$、$Θ^{(2)}_{11}$就是$20$、$Θ^{(3)}_{12}$就是$20$。</p>
<p>接下来介绍一下这个小神经元是怎样计算的。</p>
<p>回忆一下激励函数$g(z)$看起来是这样的：</p>
<p><img src="/img/16_09_11/007.png" alt=""></p>
<p>再来看看我们的假设在各种情况下的输出：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-30)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
</tbody>
</table>
</div>
<p>这就是逻辑“与”的计算结果。</p>
<hr>
<h4 id="OR"><a href="#OR" class="headerlink" title="OR"></a>OR</h4><p>下面的神经网络使用同样的原理实现了“或”的功能：</p>
<p><img src="/img/16_09_11/008.png" alt=""></p>
<p>假设函数为：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)=g(-10 + 20x\_{1} + 20x\_{2})</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(30)\approx1$</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h4 id="NOT"><a href="#NOT" class="headerlink" title="NOT"></a>NOT</h4><p>下面的神经网络使用同样的原理实现了“非”的功能：</p>
<p><img src="/img/16_09_11/009.png" alt=""></p>
<p>假设函数为：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)=g(10 - 20x\_{1})</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="一个更复杂的例子"><a href="#一个更复杂的例子" class="headerlink" title="一个更复杂的例子"></a>一个更复杂的例子</h3><p>下面的神经网络使用同样的原理实现了“$(NOTx_{1})AND(NOTx_{2})$”的功能：</p>
<p><img src="/img/16_09_11/010.png" alt=""></p>
<p>假设函数为：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)=g(10 - 20x\_{1} - 20x\_{2})</script><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(10)\approx1$</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$g(-10)\approx0$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">$g(-30)\approx0$</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h3 id="求解-XNOR"><a href="#求解-XNOR" class="headerlink" title="求解    XNOR"></a>求解    XNOR</h3><p>接下来我们使用上面求解的以下三个神经网络，就可以来运算$x_{1}XNORx_{2}$了：</p>
<p><img src="/img/16_09_11/011.png" alt=""></p>
<p>为了拟合$x_{1}XNORx_{2}$的非线性的样本分布：</p>
<p><img src="/img/16_09_11/001.png" alt=""></p>
<p>我们可以构建以下神经网络的隐藏层：</p>
<p><img src="/img/16_09_11/012.png" alt=""></p>
<p>对应的真值表如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$a_{1}^{(2)}$</th>
<th style="text-align:center">$a_{2}^{(2)}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center"><strong>1</strong></td>
<td style="text-align:center"><strong>0</strong></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p>有了$a_{1}^{(2)}$、$a_{2}^{(2)}$后，我们加入偏置单元，然后就可以得到输出层了：</p>
<p><img src="/img/16_09_11/012.png" alt=""></p>
<p>最终的真值表如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$x_{1}$</th>
<th style="text-align:center">$x_{2}$</th>
<th style="text-align:center">$a_{1}^{(2)}$</th>
<th style="text-align:center">$a_{2}^{(2)}$</th>
<th style="text-align:center">$h_{Θ}(x)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center"><strong>1</strong></td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>0</strong></td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center"><strong>1</strong></td>
</tr>
</tbody>
</table>
</div>
<p>通过一个含有输入层、隐藏层、输出层的神经网络，我们最终拟合了$x_{1}XNORx_{2}$。</p>
<blockquote>
<p>更一般的理解是：在输入层中，我们有原始输入值，然后我们建立了一个隐藏层，用来计算稍微复杂一些的输入量的函数，然后通过添加另一个层我们得到了一个更复杂一点的函数，这就是神经网络可以计算较复杂函数的某种直观解释。</p>
<p>我们知道，当层数很多的时候，你有一个相对简单的输入量的函数作为第二层，而第三层可以建立在此基础上来计算更加复杂一些的函数，然后再下一层，又可以计算再复杂一些的函数：</p>
</blockquote>
<p><img src="/img/16_09_11/014.png" alt=""></p>
<h3 id="手写识别的展示"><a href="#手写识别的展示" class="headerlink" title="手写识别的展示"></a>手写识别的展示</h3><p>接下来，将展示一段视频，来源于<strong>阳乐昆(Yann LeCun)</strong>，他是一名教授，供职于纽约大学，也是神经网络研究早期的奠基者之一，也是这一领域 大牛。他的很多理论和想法现在都已经被应用于各种各样的产品和应用中，遍布全世界。所以我想向大家展示一段他早期工作中的视频，这段视频中，他使用神经网络算法进行手写数字的识别。你也许记得，这门课刚开始的时候，我说过关于神经网络的一个早期成就，就是应用神经网络读取邮政编码，以帮助我们进行邮递。那么这便是其中的一项尝试。</p>
<p>视频说明：</p>
<p><img src="/img/16_09_11/015.png" alt=""></p>
<p>视频如下：</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/yxuRnBEczUU" frameborder="0" allowfullscreen></iframe>

<h2 id="神经网络的多类别分类"><a href="#神经网络的多类别分类" class="headerlink" title="神经网络的多类别分类"></a>神经网络的多类别分类</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/gFpiW/multiclass-classification">视频地址</a></p>
<blockquote>
<p>这一节将介绍<strong>如何使用神经网络做多类别分类</strong>。</p>
<p>在多类别分类中，通常有不止一个类别需要我们去区分，在上一节最后的视频中，我们提到了有关手写数字识别的问题，这实际上正是一个多类别分类的问题。因为识别数字从0到9，正好是10个类别。</p>
</blockquote>
<p>我们处理多类别分类的方法实际上是基于一个<strong>多神经网络算法</strong>而延伸出来的。</p>
<p>让我们来看看下面这个例子：</p>
<p>还是一个有关计算机视觉识别的例子，就像我之前介绍过的识别汽车的例子一样，但与之不同的是现在我们希望处理的是四个类别的分类问题，四个类别分别是行人、轿车、摩托车、卡车：</p>
<p><img src="/img/16_09_11/016.png" alt=""></p>
<p>任意给出一副图片，我们需要知道图片上是这四个类别中的哪一个。</p>
<p>对于这样的一个问题，我们的做法是，<strong>建立一个具有四个输出单元的神经网络</strong>：</p>
<p><img src="/img/16_09_11/017.png" alt=""></p>
<p>也就是说，此时神经网络的输出是一个思维向量。因此现在的输出需要用一个向量来表示，这个向量中有四个元素，而我们要做的是对第一个输出元素进行分辨图片上是不是一个行人(Pedestrian)，然后对第二个元素分辨它是不是一辆轿车(Car)，对第三个元素分辨它是不是摩托车(Motorcycle)，对第四个元素分辨它是不是一辆卡车(Truck)。</p>
<p>因此，如果图上是行人的话，我希望输出结果是：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)\approx
\begin{bmatrix}
1 \\\\
0 \\\\
0 \\\\
0
\end{bmatrix}</script><p>如果图上是轿车的话，我希望输出结果是：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)\approx
\begin{bmatrix}
0 \\\\
1 \\\\
0 \\\\
0
\end{bmatrix}</script><p>如果图上是摩托车的话，我希望输出结果是：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)\approx
\begin{bmatrix}
0 \\\\
0 \\\\
1 \\\\
0
\end{bmatrix}</script><p>如果图上是卡车的话，我希望输出结果是：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)\approx
\begin{bmatrix}
0 \\\\
0 \\\\
0 \\\\
1
\end{bmatrix}</script><p>所以，这和我们介绍逻辑回归时讨论过的一对多方法其实是一样的，只不过现在我们有四个逻辑回归的分类器，而我们需要对每一个分类器都分别进行识别分类。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2016/09/07/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%9B%9B%E5%91%A8%20(2)%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2016/09/07/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%9B%9B%E5%91%A8%20(2)%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第四周 (2)神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2016-09-07 23:42:00" itemprop="dateCreated datePublished" datetime="2016-09-07T23:42:00+00:00">2016-09-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 10:04:18" itemprop="dateModified" datetime="2021-02-11T10:04:18+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="神经网络的表示I"><a href="#神经网络的表示I" class="headerlink" title="神经网络的表示I"></a>神经网络的表示I</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/ka3jK/model-representation-i">视频地址</a></p>
<p>这节课介绍神经网络如何表示我们的假设。</p>
<h3 id="神经网络起源"><a href="#神经网络起源" class="headerlink" title="神经网络起源"></a>神经网络起源</h3><p>神经网络是在模拟大脑中的神经元时发明的，因此，要解释如何表示模型假设，我们先来看看神经元在大脑中是什么样的。我们的大脑中充满了这样的神经元：</p>
<p><img src="/img/16_09_07/001.png" alt=""></p>
<p>神经元是大脑中的细胞，其中有两点值得我们注意，一是神经元有一个细胞主体(Cell body)，二是神经元有一定数量的输入神经，这些输入神经叫做<strong>树突(Dendrite)</strong>。可以把它们想象成输入电线，它们接收来自其他神经元的信息。神经元的输出神经叫做<strong>轴突(Axon)</strong>，这些输出神经是用来给其他神经元传递信号或者传送信息的。</p>
<p>简而言之，神经元是一个计算单元，它从<strong>输入神经</strong>接受一定数目的信息，并做一些计算，然后将结果通过它的<strong>轴突</strong>传送到大脑中的其他神经元。</p>
<p>下面是一组神经元的示意图：</p>
<p><img src="/img/16_09_07/002.png" alt=""></p>
<p>神经元利用微弱的电流进行沟通，这些弱电流也称作<strong>动作电位</strong>(其实就是一些微弱的电流)。</p>
<p>所以如果神经元想要传递一个消息，它就会通过它的轴突发送一段微弱的电流，给其他神经元。这就是<strong>轴突</strong>连接到输入神经元(另一个神经元的<strong>树突</strong>)。接下来这个神经元接收这条消息，做一些计算，它有可能会反过来将在轴突上的自己的消息传给其他神经元。这就是所有人类思考的模型：我们的神经元把自己收到的消息进行计算，并向其他神经元传递消息。</p>
<blockquote>
<p>顺便说一下，这也是我们感觉和肌肉运转的原理。如果你想要活动一块肌肉，就会触发一个神经元给你的肌肉，发送脉冲，并引起你的肌肉收缩。如果一些感官，比如说眼睛，想要向大脑传递一个消息，那么它就是像这样发送电脉冲给大脑的。</p>
</blockquote>
<h3 id="神经网络逻辑单元"><a href="#神经网络逻辑单元" class="headerlink" title="神经网络逻辑单元"></a>神经网络逻辑单元</h3><p>在一个计算机的神经网络里，我们将使用一个非常简单的模型来模拟神经元的工作：</p>
<p><img src="/img/16_09_07/003.png" alt=""></p>
<p>我们将神经元模拟成一个逻辑单元。上图中，黄色的圆圈，你可以理解为类似神经元的东西，然后我们通过它的树突(或者说是它的输入神经)传递给它一些信息。然后神经元做一些计算，并通过它的输出神经(即它的轴突)输出计算结果。</p>
<p>这里的$h_{\theta}(x)$通常值的是：</p>
<script type="math/tex; mode=display">
\begin{align\*}
h\_{\theta}(x)
= \frac{1}{1+e^{-\theta^{T}x}}
\end{align*}</script><p>其中$x$和$\theta$指的是我们的参数向量：</p>
<p><img src="/img/16_09_07/004.png" alt=""></p>
<p>这就是一个简单的模拟神经元的模型。</p>
<hr>
<p>当我绘制一个神经网络时，有时会额外增加一个$x_{0}$的输入节点，这个$x_{0}$节点有时也被称作<strong>偏置单位(或偏置神经元)</strong>。但由于$x_{0}=1$，有时我是不会画出它的，这取决于它是否对例子有利。</p>
<h3 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h3><p>在神经网络中，有时我们会听到“一个有Sigmoid函数或者Logistic函数作为<strong>激励函数(activation function)</strong>的人工神经元”这样的话。其实这里所指的<strong>激励函数(activation function)</strong>只是对类似非线性函数$g(z)$的另一个术语称呼：</p>
<script type="math/tex; mode=display">
\begin{align\*}
g(z)
= \frac{1}{1+e^{-z}}
\end{align*}</script><script type="math/tex; mode=display">
\begin{align\*}
z
= \theta^{T}x
\end{align*}</script><blockquote>
<p>在之前我们一值称$\theta$为模型的参数，但在神经网络的文献里，有时你可能会看到人们谈论一个模型的<strong>权重(weight)</strong>，这个<strong>权重(weight)</strong>其实和模型的参数$\theta$是一个东西。</p>
</blockquote>
<hr>
<h3 id="解读神经网络"><a href="#解读神经网络" class="headerlink" title="解读神经网络"></a>解读神经网络</h3><h4 id="输入层，输出层，隐藏层"><a href="#输入层，输出层，隐藏层" class="headerlink" title="输入层，输出层，隐藏层"></a>输入层，输出层，隐藏层</h4><p>神经网络其实就是这些不同的神经元组合在一起的集合：</p>
<p><img src="/img/16_09_07/005.png" alt=""></p>
<p>具体来说：</p>
<ul>
<li><p>上图中有三个输入单元：$x_{1}$,$x_{2}$和$x_{3}$，当然我们也可以加入值为1的$x_{0}$。</p>
</li>
<li><p>中间有三个神经元：$a_{1}^{(2)}$，$a_{2}^{(2)}$和$a_{3}^{(2)}$，同理，你可以可以加上值永远为1的偏置单元$a_{0}^{(2)}$。</p>
</li>
<li><p>然后，我们在最右侧有第三层，第三层的这个节点输出了假设函数$h(x)$的计算结果$h_{Θ}(x)$。</p>
</li>
</ul>
<p>用神经网络的术语来说，第一层也被称为<strong>输入层</strong>，因为我们在这一层输入我们的特征项$x_{1}$，$x_{2}$和$x_{3}$。</p>
<p>最后一层，也被称为<strong>输出层</strong>，因为这一层的神经元会输出假设函数的最终计算结果$h_{Θ}(x)$。</p>
<p>中间层，也被称为<strong>隐藏层</strong>，在监督学习中，你能看到输入，也能看到输出，而隐藏层的值在你的训练过程中是看不到的，它的值不是$x$也不是$y$，所以我们叫它隐藏层。神经网络可以有不止一个的隐藏层。<strong>在神经网络中，任何一个非输入层且非输出层，就被称为隐藏层</strong>。</p>
<h4 id="神经网络运行原理"><a href="#神经网络运行原理" class="headerlink" title="神经网络运行原理"></a>神经网络运行原理</h4><p><img src="/img/16_09_07/005.png" alt=""></p>
<p>接下来，让我们来逐步分析，上图所呈现的神经网络的计算步骤。</p>
<p>首先需要说明以下两个符号的含义：</p>
<ul>
<li>$a_{i}^{(j)}$ 表示第j层的第i个神经元。</li>
</ul>
<blockquote>
<p>具体来说$a_{1}^{2}$表示的是第2层的第1个激励，即隐藏层的第一个激励。</p>
<p>所谓<strong>激励(activation)</strong>是指由一个具体神经元读入计算并输出的值。</p>
</blockquote>
<ul>
<li>$Θ^{(j)}$ 表示层与层之间权重的<strong>参数矩阵</strong>(或者叫<strong>权重矩阵</strong>)(比如说从第一层到第二层、或者从第二层到第三层的作用)。</li>
</ul>
<p>具体来说，$a_{1}^{(2)}$的值的计算是这样的：</p>
<script type="math/tex; mode=display">
a\_{1}^{(2)} = g(Θ\_{10}^{(1)}x\_{0} +
Θ\_{11}^{(1)}x\_{1} + 
Θ\_{12}^{(1)}x\_{2} + 
Θ\_{13}^{(1)}x\_{3}
)</script><blockquote>
<p>不要忘记这里的$g$函数是S型函数(或者说是S激励函数，也叫作逻辑激励函数)。</p>
</blockquote>
<p>我们可以把隐藏层的三个神经元的计算结果都写出来：</p>
<script type="math/tex; mode=display">
a\_{1}^{(2)} = g(Θ\_{10}^{(1)}x\_{0} +
Θ\_{11}^{(1)}x\_{1} + 
Θ\_{12}^{(1)}x\_{2} + 
Θ\_{13}^{(1)}x\_{3}
)</script><script type="math/tex; mode=display">
a\_{2}^{(2)} = g(Θ\_{20}^{(1)}x\_{0} +
Θ\_{21}^{(1)}x\_{1} + 
Θ\_{22}^{(1)}x\_{2} + 
Θ\_{23}^{(1)}x\_{3}
)</script><script type="math/tex; mode=display">
a\_{3}^{(2)} = g(Θ\_{30}^{(1)}x\_{0} +
Θ\_{31}^{(1)}x\_{1} + 
Θ\_{32}^{(1)}x\_{2} + 
Θ\_{33}^{(1)}x\_{3}
)</script><p>这里我们有三个输入单元和三个隐藏单元，这样一来，参数矩阵$Θ^{(1)}$控制了我们来自三个输入单元到三个隐藏单元的映射。因此$Θ^{(1)}$的维数是$R^{3×4}$的(考虑$x_{0}$的情况下)矩阵。</p>
<p>更一般的，如果一个神经网络在第$j$层有$s_{j}$个单元，在$j+1$层有$s_{j+1}$个单元，那么第$j$层的参数矩阵$Θ^{(j)}$的维度就是$s_{j+1}×(s_{j} + 1)$</p>
<p>以上我们讨论了三个隐藏单位是怎么计算它们的值的，最后，在输出层，我们还有一个单元，它用来计算$h_{Θ}(x)$：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x) = a\_{1}^{(3)}=g(
Θ\_{10}^{(2)}a\_{0}^{(2)} +
Θ\_{11}^{(2)}a\_{1}^{(2)} + 
Θ\_{12}^{(2)}a\_{2}^{(2)} + 
Θ\_{13}^{(2)}a\_{3}^{(2)}
)</script><p>以上就是从数学上对一个人工神经网络的定义。</p>
<h2 id="神经网络的表示II"><a href="#神经网络的表示II" class="headerlink" title="神经网络的表示II"></a>神经网络的表示II</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/Hw3VK/model-representation-ii">视频地址</a></p>
<blockquote>
<p>这节课，将介绍如何高效地进行计算，并展示一个向量化的实现方法。更重要的是让你们明白神经网络的好处所在，并且介绍它是如何帮助我们学习复杂的非线性假设的。</p>
</blockquote>
<h3 id="前向传播-forward-propagation-的向量化实现"><a href="#前向传播-forward-propagation-的向量化实现" class="headerlink" title="前向传播(forward propagation)的向量化实现"></a>前向传播(forward propagation)的向量化实现</h3><p>以这个神经网络为例：</p>
<p><img src="/img/16_09_07/006.png" alt=""></p>
<p>上一节我们介绍过假设函数的计算方式，以及上面这些方程的含义。接下来，我要定义一些额外的项，来简化上面的式子的表示，比如我要把上面第一行(用来表示第一个隐藏层输出的值)：</p>
<script type="math/tex; mode=display">
a\_{1}^{(2)} = g(Θ\_{10}^{(1)}x\_{0} +
Θ\_{11}^{(1)}x\_{1} + 
Θ\_{12}^{(1)}x\_{2} + 
Θ\_{13}^{(1)}x\_{3}
)</script><p>改写为：</p>
<script type="math/tex; mode=display">
a\_{1}^{(2)} = g(z^{(2)}\_{1})</script><p>可见：</p>
<script type="math/tex; mode=display">
z^{(2)}\_{1} = Θ\_{10}^{(1)}x\_{0} +
Θ\_{11}^{(1)}x\_{1} + 
Θ\_{12}^{(1)}x\_{2} + 
Θ\_{13}^{(1)}x\_{3}</script><p>其中$z^{(2)}_{1}$的上标$^{(2)}$表示<strong>神经网络的第二层(即这里的隐藏层)</strong>，下标$_{1}$表示<strong>当前层的第一个元素</strong>。</p>
<p>我们通过同样的方式定义$a_{2}^{(2)}$和$a_{3}^{(2)}$。</p>
<script type="math/tex; mode=display">
a\_{2}^{(2)} = g(z^{(2)}\_{2})</script><script type="math/tex; mode=display">
a\_{3}^{(2)} = g(z^{(2)}\_{3})</script><p>因此，这些$z$值都是$x_{0}$、$x_{1}$、$x_{2}$和$x_{3}$的加权线性组合，然后带入一个特定的神经元，</p>
<p>如果你仔细观察表达式中红色方框内的区域：</p>
<p><img src="/img/16_09_07/007.png" alt=""></p>
<p>你会发现，这里其实是一个矩阵的乘法运算：</p>
<script type="math/tex; mode=display">
Θ^{(1)}x</script><p>这样一来，我们就能将神经网络的计算，向量化了。</p>
<p>具体而言，我们定义特征向量$x$：</p>
<script type="math/tex; mode=display">
x=
\begin{bmatrix}
x\_{0} \\\\
x\_{1} \\\\
x\_{2} \\\\
x\_{3} 
\end{bmatrix}</script><blockquote>
<p>其中$x_{0}=1$</p>
</blockquote>
<p>并定义$z^{(2)}$：</p>
<script type="math/tex; mode=display">
z^{(2)}=
\begin{bmatrix}
z^{(2)}\_{1} \\\\
z^{(2)}\_{2} \\\\
z^{(2)}\_{3}
\end{bmatrix}</script><blockquote>
<p>注意，这里的$z^{(2)}$是一个三维向量。</p>
</blockquote>
<p>我们只需要两个步骤，就可以计算出$a^{(2)}$向量了：</p>
<script type="math/tex; mode=display">
z^{(2)}=Θ^{(1)}x</script><script type="math/tex; mode=display">
a^{(2)}=g(z^{(2)})</script><blockquote>
<p>注意，这里的$a^{(2)}$也是一个三维向量，这里的激励函数$g()$将对$z^{(2)}$中的每个元素进行计算。<br>说明一下，在输入层虽然我们有$x$输入，但我们可以把这些输入想象成是第一层的激励。所以可以定义$a^{(1)}=x$，这样就有了向量$a^{(1)}$。</p>
</blockquote>
<p>将向量$x$替换为向量$a^{(1)}$:</p>
<script type="math/tex; mode=display">
z^{(2)}=Θ^{(1)}a^{(1)}</script><hr>
<p>现在，就我目前所写的，我得到了$a^{(2)}_{1}$、$a^{(2)}_{2}$和$a^{(2)}_{3}$的值，但是我同样还需要隐藏层的偏置单元$a^{(2)}_{0}$，这个额外的偏执单元值为1：</p>
<script type="math/tex; mode=display">
a^{(2)}\_{0}=1</script><p>加上偏置单元后，向量$a^{(2)}$的长度变成了4：</p>
<script type="math/tex; mode=display">
a^{(2)}\in \mathbb{R}^{4}</script><p>最后，为了计算实际输出值：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x) = a\_{1}^{(3)}=g(
Θ\_{10}^{(2)}a\_{0}^{(2)} +
Θ\_{11}^{(2)}a\_{1}^{(2)} + 
Θ\_{12}^{(2)}a\_{2}^{(2)} + 
Θ\_{13}^{(2)}a\_{3}^{(2)}
)</script><p>我们计算出代表$Θ_{10}^{(2)}a_{0}^{(2)} +Θ_{11}^{(2)}a_{1}^{(2)} + Θ_{12}^{(2)}a_{2}^{(2)} + Θ_{13}^{(2)}a_{3}^{(2)}$的$z^{(3)}$，并将其带入激励函数，最后就能得出$h(x)$的值了：</p>
<script type="math/tex; mode=display">
z^{(3)}=Θ^{(2)}a^{(2)}</script><script type="math/tex; mode=display">
h\_{Θ}(x)=a^{(3)}=g(z^{(3)})</script><p>这就是计算$h_{Θ}(x)$的过程，也称为<strong>前向传播(forward propagation)</strong>。</p>
<blockquote>
<p>这样的命名是因为我们从输入层的激励开始，然后进行前向传播给隐藏层，并计算隐藏层的激励，然后我们继续向前传播，并计算出层的激励，这个从输入层，到隐藏层，再到输出层，依次计算激励的过程，叫<strong>前向传播</strong>。</p>
</blockquote>
<h3 id="神经网络与逻辑回归的对比，以及学习非线性假设的原理"><a href="#神经网络与逻辑回归的对比，以及学习非线性假设的原理" class="headerlink" title="神经网络与逻辑回归的对比，以及学习非线性假设的原理"></a>神经网络与逻辑回归的对比，以及学习非线性假设的原理</h3><p>这种前向传播也可以帮助我们了解神经网络的原理，以及解释为什么神经网络可以帮助我们学习非线性假设。</p>
<p>例如下面这个神经网络：</p>
<p><img src="/img/16_09_07/008.png" alt=""></p>
<p>我们暂时盖住左边部分，看右侧这一部分：</p>
<p><img src="/img/16_09_07/009.png" alt=""></p>
<p>这看起来很像逻辑回归，在逻辑回归中，我们就用这一个逻辑回归单元来预测$h(x)$的值。具体来说：</p>
<script type="math/tex; mode=display">
h\_{Θ}(x)=g(Θ\_{10}^{(2)}a\_{0}^{(2)} + Θ\_{11}^{(2)}a\_{1}^{(2)} + Θ\_{12}^{(2)}a\_{2}^{(2)} + Θ\_{13}^{(2)}a\_{3}^{(2)})</script><p>这很像一个很标准的逻辑回归模型，不同之处在于这里使用的是大写的$Θ$而不是小写的$\theta$，神经网络的输入特征值是通过隐藏层计算的。即，神经网络所做的工作看起来就像是逻辑回归，但是它不是使用$x_{1}$、$x_{2}$、$x_{3}$作为输入特征，而是使用$a_{1}^{(2)}$、$a_{2}^{(2)}$、$a_{3}^{(2)}$。</p>
<hr>
<p><img src="/img/16_09_07/010.png" alt=""></p>
<p>然而有趣的是，特征项$a_{1}^{(2)}$、$a_{2}^{(2)}$、$a_{3}^{(2)}$是通过输入的函数来学习的。具体来说，就是从第一层(Layer1)映射到第二层(Layer2)的函数。这个函数由其他一组参数$Θ^{(1)}$决定。所以，在神经网络中，它没有用输入特征$x_{1}$、$x_{2}$、$x_{3}$来训练逻辑回归，而是训练逻辑回归的输入:$a_{1}^{(2)}$、$a_{2}^{(2)}$、$a_{3}^{(2)}$。</p>
<p>可以想象，如果在$Θ^{(1)}$中选择不同的参数有时可以学习到一些很有趣的，和很复杂的特征，就可以得到一个比使用原始输入$x_{1}$、$x_{2}$、$x_{3}$时得到的假设更好的假设。</p>
<p>你也可以使用多项式作为输入，例如：$x_{1}x_{2}$、$x_{2}x_{3}$等作为输入项。这个算法都可以灵活的快速学习任意的特征项，并把最后的结果通过最后一个单元的逻辑回归输出出来。</p>
<h3 id="神经网络的架构"><a href="#神经网络的架构" class="headerlink" title="神经网络的架构"></a>神经网络的架构</h3><p>你还可以用其他类型的图来表示神经网络。神经网络中神经元相连接的方式，称为<strong>神经网络的架构(Architecture)</strong>。</p>
<p>下面是另外一个神经网络架构的例子：</p>
<p><img src="/img/16_09_07/011.png" alt=""></p>
<p>在这个神经网络中，第一层被称为输入层，第四层任然是我们的输出层。其中，第二层和第三层都是隐藏层。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="DannyLee"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">DannyLee</p>
  <div class="site-description" itemprop="description">愿你的努力终取得成果</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">138</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
