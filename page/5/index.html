<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="愿你的努力终取得成果">
<meta property="og:type" content="website">
<meta property="og:title" content="圣巢">
<meta property="og:url" content="http://example.com/page/5/index.html">
<meta property="og:site_name" content="圣巢">
<meta property="og:description" content="愿你的努力终取得成果">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="DannyLee">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>圣巢</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8d12c5d1bc83189640335b2363468a74";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">圣巢</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/03/05/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E3%80%90tf.contrib.learn%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E3%80%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/03/05/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E3%80%90tf.contrib.learn%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E3%80%91/" class="post-title-link" itemprop="url">【Tensorflow r1.0 文档翻译】【tf.contrib.learn快速入门】</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-03-05 15:19:58" itemprop="dateCreated datePublished" datetime="2017-03-05T15:19:58+00:00">2017-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>TensorFlow的高级机器学习API(tf.contrib.learn)使得各种机器学习模型的配置、训练和评估都变得简单。在本教程中，你将使用tf.contrib.learn来构建一个<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Artificial_neural_network">神经网络</a>分类器，并且在<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris数据集</a>上进行训练，以达到通过萼片/花瓣几何来预测花的种类。您将编写代码以执行以下五个步骤：</p>
<ul>
<li>1.加载格包含Iris的训练和测试数据的CSV到TensorFlow数据集中。</li>
<li>2.构建一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier">神经网络分类器</a>。</li>
<li>3.使用训练数据拟合模型</li>
<li>4.评估模型的准确性</li>
<li>5.分类新样品</li>
</ul>
<blockquote>
<p><strong>注意：</strong>在开始本教程之前，请确认在你的机器上已经<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/index">安装了TensorFlow</a>。</p>
</blockquote>
<h2 id="完整的神经网络源代码"><a href="#完整的神经网络源代码" class="headerlink" title="完整的神经网络源代码"></a>完整的神经网络源代码</h2><p>这里是神经网络分类器的完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Data sets</span></span><br><span class="line">IRIS_TRAINING = <span class="string">&quot;iris_training.csv&quot;</span></span><br><span class="line">IRIS_TEST = <span class="string">&quot;iris_test.csv&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load datasets.</span></span><br><span class="line">training_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TRAINING,</span><br><span class="line">    target_dtype=np.<span class="built_in">int</span>,</span><br><span class="line">    features_dtype=np.float32)</span><br><span class="line">test_set = tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename=IRIS_TEST,</span><br><span class="line">    target_dtype=np.<span class="built_in">int</span>,</span><br><span class="line">    features_dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify that all features have real-value data</span></span><br><span class="line">feature_columns = [tf.contrib.layers.real_valued_column(<span class="string">&quot;&quot;</span>, dimension=<span class="number">4</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build 3 layer DNN with 10, 20, 10 units respectively.</span></span><br><span class="line">classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,</span><br><span class="line">                                            hidden_units=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">10</span>],</span><br><span class="line">                                            n_classes=<span class="number">3</span>,</span><br><span class="line">                                            model_dir=<span class="string">&quot;/tmp/iris_model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit model.</span></span><br><span class="line">classifier.fit(x=training_set.data,</span><br><span class="line">               y=training_set.target,</span><br><span class="line">               steps=<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate accuracy.</span></span><br><span class="line">accuracy_score = classifier.evaluate(x=test_set.data,</span><br><span class="line">                                     y=test_set.target)[<span class="string">&quot;accuracy&quot;</span>]</span><br><span class="line">print(<span class="string">&#x27;Accuracy: &#123;0:f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Classify two new flower samples.</span></span><br><span class="line">new_samples = np.array(</span><br><span class="line">    [[<span class="number">6.4</span>, <span class="number">3.2</span>, <span class="number">4.5</span>, <span class="number">1.5</span>], [<span class="number">5.8</span>, <span class="number">3.1</span>, <span class="number">5.0</span>, <span class="number">1.7</span>]], dtype=<span class="built_in">float</span>)</span><br><span class="line">y = <span class="built_in">list</span>(classifier.predict(new_samples, as_iterable=<span class="literal">True</span>))</span><br><span class="line">print(<span class="string">&#x27;Predictions: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(y)))</span><br></pre></td></tr></table></figure>
<p>接下来，我们将详细介绍这部分代码的细节。</p>
<h2 id="将Iris-CSV数据加载到TensorFlow"><a href="#将Iris-CSV数据加载到TensorFlow" class="headerlink" title="将Iris CSV数据加载到TensorFlow"></a>将Iris CSV数据加载到TensorFlow</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris数据集</a>包含150行数据，包括来自三个相关鸢尾花物种，其中每个物种包含50个样本：山鸢尾，杂色鸢尾和维吉尼亚鸢尾。</p>
<p><img src="/img/17_03_05/001.jpg" alt=""></p>
<p><strong>从左到右依次是：<a target="_blank" rel="noopener" href="https://commons.wikimedia.org/w/index.php?curid=170298">山鸢尾</a>(by <a target="_blank" rel="noopener" href="https://commons.wikimedia.org/wiki/User:Radomil">Radomil</a>, CC BY-SA 3.0),<a target="_blank" rel="noopener" href="https://commons.wikimedia.org/w/index.php?curid=248095">杂色鸢尾</a>(by <a target="_blank" rel="noopener" href="https://commons.wikimedia.org/wiki/User:Dlanglois">Dlanglois</a>, CC BY-SA 3.0)和<a target="_blank" rel="noopener" href="https://www.flickr.com/photos/33397993@N05/3352169862">维吉尼亚鸢尾</a>(by <a target="_blank" rel="noopener" href="https://www.flickr.com/photos/33397993@N05">Frank Mayfield</a>, CC BY-SA 2.0)</strong></p>
<p>每行包含每个花样品的以下数据：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Sepal">萼片</a>长度，萼片宽度，<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Petal">花瓣</a>长度，花瓣宽度以及花的品种。花的品种用整数表示，0表示山鸢尾，1表示杂色鸢尾，2表示维吉尼亚鸢尾。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">萼片长度(Sepal Length)</th>
<th style="text-align:left">萼片宽度(Sepal Width)</th>
<th style="text-align:left">花瓣长度(Petal Length)</th>
<th style="text-align:left">花瓣宽度(Petal Width)</th>
<th style="text-align:left">品种(Species)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">5.1</td>
<td style="text-align:left">3.5</td>
<td style="text-align:left">1.4</td>
<td style="text-align:left">0.2</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">4.9</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">1.4</td>
<td style="text-align:left">0.2</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">4.7</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">1.3</td>
<td style="text-align:left">0.2</td>
<td style="text-align:left">0</td>
</tr>
<tr>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
</tr>
<tr>
<td style="text-align:left">7.0</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">4.7</td>
<td style="text-align:left">1.4</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">6.4</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">4.5</td>
<td style="text-align:left">1.5</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">6.9</td>
<td style="text-align:left">3.1</td>
<td style="text-align:left">4.9</td>
<td style="text-align:left">1.5</td>
<td style="text-align:left">1</td>
</tr>
<tr>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
<td style="text-align:left">…</td>
</tr>
<tr>
<td style="text-align:left">6.5</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">5.2</td>
<td style="text-align:left">2.0</td>
<td style="text-align:left">2</td>
</tr>
<tr>
<td style="text-align:left">6.2</td>
<td style="text-align:left">3.4</td>
<td style="text-align:left">5.4</td>
<td style="text-align:left">2.3</td>
<td style="text-align:left">2</td>
</tr>
<tr>
<td style="text-align:left">5.9</td>
<td style="text-align:left">3.0</td>
<td style="text-align:left">5.1</td>
<td style="text-align:left">1.8</td>
<td style="text-align:left">2</td>
</tr>
</tbody>
</table>
</div>
<p>在本教程中，Iris数据已随机分到两个单独的CSV中：</p>
<ul>
<li>一个包含了120个样本的训练集(<a target="_blank" rel="noopener" href="http://download.tensorflow.org/data/iris_training.csv">iris_training.csv</a>)</li>
<li>一个包含了30个样本的测试集(<a target="_blank" rel="noopener" href="http://download.tensorflow.org/data/iris_test.csv">iris_test.csv</a>)</li>
</ul>
<p>将这些文件放在与Python代码相同的目录中。</p>
<p>首先导入TensorFlow和numpy：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import absolute_import</span><br><span class="line">from __future__ import division</span><br><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure>
<p>接下来，使用<code>learn.datasets.base</code>中的<a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/contrib/learn/python/learn/datasets/base.py"><code>load_csv_with_header()</code></a>方法将训练和测试集装入数据集。<code>load_csv_with_header()</code>方法需要三个必需的参数：</p>
<ul>
<li><code>filename</code>，CSV文件的路径</li>
<li><code>target_dtype</code>，接受数据集的目标值的<a target="_blank" rel="noopener" href="http://docs.scipy.org/doc/numpy/user/basics.types.html"><code>numpy</code>数据类型</a>。</li>
<li><code>features_dtype</code>，接受数据集的特征值的<a target="_blank" rel="noopener" href="http://docs.scipy.org/doc/numpy/user/basics.types.html"><code>numpy</code>数据类型</a>。</li>
</ul>
<p>在这里，target（你训练模型预测的值）是花种，它是一个从0-2的整数，所以对应的适当的<code>numpy</code>数据类型是<code>np.int</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Data sets</span><br><span class="line">IRIS_TRAINING &#x3D; &quot;iris_training.csv&quot;</span><br><span class="line">IRIS_TEST &#x3D; &quot;iris_test.csv&quot;</span><br><span class="line"></span><br><span class="line"># Load datasets.</span><br><span class="line">training_set &#x3D; tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename&#x3D;IRIS_TRAINING,</span><br><span class="line">    target_dtype&#x3D;np.int,</span><br><span class="line">    features_dtype&#x3D;np.float32)</span><br><span class="line">test_set &#x3D; tf.contrib.learn.datasets.base.load_csv_with_header(</span><br><span class="line">    filename&#x3D;IRIS_TEST,</span><br><span class="line">    target_dtype&#x3D;np.int,</span><br><span class="line">    features_dtype&#x3D;np.float32)</span><br></pre></td></tr></table></figure>
<p>tf.contrib.learn中的<code>Dataset</code>是<a target="_blank" rel="noopener" href="https://docs.python.org/2/library/collections.html#collections.namedtuple">命名元组</a>；您可以通过<code>data</code>和<code>target</code>字段访问特征数据和目标值。这里<code>training_set.data</code>和<code>training_set.target</code>分别包含训练集的特征数据和目标值；<code>test_set.data</code>和<code>test_set.target</code>分别包含测试集的特征数据和目标值。</p>
<p>在后面的<a href="#将DNN分类器用于Iris训练数据">“将DNN分类器用于Iris训练数据”</a>中，你将使用到<code>training_set.data</code>和<code>training_set.target</code>来训练你的模型，在<a href="#评估模型精度">“评估模型精度”</a>中，你将使用<code>test_set.data</code>和<code>test_set.target</code>。但首先，你需要在下一节中构建你的模型。</p>
<h2 id="构建一个深度神经网络分类器"><a href="#构建一个深度神经网络分类器" class="headerlink" title="构建一个深度神经网络分类器"></a>构建一个深度神经网络分类器</h2><p>tf.contrib.learn提供了一系列预定义的模型，叫做<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators">Estimator</a>s。通过Estimator，您可以对您的数据很方便的进行训练和评估操作，达到“开箱即用”的效果。在这里，您将配置一个深层神经网络分类器模型以适应Iris数据。通过使用tf.contrib.learn，你可以仅仅使用一行代码就实例化一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier"><code>tf.contrib.learn.DNNClassifier</code></a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Specify that all features have real-value data</span><br><span class="line">feature_columns &#x3D; [tf.contrib.layers.real_valued_column(&quot;&quot;, dimension&#x3D;4)]</span><br><span class="line"></span><br><span class="line"># Build 3 layer DNN with 10, 20, 10 units respectively.</span><br><span class="line">classifier &#x3D; tf.contrib.learn.DNNClassifier(feature_columns&#x3D;feature_columns,</span><br><span class="line">                                            hidden_units&#x3D;[10, 20, 10],</span><br><span class="line">                                            n_classes&#x3D;3,</span><br><span class="line">                                            model_dir&#x3D;&quot;&#x2F;tmp&#x2F;iris_model&quot;)</span><br></pre></td></tr></table></figure>
<p>上面的代码首先定义了模型的特征列，它指定了数据集中特征的数据类型。所有的特征数据都是连续的，因此<code>tf.contrib.layers.real_valued_column</code>是用于构造特征列的适当函数。数据集中有四个特征（萼片宽度，萼片高度，花瓣宽度和花瓣高度），因此相应的尺寸必须设置为4以保存所有数据。</p>
<p>然后，代码使用以下参数创建<code>DNNClassifier</code>模型：</p>
<ul>
<li><code>feature_columns=feature_columns</code>。上面定义的一组特征</li>
<li><code>hidden_units=[10, 20, 10]</code>。三个<a target="_blank" rel="noopener" href="http://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw">隐藏层</a>分别包含10，20，10个神经元。</li>
<li><code>n_classes=3</code>。三个目标类，代表三个鸢尾物种。</li>
<li><code>model_dir=/tmp/iris_model</code>。TensorFlow在模型训练期间将保存检查点数据的目录。有关使用TensorFlow进行日志记录和监视的更多信息，请见<a href="">使用tf.contrib.learn记录和监视的基本知识</a>。</li>
</ul>
<h2 id="将DNN分类器用于Iris训练数据"><a href="#将DNN分类器用于Iris训练数据" class="headerlink" title="将DNN分类器用于Iris训练数据"></a>将DNN分类器用于Iris训练数据</h2><p>现在，你已经配置好了你的DNN<code>classifier</code>模型，你可以使用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/BaseEstimator#fit"><code>fit</code></a>方法来将Iris训练数据应用到分类器上。将特征数据（<code>training_set.data</code>），目标值（<code>training_set.target</code>）和要训练的步数（这里是<code>2000</code>）作为参数传递：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Fit model</span><br><span class="line">classifier.fit(x&#x3D;training_set.data, y&#x3D;training_set.target, steps&#x3D;2000)</span><br></pre></td></tr></table></figure>
<p>模型的状态保存在<code>classifier</code>(分类器)中，这意味着如果你喜欢，你可以迭代地训练。上面的代码执行效果等同于下面这两行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classifier.fit(x&#x3D;training_set.data, y&#x3D;training_set.target, steps&#x3D;1000)</span><br><span class="line">classifier.fit(x&#x3D;training_set.data, y&#x3D;training_set.target, steps&#x3D;1000)</span><br></pre></td></tr></table></figure>
<p>但是，如果您希望在训练时跟踪模型，则可能需要使用TensorFlow<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/monitors">monitor</a>(监视器)来执行日志操作。关于这个主题更多的内容，请见教程<a href="">使用tf.contrib.learn记录和监视的基本知识</a>。</p>
<h2 id="评估模型精度"><a href="#评估模型精度" class="headerlink" title="评估模型精度"></a>评估模型精度</h2><p>你已经将Iris的训练数据适配到了<code>DNNClassifier</code>模型上；现在，您可以使用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/BaseEstimator#evaluate"><code>evaluate</code></a>方法在Iris测试数据上检查其准确性。像<code>fit</code>（拟合）一样，<code>evaluate</code>（评估操作）将特征数据和目标值作为参数，并返回带有评估结果的<code>dict</code>（字典）。以下代码通过了Iris测试数据-<code>test_set.data</code>和<code>test_set.target</code>来评估和打印结果的准确性：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score &#x3D; classifier.evaluate(x&#x3D;test_set.data, y&#x3D;test_set.target)[&quot;accuracy&quot;]</span><br><span class="line">print(&#39;Accuracy: &#123;0:f&#125;&#39;.format(accuracy_score))</span><br></pre></td></tr></table></figure>
<p>运行全部的脚本，并检查结果的准确度：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.966667</span><br></pre></td></tr></table></figure>
<p>您的准确度结果可能有所不同，但应该是高于90％的。这对于相对较小的数据集是一个不错的结果了！</p>
<h2 id="分类新样品"><a href="#分类新样品" class="headerlink" title="分类新样品"></a>分类新样品</h2><p>使用评估器的<code>predict()</code>方法来分类一个新的样本。例如，说你有这两个新的花样本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">萼片长度(Sepal Length)</th>
<th style="text-align:left">萼片宽度(Sepal Width)</th>
<th style="text-align:left">花瓣长度(Petal Length)</th>
<th style="text-align:left">花瓣宽度(Petal Width)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">6.4</td>
<td style="text-align:left">3.2</td>
<td style="text-align:left">4.5</td>
<td style="text-align:left">1.5</td>
</tr>
<tr>
<td style="text-align:left">5.8</td>
<td style="text-align:left">3.1</td>
<td style="text-align:left">5.0</td>
<td style="text-align:left">1.7</td>
</tr>
</tbody>
</table>
</div>
<p>你可以用以下代码预测他们的物种：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Classify two new flower samples.</span><br><span class="line">new_samples &#x3D; np.array(</span><br><span class="line">    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype&#x3D;float)</span><br><span class="line">y &#x3D; list(classifier.predict(new_samples, as_iterable&#x3D;True))</span><br><span class="line">print(&#39;Predictions: &#123;&#125;&#39;.format(str(y)))</span><br></pre></td></tr></table></figure>
<p><code>predict()</code>方法返回了一个预测数组，每个样本对应其中的一个结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Prediction: [1 2]</span><br></pre></td></tr></table></figure>
<p>该模型预测的结果为：第一个样本是杂色鸢尾，第二个样本是维吉尼亚鸢尾。</p>
<h2 id="其他资源"><a href="#其他资源" class="headerlink" title="其他资源"></a>其他资源</h2><ul>
<li>有关tf.contrib.learn的更多参考资料，请参阅<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_guides/python/contrib.learn">官方API文档</a>。</li>
<li>要了解有关使用tf.contrib.learn创建线性模型的更多信息，请参阅<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/linear">使用TensorFlow的大型线性模型</a>。</li>
<li>要使用tf.contrib.learn API构建自己的评估器，请查看<a target="_blank" rel="noopener" href="http://terrytangyuan.github.io/2016/07/08/understand-and-build-tensorflow-estimator/">TensorFlow中的Building Machine Learning Estimator</a>。</li>
<li>要在浏览器中尝试神经网络建模和可视化，请查看<a target="_blank" rel="noopener" href="http://playground.tensorflow.org/">Deep Playground</a>。</li>
<li>有关神经网络的更高级教程，请参阅<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/deep_cnn">卷积神经网络</a>和<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/recurrent">循环神经网络</a>。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/03/03/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91Tensorflow%E5%8E%9F%E7%90%86%E5%AF%BC%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/03/03/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91Tensorflow%E5%8E%9F%E7%90%86%E5%AF%BC%E8%AE%BA/" class="post-title-link" itemprop="url">【Tensorflow r1.0 文档翻译】Tensorflow原理导论</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-03-03 21:19:58" itemprop="dateCreated datePublished" datetime="2017-03-03T21:19:58+00:00">2017-03-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>代码：<a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/">tensorflow/examples/tutorials/mnist/</a></p>
<p>这篇教程的目的是为了展示如何使用TensorFlow来训练并评估一个简单的<strong>前馈神经网络(feed-forward neural network)</strong>用来识别MNIST手写数字数据集。本教程的目标读者是有兴趣使用TensorFlow的有经验的机器学习用户。</p>
<p>这部分教程不是为了教授普通的机器学习。</p>
<p>请确保您已按照说明<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/index">安装了TensorFlow</a>。</p>
<h2 id="教程文件"><a href="#教程文件" class="headerlink" title="教程文件"></a>教程文件</h2><p>本教程引用以下文件：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">文件</th>
<th style="text-align:left">目标</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py"><code>mnist.py</code></a></td>
<td style="text-align:left">构建一个完全连接的MNIST模型的代码。</td>
</tr>
<tr>
<td style="text-align:left"><a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py"><code>fully_connected_feed.py</code></a></td>
<td style="text-align:left">利用下载的数据集训练构建好的MNIST模型的主要代码，以数据反馈字典（feed dictionary）的形式作为输入模型。</td>
</tr>
</tbody>
</table>
</div>
<p>只需要运行<code>fully_connected_feed.py</code>文件，就可以开启训练：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python fully_connected_feed.py</span><br></pre></td></tr></table></figure>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>MNIST是机器学习中的经典问题。这个问题是查看28x28像素的手写数字灰度图像，并确定图像表示的数字，数字范围是0到9。</p>
<p><img src="/img/17_03_03/001.png" alt=""></p>
<p>更多的信息，参加<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">Yann LeCun’s MNIST page</a>或者<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/">Chris Olah’s visualizations of MNIST</a>。</p>
<h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>在<code>run_training()</code>方法的开始部分，<code>input_data.read_data_sets()</code>方法会确保你的本地训练文件夹中，已经下载了正确的数据，然后将这些数据解压并返回一个含有<code>DataSet</code>实例的字典。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_sets &#x3D; input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong><code>fake_data</code>标记是用于单元测试的，读者可以不必理会。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">数据集</th>
<th style="text-align:left">目标</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>data_sets.train</code></td>
<td style="text-align:left">55000图像和标签，用于初级训练。</td>
</tr>
<tr>
<td style="text-align:left"><code>data_sets.validation</code></td>
<td style="text-align:left">5000图像和标签，用于迭代验证训练准确性。</td>
</tr>
<tr>
<td style="text-align:left"><code>data_sets.test</code></td>
<td style="text-align:left">10000图像和标签，用于最终测试训练的准确性。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="输入和占位符"><a href="#输入和占位符" class="headerlink" title="输入和占位符"></a>输入和占位符</h3><p><code>placeholder_inputs()</code>方法创建了两个<code>tf.placeholder</code>操作，用于定义输入的形状。形状参数中包含<code>batch_size</code>值，后续还会将实际的训练样本传入图中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">images_placeholder &#x3D; tf.placeholder(tf.float32, shape&#x3D;(batch_size,</span><br><span class="line">                                                       mnist.IMAGE_PIXELS))</span><br><span class="line">labels_placeholder &#x3D; tf.placeholder(tf.int32, shape&#x3D;(batch_size))</span><br></pre></td></tr></table></figure>
<p>在训练的循环代码的下方，传入的整个图像和标签数据集会被切片，以符合每一个操作所设置的<code>batch_size</code>值，占位符操作将会填补以符合这个<code>batch_size</code>值。然后使用<code>feed_dict</code>参数，将数据传入<code>sess.run()</code>函数。</p>
<h2 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h2><p>在为数据创建占位符之后，就可以运行<code>mnist.py</code>文件，经过三阶段的模式函数操作：<code>inference()</code>， <code>loss()</code>，和<code>training()</code>。图表就构建完成了。</p>
<ul>
<li>1.<code>inference()</code>-尽可能地构建好图表，满足促使神经网络向前反馈并做出预测的要求。</li>
<li>2.<code>loss()</code>-往inference图表中添加生成损失（loss）所需要的操作（ops）。</li>
<li>3.<code>training()</code>-往损失图表中添加计算并应用梯度（gradients）所需的操作。</li>
</ul>
<p><img src="/img/17_03_03/002.png" alt=""></p>
<h3 id="推理-Inference"><a href="#推理-Inference" class="headerlink" title="推理(Inference)"></a>推理(Inference)</h3><p><code>inference()</code>函数会尽可能地构建图表，做到返回包含了预测结果（output prediction）的Tensor。</p>
<p>它采用图像占位符作为输入，并在其上借助<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">ReLU</a>)激活函数构建一对完全连接层，以及一个有着十个节点、指明了输出logtis模型的线性层。</p>
<p>每个图层都在唯一的<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/name_scope"><code>tf.name_scope</code></a>下创建，创建于该作用域之下的所有元素都将带有其前缀。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#39;hidden1&#39;):</span><br></pre></td></tr></table></figure>
<p>在定义的范围内，由这些层中的每一个使用的权重和偏差被生成为<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Variable"><code>tf.Variable</code></a>实例，具有它们期望的形状：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weights &#x3D; tf.Variable(</span><br><span class="line">    tf.truncated_normal([IMAGE_PIXELS, hidden1_units],</span><br><span class="line">                        stddev&#x3D;1.0 &#x2F; math.sqrt(float(IMAGE_PIXELS))),</span><br><span class="line">    name&#x3D;&#39;weights&#39;)</span><br><span class="line">biases &#x3D; tf.Variable(tf.zeros([hidden1_units]),</span><br><span class="line">                     name&#x3D;&#39;biases&#39;)</span><br></pre></td></tr></table></figure>
<p>例如，当在<code>hidden1</code>范围下创建这些时，赋予权重变量的唯一名称将是“<code>hidden1 / weights</code>”。</p>
<p>每个变量在构建时，都会执行初始化操作。</p>
<p>在大多数情况下，通过<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal"><code>tf.truncated_normal</code></a>函数初始化权重变量，给赋予的shape则是一个二维tensor，其中第一个维度代表该层中权重变量所连接（connect from）的单元数量，第二个维度代表该层中权重变量所连接到的（connect to）单元数量。第一层，名字为<code>hidden1</code>，它的尺寸是<code>[IMAGE_PIXELS, hidden1_units]</code>，因为权重变量将图像输入连接到了<code>hidden1</code>层。<code>tf.truncated_normal</code>初始函数将根据所得到的均值和标准差，生成一个随机分布。</p>
<p>然后，通过<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/zeros"><code>tf.zeros</code></a>函数初始化偏差变量（biases），确保所有偏差的起始值都是0，而它们的形状则是其在该层中所接到的（connect to）单元数量。</p>
<p>图表的三个主要操作，分别是两个<code>tf.nn.relu</code>操作，它们中嵌入了隐藏层所需的<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/matmul"><code>tf.matmul</code></a>；以及logits模型所需的另外一个<code>tf.matmul</code>。三者依次生成，各自的<code>tf.Variable</code>实例则与输入占位符或下一层的输出tensor所连接。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden1 &#x3D; tf.nn.relu(tf.matmul(images, weights) + biases)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden2 &#x3D; tf.nn.relu(tf.matmul(hidden1, weights) + biases)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits &#x3D; tf.matmul(hidden2, weights) + biases</span><br></pre></td></tr></table></figure>
<p>最终，程序会返回包含了输出结果的<code>logits</code>Tensor。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p><code>loss()</code>函数通过添加所需的损失操作，进一步构建图表。</p>
<p>首先，来自<code>labels_placeholder</code>的值将转换为64位整数。然后，添加一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits">tf.nn.sparse_softmax_cross_entropy_with_logits</a>操作，以从<code>labels_placeholder</code>自动生成1-hot标签，并且与<code>inference()</code>函数的输出logits与那些1-hot标签进行比较。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">labels &#x3D; tf.to_int64(labels)</span><br><span class="line">cross_entropy &#x3D; tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    labels&#x3D;labels, logits&#x3D;logits, name&#x3D;&#39;xentropy&#39;)</span><br></pre></td></tr></table></figure>
<p>然后使用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean"><code>tf.reduce_mean</code></a>来求在批量维度（第一维度）上的交叉熵的平均值，作为总损失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; tf.reduce_mean(cross_entropy, name&#x3D;&#39;xentropy_mean&#39;)</span><br></pre></td></tr></table></figure>
<p>然后将包含损失值的张量返回。</p>
<blockquote>
<p><strong>注意：</strong>交叉熵是信息论中的一种理论，它用于描述神经网络的预测结果相对于实际所给定的真实结果的偏差程度。更多的信息，请参阅博文<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-09-Visual-Information/">《可视化信息理论》</a>。</p>
</blockquote>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><code>training()</code>方法通过添加<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降</a>的操作来最小化损失。</p>
<p>首先，它通过<code>loss()</code>方法接受损失tensor，然后传递到<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar"><code>tf.summary.scalar</code></a>，用于在与<code>SummaryWriter</code>（见下文）一起使用时生成事件文件中的摘要值的操作。在这里，它将在每次写出摘要时发出损失的快照值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(&#39;loss&#39;, loss)</span><br></pre></td></tr></table></figure>
<p>接下来，我们实例化一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"><code>tf.train.GradientDescentOptimizer</code></a>，负责按照所要求的学习效率（learning rate）应用梯度下降法（gradients）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate)</span><br></pre></td></tr></table></figure>
<p>之后，我们生成一个单个的变量用于统计全局训练的次数，<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize"><code>tf.train.Optimizer.minimize</code></a>操作被同时用作在系统中更新可训练的权值，以及增加全局步长（global step）。按照惯例，这个操作被称为<code>train_op</code>，TensorFlow会话必须运行的，以便引入一个完整的训练步骤（见下文）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">global_step &#x3D; tf.Variable(0, name&#x3D;&#39;global_step&#39;, trainable&#x3D;False)</span><br><span class="line">train_op &#x3D; optimizer.minimize(loss, global_step&#x3D;global_step)</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>一旦图被构建，它就可以在由<code>fully_connected_feed.py</code>中的用户代码控制的循环中迭代地训练和求值。</p>
<h3 id="图"><a href="#图" class="headerlink" title="图"></a>图</h3><p>在<code>run_training()</code>方法的一开始的部分，是一个python的<code>with</code>命令，这表示所有构建的操作将与默认全局<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Graph"><code>tf.Graph</code></a>实例相关联。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.Graph().as_default():</span><br></pre></td></tr></table></figure>
<p><code>tf.Graph</code>实例是一系列可以作为整体执行的操作。TensorFlow的大部分场景只需要依赖默认图表一个实例即可。</p>
<p>利用多个图表的更加复杂的使用场景也是可能的，但是超出了本教程的范围。</p>
<h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><p>完成全部的构建准备、生成全部所需的操作之后，我们就可以创建一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Session">tf.Session</a>，用于运行图表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess &#x3D; tf.Session()</span><br></pre></td></tr></table></figure>
<p>另外，也可以利用<code>with</code>代码块生成<code>Session</code>，限制作用域：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br></pre></td></tr></table></figure>
<p><code>Session</code>函数中没有传入参数，表明该代码将会依附于（如果还没有创建会话，则会创建新的会话）默认的本地会话。</p>
<p>生成会话之后，所有<code>tf.Variable</code>实例都会立即通过调用各自初始化操作中的<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Session#run"><code>tf.Session.run</code></a>函数进行初始化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Session#run"><code>tf.Session.run</code></a>方法将会运行图表中与作为参数传入的操作相对应的完整子集。在初次调用时，<code>init</code>操作只包含了变量初始化程序<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/group"><code>tf.group</code></a>。图表的其他部分不会在这里，而是在下面的训练循环运行。</p>
<h3 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h3><p>在通过会话来初始化变量后，就可以开始训练了。</p>
<p>训练的每一步都是通过用户代码控制，而能实现有效训练的最简单循环就是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for step in xrange(FLAGS.max_steps):</span><br><span class="line">    sess.run(train_op)</span><br></pre></td></tr></table></figure>
<p>但是，本教程中的例子要更为复杂一点，原因是我们必须把输入的数据根据每一步的情况进行切分，以匹配之前生成的占位符。</p>
<h3 id="向图表提供反馈"><a href="#向图表提供反馈" class="headerlink" title="向图表提供反馈"></a>向图表提供反馈</h3><p>执行每一步时，我们的代码会生成一个反馈字典（feed dictionary），其中包含对应步骤中训练所要使用的样本，这些样本的key就是其所代表的占位符操作。</p>
<p><code>fill_feed_dict</code>函数会查询给定的<code>DataSet</code>，索要下一批次`batch_size的图像和标签，与占位符相匹配的Tensor则会包含下一批次的图像和标签。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">images_feed, labels_feed &#x3D; data_set.next_batch(FLAGS.batch_size,</span><br><span class="line">                                               FLAGS.fake_data)</span><br></pre></td></tr></table></figure>
<p>然后，以占位符作为键，创建一个Python字典对象，值则是其代表的反馈Tensor。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feed_dict &#x3D; &#123;</span><br><span class="line">    images_placeholder: images_feed,</span><br><span class="line">    labels_placeholder: labels_feed,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个字典随后作为<code>feed_dict</code>参数，传入<code>sess.run()</code>函数中，为这一步的训练提供输入样本。</p>
<h3 id="检查状态"><a href="#检查状态" class="headerlink" title="检查状态"></a>检查状态</h3><p>在运行<code>sess.run</code>时，要在代码中明确其需要获取的两个值：<code>[train_op, loss]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for step in xrange(FLAGS.max_steps):</span><br><span class="line">    feed_dict &#x3D; fill_feed_dict(data_sets.train,</span><br><span class="line">                               images_placeholder,</span><br><span class="line">                               labels_placeholder)</span><br><span class="line">    _, loss_value &#x3D; sess.run([train_op, loss],</span><br><span class="line">                             feed_dict&#x3D;feed_dict)</span><br></pre></td></tr></table></figure>
<p>因为要获取这两个值，<code>sess.run()</code>会返回一个有两个元素的元组。其中每一个<code>Tensor</code>对象，对应了返回的元组中的numpy数组，而这些数组中包含了当前这步训练中对应Tensor的值。由于<code>train_op</code>并不会产生输出，其在返回的元祖中的对应元素就是<code>None</code>，所以会被抛弃。但是，如果模型在训练中出现偏差，<code>loss</code> Tensor的值可能会变成NaN，所以我们要获取它的值，并记录下来。</p>
<p>假设训练一切正常，没有出现NaN，训练循环会每隔100个训练步骤，就打印一行简单的状态文本，告知用户当前的训练状态。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if step % 100 &#x3D;&#x3D; 0:</span><br><span class="line">    print &#39;Step %d: loss &#x3D; %.2f (%.3f sec)&#39; % (step, loss_value, duration)</span><br></pre></td></tr></table></figure>
<h3 id="状态可视化"><a href="#状态可视化" class="headerlink" title="状态可视化"></a>状态可视化</h3><p>为了发出<a target="_blank" rel="noopener" href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a>所使用的事件文件（events file），所有的摘要（在这里只有一个）都要在图构建阶段合并至一个Tensor中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary &#x3D; tf.summary.merge_all()</span><br></pre></td></tr></table></figure>
<p>在创建好会话（session）之后，可以实例化一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter"><code>tf.summary.FileWriter</code></a>，用于写入包含了图表本身和即时数据具体值的事件文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_writer &#x3D; tf.summary.FileWriter(FLAGS.train_dir, sess.graph)</span><br></pre></td></tr></table></figure>
<p>最后，每次评估<code>summary</code>(摘要)并将输出传递给<code>add_summary()</code>函数时，事件文件将被新的摘要值更新。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">summary_str &#x3D; sess.run(summary, feed_dict&#x3D;feed_dict)</span><br><span class="line">summary_writer.add_summary(summary_str, step)</span><br></pre></td></tr></table></figure>
<p>事件文件写入完毕之后，可以就训练文件夹打开一个TensorBoard，查看即时数据的情况。</p>
<p><img src="/img/17_03_03/003.png" alt=""></p>
<blockquote>
<p><strong>注意：</strong>了解更多如何构建并运行TensorBoard的信息，请查看相关教程<a target="_blank" rel="noopener" href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">Tensorboard：训练过程可视化</a>。</p>
</blockquote>
<h3 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h3><p>为了得到可以用来后续恢复模型以进一步训练或评估的检查点文件（checkpoint file），我们实例化一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Saver"><code>tf.train.Saver</code></a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver &#x3D; tf.train.Saver()</span><br></pre></td></tr></table></figure>
<p>在训练循环中，将定期调用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#save"><code>tf.train.Saver.save</code></a>方法，使用所有可训练变量的当前值将检查点文件写入训练目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, FLAGS.train_dir, global_step&#x3D;step)</span><br></pre></td></tr></table></figure>
<p>在将来的某个时间点，可以通过使用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#restore"><code>tf.train.Saver.restore</code></a>方法重新加载模型参数来恢复训练。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.restore(sess, FLAGS.train_dir)</span><br></pre></td></tr></table></figure>
<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>每隔一千个训练步骤，我们的代码会尝试使用训练数据集与测试数据集，对模型进行评估。<code>do_eval</code>函数会被调用三次，分别使用训练数据集、验证数据集合测试数据集。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">print &#39;Training Data Eval:&#39;</span><br><span class="line">do_eval(sess,</span><br><span class="line">        eval_correct,</span><br><span class="line">        images_placeholder,</span><br><span class="line">        labels_placeholder,</span><br><span class="line">        data_sets.train)</span><br><span class="line">print &#39;Validation Data Eval:&#39;</span><br><span class="line">do_eval(sess,</span><br><span class="line">        eval_correct,</span><br><span class="line">        images_placeholder,</span><br><span class="line">        labels_placeholder,</span><br><span class="line">        data_sets.validation)</span><br><span class="line">print &#39;Test Data Eval:&#39;</span><br><span class="line">do_eval(sess,</span><br><span class="line">        eval_correct,</span><br><span class="line">        images_placeholder,</span><br><span class="line">        labels_placeholder,</span><br><span class="line">        data_sets.test)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，更复杂的使用场景通常是，先隔绝<code>data_sets.test</code>测试数据集，只有在大量的超参数优化调整（hyperparameter tuning）之后才进行检查。但是，由于MNIST问题比较简单，我们在这里一次性评估所有的数据。</p>
</blockquote>
<h3 id="构建评估图-Eval-Graph"><a href="#构建评估图-Eval-Graph" class="headerlink" title="构建评估图(Eval Graph)"></a>构建评估图(Eval Graph)</h3><p>在打开默认图表（Graph）之前，我们应该先调用get_data(train=False)函数，抓取测试数据集。</p>
<p>在进入训练循环之前，评估操作应该通过<code>mnist.py</code>中的<code>evaluate()</code>函数来构建。<code>evaluate()</code>传入的<code>logist</code>和标签参数与<code>loss()</code>函数相同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval_correct &#x3D; mnist.evaluation(logits, labels_placeholder)</span><br></pre></td></tr></table></figure>
<p><code>evaluation()</code>函数会生成<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k"><code>tf.nn.in_top_k</code></a>操作，如果在K个最有可能的预测中可以发现真的标签，那么这个操作就会将模型输出标记为正确。在本文中，我们把K的值设置为1，也就是只有在预测是真的标签时，才判定它是正确的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval_correct &#x3D; tf.nn.in_top_k(logits, labels, 1)</span><br></pre></td></tr></table></figure>
<h3 id="评估输出"><a href="#评估输出" class="headerlink" title="评估输出"></a>评估输出</h3><p>之后，我们可以创建一个循环，往其中添加<code>feed_dict</code>，并在调用<code>sess.run()</code>函数时传入<code>eval_correct</code>操作，目的就是用给定的数据集评估模型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for step in xrange(steps_per_epoch):</span><br><span class="line">    feed_dict &#x3D; fill_feed_dict(data_set,</span><br><span class="line">                               images_placeholder,</span><br><span class="line">                               labels_placeholder)</span><br><span class="line">    true_count +&#x3D; sess.run(eval_correct, feed_dict&#x3D;feed_dict)</span><br></pre></td></tr></table></figure>
<p><code>true_count</code>变量会累加所有<code>in_top_k</code>操作判定为正确的预测之和。接下来，只需要将正确测试的总数，除以例子总数，就可以得出准确率了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">precision &#x3D; true_count &#x2F; num_examples</span><br><span class="line">print(&#39;  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f&#39; %</span><br><span class="line">      (num_examples, true_count, precision))</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/26/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E6%B7%B1%E5%85%A5MNIST--%E4%B8%93%E5%AE%B6%E7%BA%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/26/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E6%B7%B1%E5%85%A5MNIST--%E4%B8%93%E5%AE%B6%E7%BA%A7/" class="post-title-link" itemprop="url">【Tensorflow r1.0 文档翻译】深入MNIST--专家级</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-26 14:50:58" itemprop="dateCreated datePublished" datetime="2017-02-26T14:50:58+00:00">2017-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>TensorFlow是一个用于进行大规模数值计算的强大库。其擅长的任务之一是实施和训练深层神经网络。在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。</p>
<p>这个教程假设你已经熟悉神经网络和MNIST数据集。如果你尚未了解，请查看<a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">新手指南</a>。在开始之前，请确认<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/index">安装</a>了TensorFlow。</p>
<h2 id="关于本教程"><a href="#关于本教程" class="headerlink" title="关于本教程"></a>关于本教程</h2><p>本教程的第一部分解释了<a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py">mnist_softmax.py</a>代码中发生了什么，这是Tensorflow模型的基本实现。第二部分显示了一些提高精度的方法。</p>
<p>您可以将本教程中的每个代码段复制并粘贴到Python环境中，当然你也可以选择只是读一下这部分代码。</p>
<p>我们将在本教程中完成：</p>
<ul>
<li>创建一个softmax回归函数，这是一个用于识别MNIST数字的模型，其原理是基于查看图像中的每个像素。</li>
<li>使用Tensorflow来训练模型以识别数字，方法是“查看”数千个示例（并运行我们的第一个Tensorflow会话）。</li>
<li>使用我们的测试数据检查模型的精度。</li>
<li>构建，训练和测试多层卷积神经网络以提高结果。</li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在我们创建模型之前，我们首先加载MNIST数据集，并启动TensorFlow会话。</p>
<h3 id="加载MNIST数据"><a href="#加载MNIST数据" class="headerlink" title="加载MNIST数据"></a>加载MNIST数据</h3><p>如果您要复制粘贴本教程中的代码，请从这两行代码开始，这两行代码将自动下载并读入数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot&#x3D;True)</span><br></pre></td></tr></table></figure>
<p>这里的<code>mnist</code>是一个轻量级类，将训练集，验证集和测试集存储为NumPy数组。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。</p>
<h3 id="启动TensorFlow-InteractiveSession"><a href="#启动TensorFlow-InteractiveSession" class="headerlink" title="启动TensorFlow InteractiveSession"></a>启动TensorFlow InteractiveSession</h3><p>Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。</p>
<p>这里，我们使用更加方便的<code>InteractiveSession</code>类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些<a target="_blank" rel="noopener" href="https://www.tensorflow.org/get_started/get_started#the_computational_graph">计算图</a>，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用<code>InteractiveSession</code>，那么你需要在启动session之前构建整个计算图，然后启<a target="_blank" rel="noopener" href="https://www.tensorflow.org/get_started/get_started#the_computational_graph">动该计算图</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">sess &#x3D; tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>为了在Python中执行高效的数值计算，我们通常引入类似<strong><a target="_blank" rel="noopener" href="http://www.numpy.org/">NumPy</a></strong>这种库来执行开销昂贵的操作。例如在Python之外其他高效的语言来执行矩阵乘法这类操作。不幸的是，每次操作之后切换回Python的动作依然是一个巨大的开销。这种开销特别的差，如果你想要以一种分布式的方式运行在GPU上的话，这里传输数据将会是一个巨大的开销。</p>
<p>TensorFlow也会在Python外部执行大量的运算，但它做了进一步的处理来规避了这种开销。取代独立于Python运行单一的代价昂贵的操作的模式，TensorFlow的方式是通过在Python中描述一个可交互的操作图，然后完全在Python之外进行运行。<strong>Theano</strong>或者<strong>Torch</strong>也有与此类似的实现。</p>
<p>在这里Python代码的作用是用来在外部定义一个操作图，然后决定具体哪一部分的运算图要被运行。详细内容，见<a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门/">TensorFlow入门</a>中的<a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门#用于计算的Graph（图）">用于计算的Graph（图）</a>部分。</p>
<h2 id="构建一个Softmax回归模型"><a href="#构建一个Softmax回归模型" class="headerlink" title="构建一个Softmax回归模型"></a>构建一个Softmax回归模型</h2><p>这一节，我们通过一个单一的线性层来构建一个softmax回归模型。在下一节中，我们将把这个softmax回归扩展为一个多层卷积网络。</p>
<h3 id="占位符（Placeholders）"><a href="#占位符（Placeholders）" class="headerlink" title="占位符（Placeholders）"></a>占位符（Placeholders）</h3><p>我们通过创建输入的图像创建的节点和输出的类别创建的分类来构建一个计算图。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, 784])</span><br><span class="line">y_ &#x3D; tf.placeholder(tf.float32, shape&#x3D;[None, 10])</span><br></pre></td></tr></table></figure>
<p>这里<code>x</code>和<code>y_</code>不是具体的值。相反，他们都是一个<code>placeholder</code>（占位符）—当我们让TensorFlow开始执行计算时才被输入具体值。</p>
<p>输入图像的<code>x</code>包含一个2维的浮点数张量。这里我们赋予它一个<code>shape</code>（形状）为<code>[None, 784]</code>，其中<code>784</code>是由28乘28像素的图片单行展开后的维度数，<code>None</code>表示第一个维度大小不定，可以是任意尺寸，用以指代batch的大小。目标输出类别<code>y_</code>也包含一个2维的tensor，它每行都是一个10维的one-hot向量，用于表示相应的MNIST图像属于哪个数字类（0到9）。</p>
<p>虽然<code>placeholder</code>的<code>shape</code>参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。</p>
<h3 id="变量（Variables）"><a href="#变量（Variables）" class="headerlink" title="变量（Variables）"></a>变量（Variables）</h3><p>我们现在为我们的模型定义了权值<code>W</code>和偏置量<code>b</code>。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：<code>Variable</code>。一个<code>Variable</code>代表TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用<code>Variable</code>来表示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W &#x3D; tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]))</span><br></pre></td></tr></table></figure>
<p>我们在调用<code>tf.Variable</code>的时候传入初始值。在这个例子中，我们把<code>W</code>和<code>b</code>初始化全为0的tensor。<code>W</code>是一个$784×10$的矩阵（因为我们有784个输入特征以及10个输出值），<code>b</code>是一个10维向量（因为我们有10种分类）。</p>
<p>在<code>Variable</code>可以在session中被使用之前，他们必须被session初始化。此步骤使用已经指定的初始值（在这里tensor全部以0填充），并将它们分配给每个$Variable$。下面的代码可以一次初始化全部的<code>Variables</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<h3 id="类别预测与损失函数"><a href="#类别预测与损失函数" class="headerlink" title="类别预测与损失函数"></a>类别预测与损失函数</h3><p>现在我们可以实现我们自己的回归模型了。只需要一行代码！我们把向量化后的图片输入<code>x</code>和权重矩阵<code>W</code>相乘，加上偏置<code>b</code>，然后计算每个分类的softmax概率值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; tf.matmul(x,W) + b</span><br></pre></td></tr></table></figure>
<p>我们可以很容易地指定一个损失函数。损失表示模型的预测效果在单个示例的糟糕程度；在我们的训练过程中，我们会尽量去最小化这个值。在这里，我们的损失函数就是介于目标值和应用于模型预测的softmax激励函数之间的交叉熵。正如我们在新手教学中用到的稳定的方程一样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy &#x3D; tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels&#x3D;y_, logits&#x3D;y))</span><br></pre></td></tr></table></figure>
<p>请注意，<code>tf.nn.softmax_cross_entropy_with_logits</code>内部将softmax应用到非规范化的模型预测中，并且将所有的结果求和，通过<code>tf.reduce_mean</code>来取这些和的平均值。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>现在我们已经定义好了我们的模型和用于训练的损失函数，那么用TensorFlow进行训练就很简单了。由于TensorFlow知道整个计算图，所以它可以使用自动微分来找出关于每个变量的损失梯度。TensorFlow有多种<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_guides/python/train#optimizers">内置的优化算法</a>。对于这个例子，我们将使用最大梯度下降，步长为0.5，来下降交叉熵。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>TensorFlow在这一行中实际上是在计算图中添加新的操作。这些操作包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</p>
<p>返回的<code>train_step</code>操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行<code>train_step</code>来完成。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for _ in range(1000):</span><br><span class="line">  batch &#x3D; mnist.train.next_batch(100)</span><br><span class="line">  train_step.run(feed_dict&#x3D;&#123;x: batch[0], y_: batch[1]&#125;)</span><br></pre></td></tr></table></figure>
<p>每次训练迭代我们都会加入100个训练样本。然后，然后执行一次<code>train_step</code>操作，并通过<code>feed_dict</code>将<code>placeholder</code>tensor<code>x</code>和<code>y_</code>，用训练训练数据替代。请注意，您可以使用<code>feed_dict</code>替换计算图形中的任何tensor。—它不仅仅局限于<code>placeholder</code>。</p>
<h3 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h3><p>那么我们的模型表现如何呢？</p>
<p>首先，来让我们找出那些预测正确的标签。<code>tf.argmax</code>是一个很有用的方法，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。例如，<code>tf.argmax(y，1)</code>是我们的模型认为每个输入最可能的标签，而<code>tf.argmax(y_，1)</code>是正确的标签。我们可以用<code>tf.equal</code>来检查我们我预测值与真实值是否相符。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</span><br></pre></td></tr></table></figure>
<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code>会变成<code>[1,0,1,1]</code>，取平均值后得到<code>0.75</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(accuracy.eval(feed_dict&#x3D;&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<h2 id="构建多层卷积网络"><a href="#构建多层卷积网络" class="headerlink" title="构建多层卷积网络"></a>构建多层卷积网络</h2><p>在MNIST数据集上获得92%的准确率是相当差的。甚至差到令人感到尴尬的地步。在本节中，我们将解决这个问题。我们将从一个非常简单的模型跳转到一个中等复杂的模型：一个小型的卷积神经网络。这将会使我们得到一个大概在99.2%的准确率。—虽然不是最好的结果，但还算是令人满意的一个结果。</p>
<h3 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h3><p>要创建这个模型，我们需要创建很多权值和偏置量。通常情况下，应该使用少量噪音数据来初始化权值以用于打破对称性，并且防止0梯度产生。由于我们使用的是<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks">ReLU</a>)神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个用于初始化的函数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def weight_variable(shape):</span><br><span class="line">  initial &#x3D; tf.truncated_normal(shape, stddev&#x3D;0.1)</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">def bias_variable(shape):</span><br><span class="line">  initial &#x3D; tf.constant(0.1, shape&#x3D;shape)</span><br><span class="line">  return tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<h3 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h3><p>TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做最大池（max pooling）。为了使代码更简洁，我们把这部分抽象成一个函数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def conv2d(x, W):</span><br><span class="line">  return tf.nn.conv2d(x, W, strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line"></span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">  return tf.nn.max_pool(x, ksize&#x3D;[1, 2, 2, 1],</span><br><span class="line">                        strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;SAME&#39;)</span><br></pre></td></tr></table></figure>
<h3 id="第一层卷积"><a href="#第一层卷积" class="headerlink" title="第一层卷积"></a>第一层卷积</h3><p>现在，我们可以实现我们的第一层了。它由一个卷积接一个最大池组成。卷积在每个5x5的patch中算出32个特征。卷积的权重tensor形状是<code>[5, 5, 1, 32]</code>。前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 &#x3D; weight_variable([5, 5, 1, 32])</span><br><span class="line">b_conv1 &#x3D; bias_variable([32])</span><br></pre></td></tr></table></figure>
<p>为了用这一层，我们把<code>x</code>变成一个4维tensor，其第<code>2</code>、第<code>3</code>维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_image &#x3D; tf.reshape(x, [-1,28,28,1])</span><br></pre></td></tr></table></figure>
<p>然后我们将<code>x_image</code>与权值tensor进行卷积，加上偏置量，然后应用ReLU激励函数，最后最大池化。<code>max_pool_2x2</code>方法可将图片大小缩小为14x14。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h_conv1 &#x3D; tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 &#x3D; max_pool_2x2(h_conv1)</span><br></pre></td></tr></table></figure>
<h3 id="第二层卷积"><a href="#第二层卷积" class="headerlink" title="第二层卷积"></a>第二层卷积</h3><p>为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 &#x3D; weight_variable([5, 5, 32, 64])</span><br><span class="line">b_conv2 &#x3D; bias_variable([64])</span><br><span class="line"></span><br><span class="line">h_conv2 &#x3D; tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 &#x3D; max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<h3 id="密集连接层"><a href="#密集连接层" class="headerlink" title="密集连接层"></a>密集连接层</h3><p>现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的tensor reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 &#x3D; weight_variable([7 * 7 * 64, 1024])</span><br><span class="line">b_fc1 &#x3D; bias_variable([1024])</span><br><span class="line"></span><br><span class="line">h_pool2_flat &#x3D; tf.reshape(h_pool2, [-1, 7*7*64])</span><br><span class="line">h_fc1 &#x3D; tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>为了减少过拟合，我们在输出层之前加入<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout</a>。我们用一个<code>placeholder</code>来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的<code>tf.nn.dropout</code>操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。</p>
<blockquote>
<p>对于这个小型卷积网络，性能实际上几乎相同，没有压差。Dropout往往是非常有效的减少过度拟合的方式，但当训练非常大的神经网络时，它是最有用的。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keep_prob &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop &#x3D; tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<h3 id="读出层"><a href="#读出层" class="headerlink" title="读出层"></a>读出层</h3><p>最后，我们添加一个softmax层，就像前面的单层softmax 回归一样。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 &#x3D; weight_variable([1024, 10])</span><br><span class="line">b_fc2 &#x3D; bias_variable([10])</span><br><span class="line"></span><br><span class="line">y_conv &#x3D; tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure>
<h3 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h3><p>这个模型的效果如何呢？为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码。</p>
<p>不过有以下几点不同：</p>
<ul>
<li>我们将用更复杂的ADAM优化器来替换最陡的梯度下降优化器。</li>
<li>我们将在<code>feed_dict</code>中包含附加参数<code>keep_prob</code>来控制丢失率。</li>
<li>我们将在训练过程中的每执行100次迭代时，添加一次日志记录。</li>
</ul>
<p>随时可以继续运行此代码，但它会进行20,000次训练迭代，可能需要一段时间（可能长达半小时），具体时间取决于您的处理器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy &#x3D; tf.reduce_mean(</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(labels&#x3D;y_, logits&#x3D;y_conv))</span><br><span class="line">train_step &#x3D; tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">for i in range(20000):</span><br><span class="line">  batch &#x3D; mnist.train.next_batch(50)</span><br><span class="line">  if i%100 &#x3D;&#x3D; 0:</span><br><span class="line">    train_accuracy &#x3D; accuracy.eval(feed_dict&#x3D;&#123;</span><br><span class="line">        x:batch[0], y_: batch[1], keep_prob: 1.0&#125;)</span><br><span class="line">    print(&quot;step %d, training accuracy %g&quot;%(i, train_accuracy))</span><br><span class="line">  train_step.run(feed_dict&#x3D;&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)</span><br><span class="line"></span><br><span class="line">print(&quot;test accuracy %g&quot;%accuracy.eval(feed_dict&#x3D;&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;))</span><br></pre></td></tr></table></figure>
<p>以上代码，在最终测试集上的准确率大概是99.2%。</p>
<p>目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/22/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84HelloWorld%20--%20MNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/22/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84HelloWorld%20--%20MNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/" class="post-title-link" itemprop="url">【Tensorflow r1.0 文档翻译】机器学习的HelloWorld -- MNIST手写数字识别</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-22 19:50:58" itemprop="dateCreated datePublished" datetime="2017-02-22T19:50:58+00:00">2017-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本教程面向那些不熟悉<strong>机器学习</strong>和<strong>TensorFlow</strong>的读者。如果你已经知道MNIST是什么，softmax（多项Logistic）回归是什么，你可能更喜欢这个<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">更快节奏的教程</a>。在开始教程之前，请确认<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/index">安装TensorFlow</a>。</p>
<p>当一个人开始学习如何编程时，有一个传统，就是编写的第一个程序是能够打印”Hello World.”的程序。正如编程中的”Hello World”一样，机器学习中有MNIST。</p>
<p>MNIST是一个简单的计算机视觉数据集。它由像以下这样的手写数字的图像组成：</p>
<p><img src="/img/17_02_22/001.png" alt=""></p>
<p>它还包括每个图像的标签，用于标识是哪个数字。例如，上述图像的标签是<code>5</code>,<code>0</code>,<code>4</code>和<code>1</code>。</p>
<p>在本教程中，我们将训练一个模型，用来查看图像并预测它们是什么数字。我们的目标不是训练一个真正精准的，拥有高性能的模型，而是浅尝辄止的来体验一下TensorFlow的使用。 - 尽管我们稍后会给出实现这种效果的代码。因此，我们将从一个非常简单的，称为<strong>Softmax回归</strong>的模型开始。</p>
<p>这个教程的实际代码非常短，其中真正有趣的东西只有三行代码。然而，了解背后的想法是非常重要的：TensorFlow如何工作和核心机器学习概念。因此，我们将非常仔细地完成这部分代码。</p>
<h2 id="关于本教程"><a href="#关于本教程" class="headerlink" title="关于本教程"></a>关于本教程</h2><p>本教程是对<a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist_softmax.py">mnist_softmax.py</a>中的代码进行逐行解释。</p>
<p>您可以通过以下几种不同的方式使用本教程：</p>
<ul>
<li>在阅读每行的解释时，将每个代码段逐行复制并粘贴到Python环境中。</li>
<li>在阅读教程期间，运行整个mnist_softmax.py，并使用本教程来了解您不清楚的代码行。</li>
</ul>
<p>我们将在本教程中完成：</p>
<ul>
<li>了解MNIST数据和softmax回归。</li>
<li>创建一个函数，它是一个用于识别数字的模型，其识别原理是基于查看图像中的每个像素的值来实现的。</li>
<li>使用TensorFlow来训练模型以识别数字，其训练方式是“查看”数千个示例（运行我们的第一个TensorFlow会话来执行此逻辑）。</li>
<li>使用我们的测试数据检查模型的精度。</li>
</ul>
<h2 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h2><p>MNIST数据集托管在<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">Yann LeCun的站点</a>。如果您要复制粘贴本教程中的代码，请从这两行代码开始，这两行代码将自动下载并读入数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">mnist &#x3D; input_data.read_data_sets(&quot;MNIST_data&#x2F;&quot;, one_hot&#x3D;True)</span><br></pre></td></tr></table></figure>
<p>MNIST数据被分为三部分：55,000个训练数据（<code>mnist.train</code>），10,000个测试数据（<code>mnist.test</code>）和5,000个验证数据（<code>mnist.validation</code>）。这种切分是非常重要的：它能通过一部分我们并没有实际用来训练学习的数据，来确保我们的算法有很好的通用性。</p>
<p>如前所述，每个MNIST数据点有两个部分：手写数字的图像和相应的标签。我们称为图像”x”和标签”y”。训练集和测试集都包含图像及其相应的标签;例如训练图像是<code>mnist.train.images</code>，训练标签是<code>mnist.train.labels</code>。</p>
<p>每张图像的尺寸是28×28像素。我们可以把它解释为一个大的数组：</p>
<p><img src="/img/17_02_22/002.png" alt=""></p>
<p>我们可以将这个数组变成一个长度为28x28 = 784的向量。如何平铺数组其实并不重要，重要的是要保证图像和数组之间的一致性。从这个角度来看，MNIST图像只是784维向量空间中的一堆点，具有<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/">非常丰富的结构</a>（警告：计算密集的可视化）。</p>
<p>展平数据丢弃了关于图像的2D结构的信息。这样做是不是并不够好？没错，最好的计算机视觉方法确实可以利用这种2D结构信息，我们将在后面的教程进行介绍。但是我们在这里所使用的一种简单方法：<strong>softmax回归</strong>（下面会给出定义），不会利用到这种信息。</p>
<p><code>mnist.train.images</code>是形状为<code>[55000,784]</code>的张量（n维数组）。第一个维度是在列表中图像的索引，第二个维度是每个图像中的每个像素点的索引。对于特定图像中的特定像素，张量中的每个条目是介于0和1之间的像素强度。</p>
<p><img src="/img/17_02_22/003.png" alt=""></p>
<p>MNIST中的每个图像都有相应的标签，标签用介于0到9之间的数字表示图像中绘制的数字。</p>
<p>为了达到本教程的目的，我们需要要将我们的标签作为“one-hot 向量”。one-hot向量是指在大多数维度上数值为0，仅在其中一个维度上数值为1的向量。在这种情况下，第n个数字将被表示为在第n维中为1的向量。例如，3将表示为$[0,0,0,1,0,0,0,0,0,0]$。因此，<code>mnist.train.labels</code>是一个形状为<code>[55000, 10]</code>的数字矩阵。</p>
<p><img src="/img/17_02_22/004.png" alt=""></p>
<p>现在，我们可以开始构建我们的模型啦！</p>
<h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>我们知道MNIST中的每个图像都是一个在0和9之间的手写数字。因此，对于给定的图像，只有10种可能的结果。我们想要能够看到一个图像，并给出它的每个数字的概率。例如，用我们的模型来查看一个9的图片，80％的可能性确认是9，但有5％的可能是8（因为8和9顶部都有一个圈），剩余的可能性分布在其他数值上。</p>
<p>这是一个<strong>softmax回归</strong>的典型案例。如果你想给一个对象赋予其表示不同数字的概率，可以使用softmax，因为softmax可以得出一组介于0到1之间的值，并且这组值加起来结果为1。即使在以后，当我们训练其他更复杂的模型时，最后一步也是一层softmax。</p>
<p>softmax回归有两个步骤：首先我们将图片中属于某个特定数字的证据（evidence）相加，然后将该证据转换为概率。</p>
<p>为了计算给定图像在特定类中的证据，我们对像素强度进行加权求和。如果像素点有很高的强度表示和对应的标签数字不匹配，那么这一点的权值是负数，相反，权值是正数。</p>
<p>下面的图片显示了一个模型学习到的图片上每个像素对于特定数字类的权值。红色表示负权重，蓝色表示正权重。</p>
<p><img src="/img/17_02_22/005.png" alt=""></p>
<p>我们还需要增加一个偏置量（bias），因为输入往往会带有一些无关的干扰量。因此对于给定的输入图片<strong>x</strong>它代表的是数字<strong>i</strong>的证据可以表示为：</p>
<script type="math/tex; mode=display">
\text{evidence}\_i = \sum\_j W_\{i,~ j} x\_j + b\_i</script><p>其中，$W_i$表示权值，$b_i$代表$i$类别的偏置量，$j$代表给定图片$x$的像素索引，用于像素求和。然后用softmax函数可以把这些证据转换成概率<strong>y</strong>：</p>
<script type="math/tex; mode=display">
y = \text{softmax}(\text{evidence})</script><p>这里softmax用作“激活”或“链接”函数，将我们的线性函数的输出变形为我们想要的形式 - 在这里，也就是10种数字的概率分布。你可以把它看作是将证据转换为每种分类的概率。它的定义是：</p>
<script type="math/tex; mode=display">
\text{softmax}(x) = \text{normalize}(\exp(x))</script><p>如果你把这个方程展开，你将得到：</p>
<script type="math/tex; mode=display">
\text{softmax}(x)\_i = \frac{\exp(x\_i)}{\sum\_j \exp(x\_j)}</script><p>但通常我们把softmax定义为第一种形式：对其输入求幂，然后将其归一化处理。这里幂运算表示，更大的证据对应更大的假设模型（hypothesis）里面的乘数权重值。反之，拥有更少的证据意味着在假设模型里面拥有更小的乘数系数。假设模型里的权值不可以是0值或者负值。Softmax然后会正则化这些权重值，使它们的总和等于1，以此构造一个有效的概率分布。（更多的关于Softmax函数的信息，可以参考Michael Nieslen的书里面的这个<a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap3.html#softmax">部分</a>，其中有关于softmax的可交互式的可视化解释。）</p>
<p>softmax回归可以表示为下面这张图，不过真实情况下会有更多的$x$值。我们通过计算出$x$的权值之和加上一个偏置量，然后代入到一个softmax中，来计算出每个输出值。</p>
<p><img src="/img/17_02_22/006.png" alt=""></p>
<p>如果我们把它写成方程的形式，我们将得到：</p>
<p><img src="/img/17_02_22/007.png" alt=""></p>
<p>我们可以“向量化”这个过程，把它变成矩阵乘法和向量加法。这有助于提升计算效率。 （这也是一个有用的思考方式。）</p>
<p><img src="/img/17_02_22/008.png" alt=""></p>
<p>更紧凑的表达形式如下：</p>
<script type="math/tex; mode=display">
y = \text{softmax}(Wx + b)</script><p>现在让我们把它变成TensorFlow可以使用的形式。</p>
<h2 id="回归的实现"><a href="#回归的实现" class="headerlink" title="回归的实现"></a>回归的实现</h2><p>为了在Python中进行高效的数值计算，我们通常使用像<strong><a target="_blank" rel="noopener" href="http://www.numpy.org/">NumPy</a></strong>这样的库，它们会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果要在GPU上以分布式方式运行计算，那么这种开销尤其糟糕，其中传输数据的成本很高。</p>
<p>TensorFlow也在Python之外做了很大量的计算工作，但它做了进一步的完善以改善前面说的那种切换。TensorFlow不是独立于Python运行一个昂贵的操作，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行。（这样类似的运行方式，可以在不少的机器学习库中看到。）</p>
<p>要使用TensorFlow，首先我们需要导入它。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br></pre></td></tr></table></figure>
<p>我们通过操作符号变量来描述这些交互的操作单元。让我们用下面的方式创建一个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; tf.placeholder(tf.float32, [None, 784])</span><br></pre></td></tr></table></figure>
<p><code>x</code>不是一个特定的值。它是一个占位符(<code>placeholder</code>)，当我们要求TensorFlow运行一个计算时，我们将输入一个值。我们希望能够输入任意数量的MNIST图像，其中每个图像都被展开为784维向量。我们将其表示为float类型的2-D张量，形状为<code>[None, 784]</code>。（这里的<code>None</code>表示维度可以是任何长度。）</p>
<p>我们的模型还需要权重和偏差。当然我们可以把它们当做是另外的输入（使用占位符），但TensorFlow有一个更好的方法来表示它们：<code>Variable</code>。<code>Variable</code>代表一个可修改的张量，它存在于TensorFlow中用于描述交互性操作的图中。在计算过程中，它们可以被拿来使用甚至可以修改。对于机器学习应用，一般都会有模型参数，可以用Variable表示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W &#x3D; tf.Variable(tf.zeros([784, 10]))</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]))</span><br></pre></td></tr></table></figure>
<p>我们通过给与<code>tf.Variable</code>初始值来创建<code>Variable</code>:在这种情况下，我们将<code>W</code>和<code>b</code>初始化为全部为0的张量。因为我们要通过学习得到<code>W</code>和<code>b</code>，因此它们的初始值具体是什么并不重要。</p>
<p>注意，<code>W</code>的形状为<code>[784,10]</code>，因为我们想要用784维的图片向量乘以它以得到一个10维的证据值向量，其中每一位对应着不同数字类别。<code>b</code>的形状是<code>[10]</code>，所以我们可以直接把它加到输出上面。</p>
<p>现在，我们可以实现我们的模型啦。只需要一行代码！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; tf.nn.softmax(tf.matmul(x, W) + b)</span><br></pre></td></tr></table></figure>
<p>首先，我们通过表达式<code>tf.matmul(x, W)</code>将<code>x</code>和<code>W</code>相乘。这对应于前面方程中的$Wx$，<code>x</code>是一个拥有多个输入的2D张量。紧接着，我们加上<code>b</code>，最后，代入到<code>tf.nn.softmax</code>中。</p>
<p>就是这样，在几行用来设置变量的代码之后，我们只需要一行代码就可以定义好我们的模型。这不仅仅是因为TensorFlow被设计为使<strong>softmax回归</strong>变得特别简单，它也用这种非常灵活的方式来描述其他各种数值计算，从机器学习模型对物理学模拟仿真模型。一旦被定义好之后，我们的模型就可以在不同的设备上运行：计算机的CPU，GPU，甚至是手机！</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好的。实际上，在机器学习中，我们通常定义指标来表示一个模型是坏的，这个指标称为成本（cost）或损失（loss），然后尽量最小化这个指标。</p>
<p>一个非常常见的，非常好的用来衡量模型损失的函数称为“<strong>交叉熵(cross-entropy)</strong>”。交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：</p>
<script type="math/tex; mode=display">
H\_{y'}(y) = -\sum\_i y'\_i \log(y\_i)</script><p><strong>y</strong>是我们预测的概率分布,<strong>y’</strong>是实际的分布（我们输入的one-hot vector)。比较粗糙的理解是，交叉熵是用来衡量相对于真实值我们所给出的预测的低效性。有关交叉熵的更详细的讨论超出了本教程的范畴，但<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-09-Visual-Information/">理解它的原理</a>很有必要。</p>
<p>为了计算交叉熵，我们首先需要添加一个新的占位符用于输入正确值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_ &#x3D; tf.placeholder(tf.float32, [None, 10])</span><br></pre></td></tr></table></figure>
<p>然后，我们可以实现交叉熵方法:$-\sum y’\log(y)$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy &#x3D; tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices&#x3D;[1]))</span><br></pre></td></tr></table></figure>
<p>首先，<code>tf.log</code>计算了每个<code>y</code>的对数。接下来，我们将<code>y_</code>与相应的<code>tf.log(y)</code>的元素做乘法运算。然后，由于参数<code>reduction_indices=[1]</code>，<code>tf.reduce_sum</code>将<code>y</code>中的第二维中的元素相加求和。最后，通过<code>tf.reduce_mean</code>计算批次中所有示例的平均值。</p>
<p>注意，在源码中，我们不使用这些信息，因为它在数值上并不稳定。取而代之的是，我们将<code>tf.nn.softmax_cross_entropy_with_logits</code>用于非规范化的逻辑上（例如，我们对<code>tf.matmul(x, W) + b</code>使用<code>softmax_cross_entropy_with_logits</code>），因为这样在数值上更稳定方法，它在内部执行了softmax的计算。在你的代码中考虑使用<code>tf.nn.softmax_cross_entropy_with_logits</code>来代替之前的逻辑。</p>
<p>现在，我们知道了我们想要我们的模型做什么，使用TensorFlow来训练它也非常简单。因为TensorFlow知道用于计算的整个图（graph），它会自动地使用<strong><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Backprop/">反向传播算法</a></strong>来有效地确定你的变量是如何影响你想要最小化的那个成本值的。然后它可以应用您选择的优化算法修改变量和减少损失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>在这里，我们通过使用学习率为0.5的<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降算法</a>令TensorFlow最小化<code>cross_entropy</code>（交叉熵）。梯度下降是一个简单的程序，它的原理是每次向着减少损失的方向移动一小步，来最小化代价函数。但TensorFlow也提供了<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_guides/python/train#optimizers">很多其他的优化算法</a>，只需要简单的调整一行代码就可以随意切换。</p>
<p>TensorFlow在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元，用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。</p>
<p>我们现在可以在<code>InteractiveSession</code>中启动我们的模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess &#x3D; tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<p>我们首先要创建一个操作来初始化我们创建的变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.global_variables_initializer().run()</span><br></pre></td></tr></table></figure>
<p>让我们开始执行训练 - 我们将运行1000次训练步骤！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for _ in range(1000):</span><br><span class="line">  batch_xs, batch_ys &#x3D; mnist.train.next_batch(100)</span><br><span class="line">  sess.run(train_step, feed_dict&#x3D;&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure>
<p>每循环一次，我们将从我们的训练集中得到一批100个随机数据点。然后我们用这些数据点作为参数替换之前的占位符来运行<code>train_step</code>。</p>
<p>使用小批随机数据称为<strong>随机训练(stochastic training)</strong> - 在这里更确切的说是随机梯度下降训练。理想情况下，我们希望将所有数据用于训练的每个步骤，因为这能给我们更好的训练结果，但很明显这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。</p>
<h2 id="评估我们的模型"><a href="#评估我们的模型" class="headerlink" title="评估我们的模型"></a>评估我们的模型</h2><p>那么我们的模型表现如何呢？</p>
<p>首先，来让我们找出那些预测正确的标签。<code>tf.argmax</code>是一个很有用的方法，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。例如，<code>tf.argmax(y，1)</code>是我们的模型认为每个输入最可能的标签，而<code>tf.argmax(y_，1)</code>是正确的标签。我们可以用<code>tf.equal</code>来检查我们我预测值与真实值是否相符。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</span><br></pre></td></tr></table></figure>
<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code>会变成<code>[1,0,1,1]</code>，取平均值后得到<code>0.75</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>
<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(accuracy, feed_dict&#x3D;&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<p>结果大概维持在92%左右。</p>
<p>这种结果很好吗？其实并不是很好。其实，它相当差。这是因为我们使用的是一个非常简单的模型。我们可以通过做一些简单的修改，可以将正确率提高到97%。事实上，最优秀的模型可以达到超过99.7%的准确率！（想了解更多信息，可以看看这个关于各种模型的<a target="_blank" rel="noopener" href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">性能对比列表</a>。)</p>
<p>比结果更重要的是，我们从这个模型中学习到的设计思想。不过，如果你仍然对这里的结果有点失望，可以查看<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">下一个教程</a>，在那里你可以学习如何用FensorFlow构建更加复杂的模型以获得更好的性能！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/20/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91TensorFlow%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/20/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91TensorFlow%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">【Tensorflow r1.0 文档翻译】TensorFlow入门</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-20 21:42:58" itemprop="dateCreated datePublished" datetime="2017-02-20T21:42:58+00:00">2017-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="TensorFlow入门"><a href="#TensorFlow入门" class="headerlink" title="TensorFlow入门"></a>TensorFlow入门</h2><p>这是一个TensorFlow的入门指南。在你使用这份指南之前，请先<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/">安装TensorFlow</a>。在充分的使用本指南之前，您应该了解以下内容：</p>
<ul>
<li>如何使用Python进行编程。</li>
<li>至少对矩阵有一些了解</li>
<li>最好是对<strong>机器学习</strong>有一点了解。但即使你对<strong>机器学习</strong>有一点了解、或者甚至完全不了解，那么你很有必要读一读这一篇指南了。</li>
</ul>
<p>TensorFlow提供了多种API。即使是最低版本的TensorFlow 核心 API，也为您提供了完整的编程控制。如果您是机器学习研究人员，或需要对模型进行精细控制的人，那么我们建议你使用TensorFlow 核心代码，否则我们建议您使用TensorFlow Core API。这些更高级的API通常比TensorFlow 核心代码更容易学习和使用。此外，较高级别的API使重复性任务更容易上手，并且在不同用户之间更一致。高级API（如<strong>tf.contrib.learn</strong>）可帮助您管理数据集、估计量、训练和推断。注意，在一些高级TensorFlow API 中，方法名称包含<code>contrib</code>的API表示仍在开发中。一些<code>contrib</code>方法可能会在随后的TensorFlow版本中发生改变或过时。</p>
<p>本指南从TensorFlow 核心教程开始。稍后，我们将演示如何在<code>tf.contrib.learn</code>中实现相同的模型。了解TensorFlow核心原则将会给你提供一个很棒的心理模型，这个模型是用于说明当您使用更紧凑的更高级别的API时，内部是如何工作的。</p>
<h2 id="Tensors（张量）"><a href="#Tensors（张量）" class="headerlink" title="Tensors（张量）"></a>Tensors（张量）</h2><p>TensorFlow中的数据的中心单元是<strong>Tensors(张量)</strong>。tensor是由一组原始数据组成，这些原始数据是由一组任意数量维度的数组形成。一个tensor的<strong>rank</strong>是表示它尺寸的一个数值。下面是几个tensor的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3 # a rank 0 tensor; this is a scalar with shape []</span><br><span class="line">[1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3]</span><br><span class="line">[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]</span><br><span class="line">[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]</span><br></pre></td></tr></table></figure>
<h2 id="TensorFlow核心教程"><a href="#TensorFlow核心教程" class="headerlink" title="TensorFlow核心教程"></a>TensorFlow核心教程</h2><h3 id="导入TensorFlow"><a href="#导入TensorFlow" class="headerlink" title="导入TensorFlow"></a>导入TensorFlow</h3><p>TensorFlow程序的标准导入语句如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br></pre></td></tr></table></figure>
<p>这样做可以使得Python能够正常访问TensorFlow的所有类、方法和符号。我们的大多数文档都假设你已经这样做了。</p>
<h3 id="用于计算的Graph（图）"><a href="#用于计算的Graph（图）" class="headerlink" title="用于计算的Graph（图）"></a>用于计算的Graph（图）</h3><p>也许你会认为TensorFlow Core的程序包含下面两部分组成：</p>
<ul>
<li>1.构建<strong>computational graph（用于计算的图）</strong></li>
<li>2.运行<strong>computational graph（用于计算的图）</strong></li>
</ul>
<p>一个<strong>computational graph（用于计算的图）</strong>是一系列排列在graph的节点上的TensorFlow操作单元。让我们来构建一个简单的<strong>computational graph</strong>。每个节点接受0个或多个tensor作为输入，并且产生一个tensor作为输出。<strong>常量类型</strong>是节点的一种类型。正如所有的TensorFlow常量一样，它是不接收输入的，并且输出一个它内部存储的值。我们可以按照下面的方式来创建两个浮点Tensor节点<code>node1</code>和<code>node2</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node1 &#x3D; tf.constant(3.0, tf.float32)</span><br><span class="line">node2 &#x3D; tf.constant(4.0) # also tf.float32 implicitly</span><br><span class="line">print(node1, node2)</span><br></pre></td></tr></table></figure>
<p>执行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;Const:0&quot;, shape&#x3D;(), dtype&#x3D;float32) Tensor(&quot;Const_1:0&quot;, shape&#x3D;(), dtype&#x3D;float32)</span><br></pre></td></tr></table></figure>
<p>你会注意到，打印节点并不会如你所想的输出<code>3.0</code>和<code>4.0</code>。相反，这些节点会在计算时分别产生<code>3.0</code>和<code>4.0</code>。为了实际评估这些节点，我们必须以一个 <strong>session（会话）</strong> 来运行 <strong>computational graph</strong>。<strong>session（会话）</strong> 封装了TensorFlow运行时的控件和状态。</p>
<p>下面的代码创建了一个<code>Session</code>对象，并且执行了它的<code>run</code>方法来运行包含了<code>node1</code>和<code>node2</code>的<strong>computational graph</strong>的计算结果。通过在<strong>session</strong>中运行<strong>computational graph</strong>的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess &#x3D; tf.Session()</span><br><span class="line">print(sess.run([node1, node2]))</span><br></pre></td></tr></table></figure>
<p>我们看到了我们期望看到的<code>3.0</code>和<code>4.0</code>的输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3.0, 4.0]</span><br></pre></td></tr></table></figure>
<p>我们可以通过将<code>Tensor</code>节点与操作节点（操作也是一种节点）组合起来的方式来构建更复杂的计算。例如，我们可以将两个常量节点执行加法操作，并且产生一个新的graph，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node3 &#x3D; tf.add(node1, node2)</span><br><span class="line">print(&quot;node3: &quot;, node3)</span><br><span class="line">print(&quot;sess.run(node3): &quot;,sess.run(node3))</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node3:  Tensor(&quot;Add_2:0&quot;, shape&#x3D;(), dtype&#x3D;float32)</span><br><span class="line">sess.run(node3):  7.0</span><br></pre></td></tr></table></figure>
<p>TensorFlow提供了一个叫做<strong>TensorBoard</strong>的很实用的程序，它可以将computational graph可视化的展示出来。下面是通过TensorBoard来可视化一个graph的效果：</p>
<p><img src="/img/17_02_20/001.png" alt=""></p>
<p>由于我们用到的是常量，因此这个图看起来并不是特别有趣，因为它总是产生一个恒定的结果。graph可以被参数化，并且通过<strong>placeholders（占位符）</strong>来接受外部的输入。<strong>placeholders（占位符）</strong>表示对稍后所提供的值的一个承诺。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">b &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">adder_node &#x3D; a + b  # + 是tf.add(a, b)的缩写形式</span><br></pre></td></tr></table></figure>
<p>上面三行的表达形式看起来有点像一个方法，或lambda表达式：其中我们定义两个输入参数（<code>a</code>和<code>b</code>），然后对它们执行一个操作。我们可以通过多个输入来计算这个graph的执行结果，其中我们的输入是通过<code>feed_dict</code>参数来指定对这些<strong>placeholders（占位符）</strong>提供具体值的<code>Tensors</code>的输入的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(adder_node, &#123;a: 3, b:4.5&#125;))</span><br><span class="line">print(sess.run(adder_node, &#123;a: [1,3], b: [2, 4]&#125;))</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7.5</span><br><span class="line">[ 3.  7.]</span><br></pre></td></tr></table></figure>
<p>在TensorBoard中，graph看起来是这个样子：</p>
<p><img src="/img/17_02_20/002.png" alt=""></p>
<p>我们可以通过添加其他操作，来让我们的<strong>computational graph</strong>看起来更复杂。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add_and_triple &#x3D; adder_node * 3.</span><br><span class="line">print(sess.run(add_and_triple, &#123;a: 3, b:4.5&#125;))</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">22.5</span><br></pre></td></tr></table></figure>
<p>上面的computational graph在TensorBoard中看起来是这样的：</p>
<p><img src="/img/17_02_20/003.png" alt=""></p>
<p>在机器学习中，我们通常需要一个可以接受任意输入的模型，例如上面的模型。为了使模型可训练，我们需要能够修改<code>graph</code>以获得具有相同输入的新输出。<strong>Variables（变量）</strong>允许我们向graph中添加可训练的参数。它们由一个类型和初始值组成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W &#x3D; tf.Variable([.3], tf.float32)</span><br><span class="line">b &#x3D; tf.Variable([-.3], tf.float32)</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">linear_model &#x3D; W * x + b</span><br></pre></td></tr></table></figure>
<p>当调用<code>tf.constant</code>时，常量被初始化，它们的值永远不会改变。相比之下，变量<code>tf.Variable</code>在调用时不会被初始化。要初始化TensorFlow程序中的所有变量，必须显式调用特殊的初始化操作，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<p><code>init</code>是TensorFlow的sub-graph的一个重要的操作，它用于初始化所有的全局变量。在这里直到我们调用<code>sess.run</code>之前，变量是未初始化的。</p>
<p>由于<code>x</code>是一个占位符，因此我们可以同时计算<code>linear_model</code>几个值，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(linear_model, &#123;x:[1,2,3,4]&#125;))</span><br></pre></td></tr></table></figure>
<p>产生如下输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ 0.          0.30000001  0.60000002  0.90000004]</span><br></pre></td></tr></table></figure>
<p>我们创建了一个模型，但目前我们还不知道它是好是坏。为了评估训练数据的模型，我们需要一个<strong>loss function（损失函数）</strong>，我们可以用一个<code>y</code>占位符来提供所需的值。</p>
<p>损失函数会计算出当前训练出的模型和所提供的数据之间的距离。我们将使用用于线性回归的标准损失模型，其原理是将当前模型和提供的数据之间的增量的平方求和。<code>linear_model - y</code>创建一个向量，其中每个元素是相应的样本的误差增量。我们称之为<code>tf.square</code>平方误差。然后，我们使用<code>tf.reduce_sum</code>将所有平方误差求和，以创建一个单一的标量，用于提取出表示所有样本的总误差值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">squared_deltas &#x3D; tf.square(linear_model - y)</span><br><span class="line">loss &#x3D; tf.reduce_sum(squared_deltas)</span><br><span class="line">print(sess.run(loss, &#123;x:[1,2,3,4], y:[0,-1,-2,-3]&#125;))</span><br></pre></td></tr></table></figure>
<p>输出损失值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">23.66</span><br></pre></td></tr></table></figure>
<p>我们可以通过手动的将<code>W</code>和<code>b</code>的值重新赋值为<code>-1</code>和<code>1</code>的方式来提高我们的算法的效果。变量可以初始化后将数据提供给<code>tf.Variable</code>对象，也可以使用像<code>tf.assign</code>这样的操作来更改。例如，<code>W=-1</code>和<code>b=1</code>是我们的模型中的最佳参数。因此我们可以更改<code>W</code>和<code>b</code>的值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fixW &#x3D; tf.assign(W, [-1.])</span><br><span class="line">fixb &#x3D; tf.assign(b, [1.])</span><br><span class="line">sess.run([fixW, fixb])</span><br><span class="line">print(sess.run(loss, &#123;x:[1,2,3,4], y:[0,-1,-2,-3]&#125;))</span><br></pre></td></tr></table></figure>
<p>最终输出结果的损失是0：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.0</span><br></pre></td></tr></table></figure>
<p>我们猜到了“最完美”的参数<code>W</code>和<code>b</code>的值，但机器学习的目标是<strong>自动</strong>找到正确的模型参数。我们将在下一节中说明如何完成这一任务。</p>
<h2 id="tf-train-API"><a href="#tf-train-API" class="headerlink" title="tf.train API"></a>tf.train API</h2><p>机器学习的完整讨论超出了本教程的范围。然而，TensorFlow提供了缓慢地改变每个变量以便<strong>最小化损失函数</strong>的<strong>optimizers（优化器）</strong>。其中最简单的优化器是<strong>gradient descent（梯度下降）</strong>。其原理是根据相对于该变量的损失导数的大小修改每个变量的值。一般来说，人工计算导数是繁琐的并且容易出错。因此，TensorFlow可以使用<code>tf.gradients</code>函数，来自动的产生当前所给模型描述的导数。为了简化操作，优化器通常会自动地为您执行此操作。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer &#x3D; tf.train.GradientDescentOptimizer(0.01)</span><br><span class="line">train &#x3D; optimizer.minimize(loss)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess.run(init) # 将值重置为不正确的默认值</span><br><span class="line">for i in range(1000):</span><br><span class="line">  sess.run(train, &#123;x:[1,2,3,4], y:[0,-1,-2,-3]&#125;)</span><br><span class="line"></span><br><span class="line">print(sess.run([W, b]))</span><br></pre></td></tr></table></figure>
<p>最终训练得出的模型参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[array([-0.9999969], dtype&#x3D;float32), array([ 0.99999082],</span><br><span class="line"> dtype&#x3D;float32)]</span><br></pre></td></tr></table></figure>
<p>现在我们已经完成了实际的机器学习！完成这个简单的线性回归不需要太多的TensorFlow核心代码，但是更复杂的学习模型和方法通常需要更多的代码。因此，TensorFlow为通用模式、结构和功能提供了一套更高级别的抽象实现。我们将在下一节中学习如何使用这些抽象实现。</p>
<h3 id="完整的程序"><a href="#完整的程序" class="headerlink" title="完整的程序"></a>完整的程序</h3><p>完成的可训练线性回归模型程序如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># Model parameters</span><br><span class="line">W &#x3D; tf.Variable([.3], tf.float32)</span><br><span class="line">b &#x3D; tf.Variable([-.3], tf.float32)</span><br><span class="line"># Model input and output</span><br><span class="line">x &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">linear_model &#x3D; W * x + b</span><br><span class="line">y &#x3D; tf.placeholder(tf.float32)</span><br><span class="line"># loss</span><br><span class="line">loss &#x3D; tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares</span><br><span class="line"># optimizer</span><br><span class="line">optimizer &#x3D; tf.train.GradientDescentOptimizer(0.01)</span><br><span class="line">train &#x3D; optimizer.minimize(loss)</span><br><span class="line"># training data</span><br><span class="line">x_train &#x3D; [1,2,3,4]</span><br><span class="line">y_train &#x3D; [0,-1,-2,-3]</span><br><span class="line"># training loop</span><br><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line">sess &#x3D; tf.Session()</span><br><span class="line">sess.run(init) # reset values to wrong</span><br><span class="line">for i in range(1000):</span><br><span class="line">  sess.run(train, &#123;x:x_train, y:y_train&#125;)</span><br><span class="line"></span><br><span class="line"># evaluate training accuracy</span><br><span class="line">curr_W, curr_b, curr_loss  &#x3D; sess.run([W, b, loss], &#123;x:x_train, y:y_train&#125;)</span><br><span class="line">print(&quot;W: %s b: %s loss: %s&quot;%(curr_W, curr_b, curr_loss))</span><br></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11</span><br></pre></td></tr></table></figure>
<p>这个更复杂的程序也可以在TensorBoard中可视化：</p>
<p><img src="/img/17_02_20/004.png" alt=""></p>
<h2 id="tf-contrib-learn"><a href="#tf-contrib-learn" class="headerlink" title="tf.contrib.learn"></a>tf.contrib.learn</h2><p><code>tf.contrib.learn</code>是一个高级别的TensorFlow库，它简化了机器学习的机制，包括：</p>
<ul>
<li>运行训练循环</li>
<li>运行评估循环</li>
<li>管理数据集</li>
<li>管理数据导入</li>
</ul>
<p><code>tf.contrib.learn</code>定义了许多常见的模型。</p>
<h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><p>请注意，线性回归在使用<code>tf.contrib.learn</code>的情况下变得更简单了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"># NumPy is often used to load, manipulate and preprocess data.</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># Declare list of features. We only have one real-valued feature. There are many</span><br><span class="line"># other types of columns that are more complicated and useful.</span><br><span class="line">features &#x3D; [tf.contrib.layers.real_valued_column(&quot;x&quot;, dimension&#x3D;1)]</span><br><span class="line"></span><br><span class="line"># An estimator is the front end to invoke training (fitting) and evaluation</span><br><span class="line"># (inference). There are many predefined types like linear regression,</span><br><span class="line"># logistic regression, linear classification, logistic classification, and</span><br><span class="line"># many neural network classifiers and regressors. The following code</span><br><span class="line"># provides an estimator that does linear regression.</span><br><span class="line">estimator &#x3D; tf.contrib.learn.LinearRegressor(feature_columns&#x3D;features)</span><br><span class="line"></span><br><span class="line"># TensorFlow provides many helper methods to read and set up data sets.</span><br><span class="line"># Here we use &#96;numpy_input_fn&#96;. We have to tell the function how many batches</span><br><span class="line"># of data (num_epochs) we want and how big each batch should be.</span><br><span class="line">x &#x3D; np.array([1., 2., 3., 4.])</span><br><span class="line">y &#x3D; np.array([0., -1., -2., -3.])</span><br><span class="line">input_fn &#x3D; tf.contrib.learn.io.numpy_input_fn(&#123;&quot;x&quot;:x&#125;, y, batch_size&#x3D;4,</span><br><span class="line">                                              num_epochs&#x3D;1000)</span><br><span class="line"></span><br><span class="line"># We can invoke 1000 training steps by invoking the &#96;fit&#96; method and passing the</span><br><span class="line"># training data set.</span><br><span class="line">estimator.fit(input_fn&#x3D;input_fn, steps&#x3D;1000)</span><br><span class="line"></span><br><span class="line"># Here we evaluate how well our model did. In a real example, we would want</span><br><span class="line"># to use a separate validation and testing data set to avoid overfitting.</span><br><span class="line">estimator.evaluate(input_fn&#x3D;input_fn)</span><br></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;global_step&#39;: 1000, &#39;loss&#39;: 1.9650059e-11&#125;</span><br></pre></td></tr></table></figure>
<h3 id="定制模型"><a href="#定制模型" class="headerlink" title="定制模型"></a>定制模型</h3><p><code>tf.contrib.learn</code>不会将你锁定在预定义模型中。假设我们想创建一个未内置到TensorFlow中的自定义模型。我们仍然可以保留<code>tf.contrib.learn</code>的数据集、馈送、训练等的高级抽象。为了说明，我们将演示如何使用我们的较低级别TensorFlow API的知识来实现​​我们自己的等效模型到<code>LinearRegressor</code>。</p>
<p>要定义与<code>tf.contrib.learn</code>一起使用的自定义模型，我们需要使用<code>tf.contrib.learn.Estimator</code>。 <code>tf.contrib.learn.LinearRegressor</code>实际上是<code>tf.contrib.learn.Estimator</code>的子类。替代子类<code>Estimator</code>，我们只是提供<code>Estimator</code>一个<code>model_fn</code>函数，用于告诉<code>tf.contrib.learn</code>如何评估预测、训练步骤和损失。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"># Declare list of features, we only have one real-valued feature</span><br><span class="line">def model(features, labels, mode):</span><br><span class="line">  # Build a linear model and predict values</span><br><span class="line">  W &#x3D; tf.get_variable(&quot;W&quot;, [1], dtype&#x3D;tf.float64)</span><br><span class="line">  b &#x3D; tf.get_variable(&quot;b&quot;, [1], dtype&#x3D;tf.float64)</span><br><span class="line">  y &#x3D; W*features[&#39;x&#39;] + b</span><br><span class="line">  # Loss sub-graph</span><br><span class="line">  loss &#x3D; tf.reduce_sum(tf.square(y - labels))</span><br><span class="line">  # Training sub-graph</span><br><span class="line">  global_step &#x3D; tf.train.get_global_step()</span><br><span class="line">  optimizer &#x3D; tf.train.GradientDescentOptimizer(0.01)</span><br><span class="line">  train &#x3D; tf.group(optimizer.minimize(loss),</span><br><span class="line">                   tf.assign_add(global_step, 1))</span><br><span class="line">  # ModelFnOps connects subgraphs we built to the</span><br><span class="line">  # appropriate functionality.</span><br><span class="line">  return tf.contrib.learn.ModelFnOps(</span><br><span class="line">      mode&#x3D;mode, predictions&#x3D;y,</span><br><span class="line">      loss&#x3D; loss,</span><br><span class="line">      train_op&#x3D;train)</span><br><span class="line"></span><br><span class="line">estimator &#x3D; tf.contrib.learn.Estimator(model_fn&#x3D;model)</span><br><span class="line"># define our data set</span><br><span class="line">x&#x3D;np.array([1., 2., 3., 4.])</span><br><span class="line">y&#x3D;np.array([0., -1., -2., -3.])</span><br><span class="line">input_fn &#x3D; tf.contrib.learn.io.numpy_input_fn(&#123;&quot;x&quot;: x&#125;, y, 4, num_epochs&#x3D;1000)</span><br><span class="line"></span><br><span class="line"># train</span><br><span class="line">estimator.fit(input_fn&#x3D;input_fn, steps&#x3D;1000)</span><br><span class="line"># evaluate our model</span><br><span class="line">print(estimator.evaluate(input_fn&#x3D;input_fn, steps&#x3D;10))</span><br></pre></td></tr></table></figure>
<p>当执行它时，会产生如下结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;loss&#39;: 5.9819476e-11, &#39;global_step&#39;: 1000&#125;</span><br></pre></td></tr></table></figure>
<p>注意，自定义<code>model()</code>函数的内容与低版本API的手册中的模型训练循环非常相似。</p>
<h2 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a>下一步</h2><p>现在你了解到了TensorFlow的基本运作的知识。我们还有几个教程，您可以查看以了解更多。如果你是机器学习的初学者，请参阅<a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">【深度学习的HelloWorld — MNIST手写数字识别】</a>，否则请参阅<a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">【深入MNIST — 专家级】</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/20/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/20/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/" class="post-title-link" itemprop="url">【Tensorflow r1.0 文档翻译】入门教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-20 21:19:58" itemprop="dateCreated datePublished" datetime="2017-02-20T21:19:58+00:00">2017-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li><a href="/2017/02/20/【Tensorflow%20r1.0%20文档翻译】TensorFlow入门/">【TensorFlow入门】</a></li>
<li><a href="/2017/02/22/【Tensorflow%20r1.0%20文档翻译】机器学习的HelloWorld%20--%20MNIST手写数字识别/">【机器学习的HelloWorld — MNIST手写数字识别】</a></li>
<li><a href="/2017/02/26/【Tensorflow%20r1.0%20文档翻译】深入MNIST--专家级/">【深入MNIST — 专家级】</a></li>
<li><a href="/2017/03/03/【Tensorflow%20r1.0%20文档翻译】Tensorflow原理导论/">【TensorFlow原理导论】</a></li>
<li><a href="/2017/03/05/【Tensorflow%20r1.0%20文档翻译】【tf.contrib.learn快速入门】/">【tf.contrib.learn快速入门】</a></li>
<li><a href="/2017/03/06/【Tensorflow%20r1.0%20文档翻译】通过tf.contrib.learn来构建输入函数/">【通过tf.contrib.learn来构建输入函数】</a></li>
<li><a href="/2017/03/07/【Tensorflow%20r1.0%20文档翻译】TensorBoard-可视化学习/">【TensorBoard:可视化学习】</a></li>
<li><a href="/2017/03/07/【Tensorflow%20r1.0%20文档翻译】TensorBoard-嵌入可视化/">【TensorBoard:嵌入可视化】</a></li>
<li><a href="/2017/03/07/【Tensorflow%20r1.0%20文档翻译】TensorBoard-图的可视化/">【TensorBoard:图的可视化】</a></li>
<li><a href="/2017/03/09/【Tensorflow%20r1.0%20文档翻译】使用tf.contrib.learn记录和监视的基本知识/">【使用tf.contrib.learn记录和监视的基本知识】</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/09/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B8%83%E5%91%A8%20(2)%E6%A0%B8%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/09/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B8%83%E5%91%A8%20(2)%E6%A0%B8%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第七周 (2)核函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-09 21:07:58" itemprop="dateCreated datePublished" datetime="2017-02-09T21:07:58+00:00">2017-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="核函数-I"><a href="#核函数-I" class="headerlink" title="核函数 I"></a>核函数 I</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/YOMHn/kernels-i">视频地址</a></p>
<p>在本节，我将对支持向量机算法做一些改变，以构造复杂的非线性分类器。我们用<strong>“kernels(核函数)”</strong>来达到此目的。</p>
<p>我们来看看<strong>核函数</strong>是什么，以及如何使用。</p>
<p>如果你有一个像这个样的训练集：</p>
<p><img src="/img/17_02_09/001.png" alt=""></p>
<p>然后你希望拟合一个非线性的判别边界来区别正负样本，那么你的判别边界可能是这样的：</p>
<p><img src="/img/17_02_09/002.png" alt=""></p>
<p>当我们这么做的时候，其实这个决策边界是由类似于下面这种多项式构成的：</p>
<ul>
<li><p>如果$<br>\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1x_2+\theta_4x_1^2 + \theta_5x_2^2 + … \ge 0<br>$，则预测$h_{\theta}(x)=1$；</p>
</li>
<li><p>如果$<br>\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1x_2+\theta_4x_1^2 + \theta_5x_2^2 + … \lt 0<br>$，则预测$h_{\theta}(x)=0$；</p>
</li>
</ul>
<p>如果我们把假设函数改写成下面这种形式：</p>
<script type="math/tex; mode=display">
\theta\_0+\theta\_1f\_1+\theta\_2f\_2+\theta\_3f\_3+\theta\_4f\_4+\theta\_5f\_5+...</script><p>因此有：</p>
<script type="math/tex; mode=display">
f\_1=x\_1</script><script type="math/tex; mode=display">
f\_2=x\_2</script><script type="math/tex; mode=display">
f\_3=x\_1x\_2</script><script type="math/tex; mode=display">
f\_4=x\_1^2</script><script type="math/tex; mode=display">
f\_5=x\_2^2</script><script type="math/tex; mode=display">
...</script><p>以此类推，可以依次加入这些高阶项，但我们其实并不知道这些高阶项是不是我们真正需要的。我们之前谈到计算机视觉的时候，提到过在这里的输入是一个有很多像素的图像，我们看到如果用高阶项作为特征变量，运算量将是非常大的，因为有太多的高阶项需要被计算。</p>
<p>因此，我们是否有不同的选择，或者是更好的选择来构造特征变量，以用来嵌入到假设函数中呢？</p>
<h3 id="用核函数构造新特征"><a href="#用核函数构造新特征" class="headerlink" title="用核函数构造新特征"></a>用核函数构造新特征</h3><p>事实上，这里有一个可以构造新特征$f_1$、$f_2$、$f_3$的方法。</p>
<p>首先我们定义三个特征变量(但是对于实际问题而言，我们可以定义非常多的特征变量）：</p>
<p><img src="/img/17_02_09/003.png" width = "300" height = "200" align=center /></p>
<p>将这三个点标记为$l^{(1)}$、$l^{(2)}$、$l^{(3)}$，接下来我要做的是定义新的特征变量：</p>
<script type="math/tex; mode=display">
f\_1=similarity(x,l^{(1)})</script><p>这里$similarite(x,l^{(1)})$是一种相似度的度量，度量样本$x$与第一个标记$l^{(1)}$的相似度。</p>
<p>这个度量相似度的公司是这样的：</p>
<script type="math/tex; mode=display">
\begin{align\*}
f\_1&=similarity(x,l^{(1)})
\\\\
&=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}</script><blockquote>
<p>$exp$是自然常数$e$为底的指数函数。</p>
</blockquote>
<p>不知道你之前是否看了上一个选修课程的视频，$||w||$是表示向量$w$的长度。因此这里的$||x-l^{(1)}||$的意思就是就是向量的欧式距离。</p>
<p>因此，我们可以依次写出$f_1$、$f_2$、$f_3$:</p>
<script type="math/tex; mode=display">
\begin{align\*}
f\_1&=similarity(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}</script><script type="math/tex; mode=display">
\begin{align\*}
f\_2&=similarity(x,l^{(2)})
=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})
\end{align\*}</script><script type="math/tex; mode=display">
\begin{align\*}
f\_3&=similarity(x,l^{(3)})
=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})
\end{align\*}</script><p>这里的$similarite(x,l)$函数，就被称为<strong>核函数(Kernels)</strong>。在这里，我们的例子中所说的<strong>核函数</strong>，实际上是<strong>高斯核函数</strong>，在后面我们还会见到不同的<strong>核函数</strong>。</p>
<p>核函数我们通常不写作$similarity(x,l^{(i)})$，而是写作：</p>
<script type="math/tex; mode=display">
k(x,l^{(i)})</script><h3 id="核函数可以做什么？"><a href="#核函数可以做什么？" class="headerlink" title="核函数可以做什么？"></a>核函数可以做什么？</h3><p>我们来看看核函数到底可以做什么？</p>
<p>首先让我们来看看第一个标记：</p>
<script type="math/tex; mode=display">
\begin{align\*}
f\_1&=similarity(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
=exp(-\frac{\sum\_{j=1}^{n}(x\_j-l\_j^{(1)})^2}{2σ^2})
\end{align\*}</script><p>$l^{(1)}$是我之前在图中选取的几个点之中的一个，上面是$x$和$l^{(1)}$之间的核函数。</p>
<p>其中$||x-l^{(1)}||^2$这一项可以表示成各个$x$向量到$l$向量的距离求和的形式：$\sum_{j=1}^{n}(x_j-l_j^{(1)})^2$。（这里我们依然忽略了截距的影响，即令$x_0=1$）。</p>
<p>假设，如果$x\approx l^{(1)}$，即$x$与其中一个标记点非常接近，那么这个欧氏距离$||x-l^{(1)}||$就会接近0，因此：</p>
<script type="math/tex; mode=display">
f\_1
\approx
exp(-\frac{0^2}{2σ^2})
\approx1</script><p>相反的，如果$x$离$l^{(1)}$很远，那么会有：</p>
<script type="math/tex; mode=display">
f\_1
\approx
exp(-\frac{(large\ number)^2}{2σ^2})
\approx0</script><p><strong>这些特征变量的作用是度量$x$到标记$l^{(1)}$的相似度的，并且如果$x$离$l$非常接近，那么特征变量$f$就接近1；如果$x$离标记$l^{(1)}$非常远，那么特征变量$f$就接近于0。</strong></p>
<p>之前我绘制的三个标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$每一个标记点会定义一个新的特征变量：</p>
<script type="math/tex; mode=display">
\begin{align\*}
f\_1&=k(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}</script><script type="math/tex; mode=display">
\begin{align\*}
f\_2&=k(x,l^{(2)})
=exp(-\frac{||x-l^{(2)}||^2}{2σ^2})
\end{align\*}</script><script type="math/tex; mode=display">
\begin{align\*}
f\_3&=k(x,l^{(3)})
=exp(-\frac{||x-l^{(3)}||^2}{2σ^2})
\end{align\*}</script><p><strong>也就是说，给出一个训练样本$x$，我们就能基于我们之前给出的标记点$l^{(1)}$、$l^{(2)}$、$l^{(3)}$来计算出三个新的特征变量$f_1$、$f_2$、$f_3$。</strong></p>
<h3 id="深入理解核函数"><a href="#深入理解核函数" class="headerlink" title="深入理解核函数"></a>深入理解核函数</h3><p>接下来让我们通过画一些图来更好地理解<strong>核函数</strong>是什么样的。</p>
<h4 id="x-对-f-的值的影响"><a href="#x-对-f-的值的影响" class="headerlink" title="$x$对$f$的值的影响"></a>$x$对$f$的值的影响</h4><p>看下面这个例子，假设我们有两个特征$x_1$和$x_2$，假设我们第一个标记点是$l^{(1)}$：</p>
<script type="math/tex; mode=display">
l^{(1)}=
\begin{bmatrix}
   3 \\\
    5
 \end{bmatrix}</script><p>假设:</p>
<script type="math/tex; mode=display">
σ^2=1</script><p>如果我画出:</p>
<script type="math/tex; mode=display">
\begin{align\*}
f\_1&=k(x,l^{(1)})
=exp(-\frac{||x-l^{(1)}||^2}{2σ^2})
\end{align\*}</script><p>结果就是这样的：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">3D曲面图</th>
<th style="text-align:center">等高线图</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/004.png" width = "300" height = "200" align=center /></td>
<td style="text-align:center"><img src="/img/17_02_09/005.png" width = "300" height = "200" align=center /></td>
</tr>
</tbody>
</table>
</div>
<p>其中左侧的图纵轴是$f_1$，水平方向的两个轴分别是$x_1$和$x_2$。右侧的图是左侧图的等高线图。</p>
<p>你会发现，当$x=(3,5)$的时候，$f_1=1$，因为它在最大值的位置上。所以如果$x$往旁边移动，离这个点越远，那么从图中可以看到$f_1$的值就越接近0。</p>
<h4 id="σ-2-对-f-的值的影响"><a href="#σ-2-对-f-的值的影响" class="headerlink" title="$σ^2$对$f$的值的影响"></a>$σ^2$对$f$的值的影响</h4><p>在这里，要提到的另一点就是$σ^2$对结果的影响。$σ^2$是<strong>高斯核函数</strong>的参数，改变它会得到略微不同的结果。</p>
<p>可以对比$σ^2=1$、$σ^2=0.5$、$σ^2=3$的情况：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">$σ^2=1$</th>
<th style="text-align:center">$σ^2=0.5$</th>
<th style="text-align:center">$σ^2=3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/004.png" width = "300" height = "200" align=center /></td>
<td style="text-align:center"><img src="/img/17_02_09/006.png" width = "300" height = "200" align=center /></td>
<td style="text-align:center"><img src="/img/17_02_09/008.png" width = "300" height = "200" align=center /></td>
</tr>
<tr>
<td style="text-align:center"><img src="/img/17_02_09/005.png" width = "300" height = "200" align=center /></td>
<td style="text-align:center"><img src="/img/17_02_09/007.png" width = "300" height = "200" align=center /></td>
<td style="text-align:center"><img src="/img/17_02_09/009.png" width = "300" height = "200" align=center /></td>
</tr>
</tbody>
</table>
</div>
<p>你会发现，函数的形状还是相似的，只是$σ^2=0.5$相较于$σ^2=1$凸起的宽度变窄了，等值线图也收缩了一些；$σ^2=3$相较于$σ^2=1$凸起的宽度变宽了，等值线也扩张了一些。</p>
<p>所以，如果我们将$σ^2$设为0.5时，特征变量$f_1$下降到0的速度也会相应变快；如果我们将$σ^2$设为3时，特征变量$f_1$下降到0的速度也会相应变慢。</p>
<h3 id="获取预测函数"><a href="#获取预测函数" class="headerlink" title="获取预测函数"></a>获取预测函数</h3><p>讲完了特征变量的定义，我们来看看我们能得到什么样的预测函数。</p>
<p>给定一个训练样本$x$，我们要计算出三个特征变量$f_1$ $f_2$ $f_3$</p>
<p><img src="/img/17_02_09/010.png" alt=""></p>
<p>并且如果$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3 \ge 0$，则预测函数的预测值为1，即$y=1$。</p>
<p>对于这个特定的例子而言，假设我们已经找到了一个学习算法，并且假设我已经得到了这些参数的值，比如如果：</p>
<script type="math/tex; mode=display">
\theta\_0=-0.5 \\\\
\theta\_1=1 \\\\
\theta\_2=1 \\\\
\theta\_3=0</script><p>如果我们现在有一个训练样本$x$：</p>
<p><img src="/img/17_02_09/011.png" alt=""></p>
<p>我想知道预测函数会给出什么结果。</p>
<p>看看这个公式：</p>
<script type="math/tex; mode=display">\theta\_0 + \theta\_1f\_1 + \theta\_2f\_2 +  \theta\_3f\_3 \ge 0</script><p>因为我的训练样本$x$接近于$l^{(1)}$，那么$f_1$就接近于1：</p>
<script type="math/tex; mode=display">
f\_1\approx1</script><p>又因为训练样本$x$离$l^{(2)}$ $l^{(3)}$都很远，所以$f_2$就接近于0，$f_3$也接近于0：</p>
<script type="math/tex; mode=display">
f\_2\approx0 \\\\
f\_3\approx0</script><p>所以带入上面的公式可以得到：</p>
<script type="math/tex; mode=display">
\begin{align\*}
\theta\_0 + \theta\_1f\_1 + \theta\_2f\_2 +  \theta\_3f\_3
&=\theta\_0 + \theta\_1·1 + \theta\_2·0 +  \theta\_3·0
\\\\
&=-0.5+1
\\\\
&=0.5\\\\
\ge0
\end{align\*}</script><p>因此对于这一点，我们预测的结果是$y=1$。</p>
<hr>
<p>现在我们选择另一个不同的点，假设我选择了另一个点：</p>
<p><img src="/img/17_02_09/012.png" alt=""></p>
<p>如果将这个训练样本$x$带入之前相同的计算，你发现$f_1$ $f_2$ $f_3$都接近于0。</p>
<p>因此，我们得到$\theta_0 + \theta_1f_1 + \theta_2f_2 +  \theta_3f_3=-0.5$，因为$θ_0=-0.5$，并且$f_1$ $f_2$ $f_3$都为0，因此最后结果是-0.5，小于0。因此这个点，我们预测的y值是0。</p>
<hr>
<p>类似的，如果你自己来对大量的点进行这样相应的处理，你应该可以确定如果你有一个非常接近于$l^{(2)}$的训练样本，那么通过这个点预测的y值也是1。</p>
<p>实际上，你最后得到的结果是：<strong>对于接近$l^{(1)}$和$l^{(2)}$的点，我们的预测值是1，对于远离$l^{(1)}$和$l^{(2)}$的点，我们最后预测的结果是等于0的</strong>。</p>
<p>我们最后会得到这个预测函数的判别边界看起来是类似这样的结果：</p>
<p><img src="/img/17_02_09/013.png" width = "300" height = "200" align=center /></p>
<p>在这个红色的判别边界里面，预测的y值等于1；在这外面预测的y值等于0。</p>
<p>因此这就是一个我们如何通过标记点，以及核函数，来训练出非常复杂的非线性判别边界的方法。</p>
<p>这就是核函数这部分的概念，以及我们如何在支持向量机中使用它们。我们通过标记点和相似性函数来定义新的特征变量从而训练复杂的非线性分类器。</p>
<p>目前还有一些问题我们并没有做出回答，其中一个是<strong>我们如何得到这些标记点</strong>；另一个是<strong>其他的相似度方程（核函数）是什么样的</strong>，如果有其他的话，我们能够用其他的相似度方程来代替我们所讲的这个高斯核函数吗？在下一个视频中我们会回答这些问题，然后把所有东西都整合到一起来看看支持向量机如何通过核函数的定义 有效地学习复杂非线性函数。</p>
<h2 id="核函数-II"><a href="#核函数-II" class="headerlink" title="核函数 II"></a>核函数 II</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/hxdcH/kernels-ii">视频地址</a></p>
<blockquote>
<p>在上一节视频里，我们讨论了<strong>核函数</strong>这个想法以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。例如，<strong>怎么处理支持向量机中的偏差方差折中</strong>。</p>
</blockquote>
<h3 id="如何选取标记点-landmark"><a href="#如何选取标记点-landmark" class="headerlink" title="如何选取标记点(landmark)"></a>如何选取标记点(landmark)</h3><p>在上一节课中，我谈到过选择标记点，例如$l^{(1)}$ $l^{(2)}$ $l^{(3)}$ 这些点使我们能够定义<strong>相似度函数</strong>，也称之为<strong>核函数</strong>。在这个例子里，我们的相似度函数为<strong>高斯核函数</strong>。</p>
<p><img src="/img/17_02_09/014.png" alt=""></p>
<p>但是，我们从哪里得到这些标记点呢？我们从哪里得到$l^{(1)}$ $l^{(2)}$ $l^{(3)}$？ 而且在一些复杂的学习问题中，也许我们需要更多的标记点，而不是我们手选的这三个。</p>
<p>因此，在实际应用时，怎么选取标记点，是机器学习中必须解决的问题。</p>
<p>这是我们的数据集：</p>
<p><img src="/img/17_02_09/015.png" alt=""></p>
<p>有一些正样本和一些负样本。我们的想法是：我们直接将训练样本作为标记点。</p>
<p><img src="/img/17_02_09/016.png" alt=""></p>
<p>如果我们有一个训练样本$x^{(1)}$，那么我将把第一个标记点就放在和第一个训练样本点完全重合的地方：</p>
<p><img src="/img/17_02_09/017.png" alt=""></p>
<p>同样，第二个标记点也是使用同样的方式来标注：</p>
<p><img src="/img/17_02_09/018.png" alt=""></p>
<p>在右边的这幅图上，我们使用红点和蓝点来阐述这幅图以及这些点的颜色，可能并不显眼，但是利用这个方法最终能得到m个标记点：</p>
<script type="math/tex; mode=display">
l^{(1)} ，l^{(2)}，...，l^{(m)}</script><p>即每一个标记点的位置，都与每一个样本点的位置精确对应。</p>
<p>这说明，特征函数基本上是在描述<strong>每一个样本距离样本集中其他样本的距离</strong>。</p>
<p>我们具体的列出这个过程的大纲：</p>
<p>给定m个训练样本：</p>
<script type="math/tex; mode=display">
(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}),</script><p>我将选取与m个训练样本精确一致的位置作为我的标记点：</p>
<script type="math/tex; mode=display">
l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},...,l^{(m)}=x^{(m)}.</script><p>当输入样本$x$（样本$x$可以属于训练集，也可以属于交叉验证集，也可以属于测试集），我们可以计算这些特征，即：</p>
<script type="math/tex; mode=display">
f\_1=similarity(x,l^{(1)}) \\\\
f\_2=similarity(x,l^{(2)}) \\\\
...</script><blockquote>
<p>这里的$l^{(1)}=x^{(1)}$，$l^{(2)}=x^{(2)}$…。</p>
</blockquote>
<p>最终我们能得到一个特征向量，我们将特征向量记为$f$：</p>
<script type="math/tex; mode=display">
f=
\left[
\begin{matrix}
 f\_1  \\\
 f\_2  \\\
 ...    \\\
 f\_m
\end{matrix}
\right]</script><p>此外，按照惯例，如果我们需要的话，可以添加额外的特征$f_0$，$f_0$的值始终为1：</p>
<script type="math/tex; mode=display">
f=
\left[
\begin{matrix}
 f\_0  \\\
 f\_1  \\\
 f\_2  \\\
 ...    \\\
 f\_m
\end{matrix}
\right]</script><p>它与我们之前讨论过的截距$x^0$的作用相似。</p>
<hr>
<p>举个例子，假设我们有训练样本$(x^{(i)},y^{(i)})$，这个样本对应的特征向量可以这样计算：</p>
<p>给定$x^{(i)}$，我们可以通过相似度函数</p>
<script type="math/tex; mode=display">
f\_1^{(i)}=similarity(x^{(i)},l^{(1)}) \\\
f\_2^{(i)}=similarity(x^{(i)},l^{(2)}) \\\
... \\\
f\_m^{(i)}=similarity(x^{(i)},l^{(m)}) \\\</script><p>在这一列中的某个位置，即第$i$个元素，有一个特征：</p>
<script type="math/tex; mode=display">
f\_i^{(i)}=similarity(x^{(i)},l^{(i)})</script><blockquote>
<p>这里的$l^{(i)}$就等于$x^{(i)}$。</p>
</blockquote>
<p>所以$f_i^{(i)}$衡量的是$x^{(i)}$与其自身的相似度，如果你使用高斯核函数的话，这一项为：</p>
<script type="math/tex; mode=display">
f\_i^{(i)}=similarity(x^{(i)},l^{(i)})=exp(-\frac{0}{2σ^2})=1</script><p>所以，对于这个样本来说，其中的某一个特征等于1。接下来，类似于我们之前的过程，我将这m个特征合并为一个特征向量。于是，相比之前用$x^{(i)}$来描述样本，$x^{(i)}$为n维或者n+1维空间。我们现在可以使用这个特征向量$f^{(i)}$来描述我的特征向量：</p>
<script type="math/tex; mode=display">
f^{(i)}=
\left[
\begin{matrix}
 f\_0^{(i)}  \\\
 f\_1^{(i)}  \\\
 f\_2^{(i)}  \\\
 ...    \\\
 f\_m^{(i)}
\end{matrix}
\right]</script><p>其中 $f_0^{(i)}=1$。</p>
<p>那么这个向量就是我们用于描述训练样本的特征向量。</p>
<p>当给定核函数和相似度函数后，我们按照这个方法来使用<strong>支持向量机</strong>。</p>
<hr>
<p>如果你已经得到参数$\theta$并且想对样本x做出预测，我们先要计算特征向量$f$，$f$是$m+1$维的特征向量（这里有m是因为我们有m个训练样本，因此就有m个标记点）。</p>
<p>我们在$\theta^Tf\ge0$时，预测$y=1$</p>
<blockquote>
<p>这里$\theta^Tf=\theta_0f_0 + \theta_1f_1 + … + \theta_mf_m$</p>
</blockquote>
<p>以上就是<strong>当已知参数$\theta$时，怎么做出预测的过程</strong>。</p>
<p>那么，怎样得到参数$\theta$呢？</p>
<p>你在使用SVM学习算法的时候，具体来说就是要求解这个最小化问题：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}f^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}f^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><p>你需要求出能使这个式子取最小值的参数$\theta$。注意，这里我们把之前的$x^{(i)}$换成了$f^{(i)}$。</p>
<p>通过解决这个最小化问题，我们就能得到支持向量机的参数。</p>
<p>最后一个对于这个优化问题的细节是：我们有$n=m$个特征。有效的特征数量应该等于$f$的维数，所以$n$其实就等于$m$。</p>
<p><img src="/img/17_02_09/019.png" alt=""></p>
<p>以上就是支持向量机的学习算法。</p>
<p>我在这里还要讲到一个数学细节，在支持向量机实现的过程中，最后一项与上面式子中的最后一项$\frac{1}{2}\sum_{j=1}^{n}\theta_{j}^2$有细微差别。其实在实现支持向量机时，你并不需要知道这个细节，事实上这个式子已经给你提供了全部需要的原理。但是在支持向量机实现的过程中，这一项$\sum_{j=1}^{n}\theta_{j}^2$可以被重写为$\theta^T\theta$(如果我们忽略$\theta_0$的话)，即：</p>
<script type="math/tex; mode=display">
\sum\_{j=1}^{n}
\theta\_{j}^2
=
\theta^T\theta</script><p>其中：</p>
<script type="math/tex; mode=display">
\theta=
\left[
\begin{matrix}
 \theta\_1  \\\
 \theta\_2  \\\
 ...    \\\
 \theta\_m  \\\
\end{matrix}
\right] \ \ (ignore \ \theta\_0)</script><p>大多数支持向量机在实现的时候其实是替换掉$\theta^T\theta$的，用$\theta^TM\theta$来代替，其中$M$是某矩阵，具体是什么矩阵取决于你采用的核函数。这其实是另一种略有区别的距离度量方法。我们用这种略有变化的度量距离的形式来取代对$||\theta||^2$（即$\theta^T\theta$，或者$\sum_{j=1}^{n}\theta_{j}^2$）进行最小化的形式，这是参数向量$\theta$的<strong>变尺度形式</strong>，这种变化和核函数相关。这个数学细节使得支持向量机能够更有效率的运行。</p>
<p>支持向量机做这种修改的理由是：这么做可以适应超大的训练集。</p>
<p>例如：当你的训练集有10000个样本时</p>
<script type="math/tex; mode=display">
M=10000</script><p>根据我们之前定义标记点的方法，我们最终有10000个标记点，$\theta$也随之是10000维的向量。或许这时这么做还可行，但是，当$m$变得非常非常大时，那么求解这么多参数时，此时利用支持向量机软件包来解决这里的最小化问题时，求解这些参数的成本会非常高。</p>
<p>这些都是数学细节，事实上你没有必要了解这些，这里$\theta^TM\theta$实际上细微的修改了最后一项，使得最终的优化目标与直接最小化$||\theta||^2$略有区别。</p>
<p>如果你愿意的话，你可以直接认为这个具体的实现细节尽管略微的改变了优化目标，但是它主要是为了计算效率，所以你不必要对此有太多担心。</p>
<p>顺便说一下，你可能会想，为什么我们不将核函数这个想法应用到其他算法，比如逻辑回归上。事实证明，如果你愿意的话，确实可以将核函数这个想法用于定义特征向量，将标记点之类的技术用于逻辑回归算法。但是用于支持向量机的计算技巧不能较好的推广到其他算法，诸如逻辑回归上。所以，将核函数用于逻辑回归上时，会变得非常的慢。相比之下，这些计算技巧，比如这一步：$\theta^TM\theta$，这些细节的修改，以及支持向量机软件的实现细节，使得支持向量机可以和核函数相得益彰，而逻辑回归和核函数则会运行的十分缓慢，更何况它们还不能使用那些高级优化技巧，因为这些技巧是人们专门为使用核函数的支持向量机开发的。但是这些问题只有在你亲自实现最小化函数时才会遇到。</p>
<p>我将在下一节视频中进一步讨论这些问题，但是你并不需要知道怎么去写一个软件，来最小化代价函数。你能找到很好的成熟的软件来做这些，就像我一直不建议自己写矩阵求逆函数，或者平方根函数的道理一样。这些软件包已经包含了那些数值优化技巧，所以你不必担心这些东西。</p>
<hr>
<h3 id="SVM参数："><a href="#SVM参数：" class="headerlink" title="SVM参数："></a>SVM参数：</h3><h4 id="C-frac-1-lambda"><a href="#C-frac-1-lambda" class="headerlink" title="$C(=\frac{1}{\lambda})$"></a>$C(=\frac{1}{\lambda})$</h4><p>但是另外一个值得说明的问题是，在你使用支持向量机时，怎么选择支持向量机中的参数？在本节视频的末尾，我想稍微说明一下，在使用支持向量机时的“偏差-方差折中”其中一个要选择的事情是，目标函数中的参数$C$。回忆一下，$C$的作用与$\frac{1}{\lambda}$相似。</p>
<p><img src="/img/17_02_09/020.png" alt=""></p>
<p>$\lambda$是逻辑回归算法中的正则化参数，所以$C$对应着我们之前在逻辑回归问题中的$\lambda$，这意味着:</p>
<ul>
<li>较小的$\lambda$对应较大的$C$，这就意味着有可能得到一个低偏差但高方差的模型。</li>
<li>较大的$\lambda$对应较小的$C$，这就意味着有可能得到一个高偏差但低方差的模型。</li>
</ul>
<p>所以使用较大的$C$值模型，为高方差，更倾向于过拟合；而使用较小的$C$值的模型，为高偏差，更倾向于欠拟合。</p>
<h4 id="σ-2"><a href="#σ-2" class="headerlink" title="$σ^2$"></a>$σ^2$</h4><p>另一个要处理的参数是<strong>高斯核函数</strong>中的$σ^2$。</p>
<p>当高斯核函数中的$σ^2$偏大时，那么对应的相似度函数为：</p>
<script type="math/tex; mode=display">
exp(-\frac{||x-l^\{(i)}||^2}{2σ^2})</script><p>在下面这个例子中，如果我们只有一个特征$x_1$，有一个标记点$l$：</p>
<p><img src="/img/17_02_09/021.png" alt=""></p>
<p><strong>如果$σ^2$越大</strong>，那么高斯核函数倾向于变得<strong>越平滑</strong>，由于函数平滑且变化的比较平缓，这会给你的模型带来<strong>较高的偏差和较低的方差</strong>，由于高斯核函数变得平缓，就更倾向于得到一个随着输入$x$变化得缓慢的模型：</p>
<p><img src="/img/17_02_09/022.png" alt=""></p>
<p>反之<strong>如果$σ^2$越小</strong>，那么高斯核函数会变化的<strong>很剧烈</strong>，在这种情况下，最终得到的模型会是<strong>低偏差和高方差</strong>：</p>
<p><img src="/img/17_02_09/023.png" alt=""></p>
<hr>
<p>这就是利用核函数的支持向量机算法。</p>
<p>希望这些关于方差和偏差的讨论能给你一些对于算法结果预期的直观印象。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/06/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B8%83%E5%91%A8%20(1)%E5%A4%A7%E9%97%B4%E8%B7%9D%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/06/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E4%B8%83%E5%91%A8%20(1)%E5%A4%A7%E9%97%B4%E8%B7%9D%E5%88%86%E7%B1%BB/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第七周 (1)大间距分类</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-06 00:42:58" itemprop="dateCreated datePublished" datetime="2017-02-06T00:42:58+00:00">2017-02-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/sHfVT/optimization-objective">视频地址</a></p>
<blockquote>
<p>到目前为止，你已经见过一系列不同的学习算法。在监督学习中许多学习算法的性能都非常类似，因此重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是应用这些算法时所创建的大量数据。</p>
<p>在应用这些算法时，表现情况通常依赖于你的水平。比如你为学习算法所设计的 特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法，广泛的应用于工业界和学术界，它被称为<strong>支持向量机(Support Vector Machine)</strong>。</p>
<p>与逻辑回归和神经网络相比，<strong>支持向量机</strong>或者简称<strong>SVM</strong>在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p>
<p>因此，在接下来的视频中我会探讨这一算法，在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于<strong>支持向量机</strong>，鉴于该算法的强大和受欢迎度，在本课中我会花许多时间来讲解它，它也是我们所介绍的最后一个监督学习算法。</p>
</blockquote>
<h3 id="支持向量机引入"><a href="#支持向量机引入" class="headerlink" title="支持向量机引入"></a>支持向量机引入</h3><p>为了描述<strong>支持向量机</strong>，我将会从逻辑回归开始，展示我们如何一点一点修改，来得到本质上的支持向量机。</p>
<p>在逻辑回归中，我们已经熟悉了它的假设函数形式：</p>
<script type="math/tex; mode=display">
h\_{\theta}(x)=\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}</script><p>和S型激励函数：</p>
<p><img src="/img/17_02_06/001.png" width = "300" height = "200" align=center /></p>
<p>现在让我们一起来考虑下，我们想要逻辑回归做什么？</p>
<ul>
<li><p>如果有一个样本为$y=1$，那么我们希望假设函数$h(x)≈1$，即$\theta^{T}&gt;&gt;0$。你不难发现，此时逻辑回归的输出将趋近于1。</p>
</li>
<li><p>如果有另一个样本为$y=0$，那么我们希望假设函数$h(x)≈0$，即$\theta^{T}&lt;&lt;0$。此时逻辑回归的输出将趋近于0。</p>
</li>
</ul>
<hr>
<p>如果你进一步观察逻辑回归的代价函数，你会发现每个样本(x, y)都会为总代价函数增加这样的一项：</p>
<script type="math/tex; mode=display">
-(ylogh\_{\theta}(x) + (1-y)log(1-h\_{\theta}(x))) 
= -ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})</script><p>在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。</p>
<p>现在一起来考虑<strong>y=1</strong>和<strong>y=0</strong>的两种情况：</p>
<ul>
<li><strong>y=1的情况下（即$\theta^{T}x&gt;&gt;0$）</strong>：</li>
</ul>
<p>对于</p>
<script type="math/tex; mode=display">
-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})</script><p>由于$(1-y)=0$，所以我们只需考虑前半部分：</p>
<script type="math/tex; mode=display">
-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}</script><p>如果画出代价函数关于$z$的图，你会看到下图：</p>
<p><img src="/img/17_02_06/002.png" width = "300" height = "200" align=center /></p>
<p>我们可以看到，当$z$增大时(即$\theta^{T}x$增大时)，$z$对应的值会变得非常小，对整个代价函数而言，影响也非常小。</p>
<p>现在开始建立<strong>支持向量机</strong>，我们会从这个代价函数$-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}}$开始，一点点的修改：</p>
<p>我们画出一个非常接近于逻辑回归函数的折线，这个折线经由$z=1$的一点的两条线段组成：</p>
<p><img src="/img/17_02_06/003.png" width = "300" height = "200" align=center /></p>
<p>到这里已经非常接近逻辑回归中使用的代价函数了，只是这里是由两条线段组成。先不要考虑线段的斜率，这并不重要，重要的是我们将在$y=1$的前提下使用新的代价函数。</p>
<ul>
<li><strong>y=0的情况下（即$\theta^{T}x&lt;&lt;0$）</strong>：</li>
</ul>
<p>对于</p>
<script type="math/tex; mode=display">
-ylog\frac{1}{1+\mathrm{e}^{-\theta^{T}x}} - (1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})</script><p>由于$y=0$，所以我们只需考虑后半部分：</p>
<script type="math/tex; mode=display">
(1-y)log(1-\frac{1}{1+\mathrm{e}^{-\theta^{T}x}})</script><p>如果画出代价函数关于$z$的图，你会看到下图：</p>
<p><img src="/img/17_02_06/004.png" width = "300" height = "200" align=center /></p>
<p>用相似的方法，我们开始建立<strong>支持向量机</strong>：</p>
<p><img src="/img/17_02_06/005.png" width = "300" height = "200" align=center /></p>
<p>我们将在$y=0$的前提下使用新的代价函数。</p>
<hr>
<p>那么现在我们来给这两个方程命名：</p>
<p>对于这个函数：</p>
<p><img src="/img/17_02_06/003.png" width = "300" height = "200" align=center /></p>
<p>我们命名为<strong>$cost_{1}(z)$</strong>。</p>
<p>对于第二个函数：</p>
<p><img src="/img/17_02_06/005.png" width = "300" height = "200" align=center /></p>
<p>我们命名为<strong>$cost_{0}(z)$</strong>。</p>
<p>这里的下标指的是在函数中对应的$y=1$和$y=0$的情况。</p>
<hr>
<h3 id="构建支持向量机"><a href="#构建支持向量机" class="headerlink" title="构建支持向量机"></a>构建支持向量机</h3><p>拥有了这些定义之后，现在我们就开始构建<strong>支持向量机</strong>。</p>
<h4 id="1-替换逻辑回归函数"><a href="#1-替换逻辑回归函数" class="headerlink" title="1.替换逻辑回归函数"></a>1.替换逻辑回归函数</h4><p>这就是我们在逻辑回归中使用的代价函数$J(\theta)$：</p>
<p><img src="/img/17_02_06/006.png" alt=""></p>
<p>对于支持向量机而言，实际上，我们要将</p>
<ul>
<li><p>上面式子中的这一项：$(-logh_{\theta}(x^{(i)}))$替换为：$cost_{1}(z)$，即:$cost_{1}(\theta^{T}x^{(i)})$</p>
</li>
<li><p>同样，这一项：$((-log(1-h_{\theta}(x^{(i)}))))$替换为：$cost_{0}(z)$，即:$cost_{0}(\theta^{T}x^{(i)})$</p>
</li>
</ul>
<p>这里替换之后的$cost_{1}(z)$和$cost_{0}(z)$就是上面提到的那两条靠近逻辑回归函数的折线。</p>
<p>所以对于<strong>支持向量机</strong>的最小化代价函数问题，代价函数的形式如下：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ} 
\frac{1}{m}[
\sum\_{i=1}^{m}
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{\lambda}{2m}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><h4 id="2-去除多余的常数项-frac-1-m"><a href="#2-去除多余的常数项-frac-1-m" class="headerlink" title="2.去除多余的常数项 $\frac{1}{m}$"></a>2.去除多余的常数项 $\frac{1}{m}$</h4><p>现在按照<strong>支持向量机</strong>的惯例，我们去除$\frac{1}{m}$这一项，因为这一项是个常数项，即使去掉我们也可以得出相同的$\theta$最优值：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ} 
\sum\_{i=1}^{m}
[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{\lambda}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><h4 id="3-正则化项系数的处理"><a href="#3-正则化项系数的处理" class="headerlink" title="3.正则化项系数的处理"></a>3.正则化项系数的处理</h4><p>在逻辑回归的目标函数中，我们有两项表达式：</p>
<ul>
<li>来自于训练样本的代价函数:</li>
</ul>
<script type="math/tex; mode=display">
\frac{1}{m}[
\sum\_{i=1}^{m}
y^{(i)}
(-logh\_{\theta}(x^{(i)}))+
(1-y^{(i)})
((-log(1-h\_{\theta}(x^{(i)}))))
]</script><ul>
<li>正则化项：</li>
</ul>
<script type="math/tex; mode=display">
\frac{\lambda}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><p>我们不得不使用正则化项来平衡我们的代价函数。这就相当于：</p>
<script type="math/tex; mode=display">
A + \lambda B</script><p>其中A相当于上面的第一项，B相当于第二项。</p>
<p>我们通过修改不同的正则化参数$\lambda$来达到优化目的，这样我们就能够使得训练样本拟合的更好。</p>
<p>但对于<strong>支持向量机</strong>，按照惯例我们将使用一个不同的参数来替换这里使用的$\lambda$来实现权衡这两项的目的。这个参数我们称为<strong>C</strong>。同时将优化目标改为:</p>
<script type="math/tex; mode=display">
CA + B</script><p>因此，在逻辑回归中，如果给$\lambda$一个很大的值，那么就意味着给与$B$了一个很大的权重，而在<strong>支持向量机</strong>中，就相当于对$C$设定了一个非常小的值，这样一来就相当于对$B$给了比$A$更大的权重。</p>
<p>因此，这只是一种来控制这种权衡关系的不同的方式。当然你也可以把这里的$C$当做$farc{1}{\lambda}$来使用。</p>
<p>因此，这样就得到了在<strong>支持向量机</strong>中的我们的整个优化目标函数：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><hr>
<p>最后有别于<strong>逻辑回归</strong>的一点，对于<strong>支持向量机</strong>假设函数的形式如下：</p>
<script type="math/tex; mode=display">
h\_{\theta}(x) = 1 \ \ \ if \ \theta^Tx \ge 0</script><script type="math/tex; mode=display">
h\_{\theta}(x) = 0 \ \ \ if \ \theta^Tx \lt 0</script><p>而不是<strong>逻辑回归</strong>中的S型曲线：</p>
<script type="math/tex; mode=display">
h\_{\theta}(x)=\frac{1}{1+e^{-x}}</script><h2 id="大间距的直觉"><a href="#大间距的直觉" class="headerlink" title="大间距的直觉"></a>大间距的直觉</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/wrjaS/large-margin-intuition">视频地址</a></p>
<blockquote>
<p>人们有时将<strong>支持向量机</strong>看做是<strong>大间距分类器</strong>。在这一部分，我将介绍其中的含义，这有助于我们直观地理解SVM模型的假设是什么样的。</p>
</blockquote>
<p>这是我的支持向量机模型的代价函数：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><ul>
<li>如果你有一个正样本，即$y=1$时，那么代价函数$cost_{1}(z)$的图像如下：</li>
</ul>
<p><img src="/img/17_02_06/007.png" width = "300" height = "200" align=center /></p>
<p>可以看出，<strong>只有在$z\ge1$(即$\theta^{T}x\ge1$)时(不仅仅是$\ge0$)，代价函数$cost_{1}(z)$的值才等于0</strong>。</p>
<ul>
<li>反之，如果你有一个负样本，即$y=0$时，那么代价函数$cost_{0}(z)$的图像如下：</li>
</ul>
<p><img src="/img/17_02_06/008.png" width = "300" height = "200" align=center /></p>
<p>可以看出，<strong>只有在$z\le-1$(即$\theta^{T}x\le-1$)时(不仅仅是$\lt0$)，代价函数$cost_{0}(z)$的值才等于0</strong>。</p>
<p>这是<strong>支持向量机</strong>的一个有趣的性质。</p>
<h3 id="安全距离因子"><a href="#安全距离因子" class="headerlink" title="安全距离因子"></a>安全距离因子</h3><p>事实上，在逻辑回归中：</p>
<ul>
<li><p>如果你有一个正样本，即$y=1$的情况下，我们仅仅需要$\theta^{T}x\ge0$；</p>
</li>
<li><p>如果你有一个负样本，即$y=0$的情况下，我们仅仅需要$\theta^{T}x\lt0$；</p>
</li>
</ul>
<p>就能将该样本恰当的分类了。</p>
<p>但是<strong>支持向量机</strong>的要求更高，不仅仅要求$\theta^{T}x\ge0$或$\theta^{T}x\lt0$，而且要求$\theta^{T}x$比0大很多，或小很多。比如这里要求$\theta^{T}x\ge1$以及$\theta^{T}x\le-1$。</p>
<p>这就相当于在<strong>支持向量机</strong>中嵌入了一个额外的安全因子（或者说是安全距离因子）。接下来让我们来看看这个因子会导致什么结果：</p>
<p>具体而言，我接下来会将代价函数中的常数项$C$设置成一个非常大的值，比如100000或者其他非常大的数，然后再来观察支持向量机会给出什么结果。</p>
<p>当代价函数中</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
C
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><p>$C$的值非常大时，则最小化代价函数的时候，我们会很希望找到一个使第一项：</p>
<script type="math/tex; mode=display">
\sum\_{i=1}^{m}[
y^{(i)}
cost\_{1}(\theta^{T}x^{(i)})+
(1-y^{(i)})
cost\_{0}(\theta^{T}x^{(i)})
]</script><p>为0的最优解。</p>
<p>可以看到当输入一个正样本$y^{(i)}=1$时，我们想令上面这一项为0，从图中可以得出</p>
<p><img src="/img/17_02_06/007.png" width = "300" height = "200" align=center /></p>
<p>对于代价函数$cost_{1}(z)$我们需要使得$\theta^{T}x^{(i)}\ge1$。</p>
<p>类似地，对于一个负训练样本$y^{(i)}=0$时，我们想令上面这一项为0，从图中可以得出</p>
<p><img src="/img/17_02_06/008.png" width = "300" height = "200" align=center /></p>
<p>对于代价函数$cost_{0}(z)$我们需要使得$\theta^{T}x^{(i)}\le-1$。</p>
<hr>
<p>这样一来会产生下面这种优化问题：</p>
<p>因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
C0+
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><p>我们知道是$C0$的结果是0，因此可以删掉，所以最终得到的结果是：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><p>其中：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>这样我们就得到了一个非常有趣的决策边界。</p>
<h3 id="SVM决策边界：线性分割案例"><a href="#SVM决策边界：线性分割案例" class="headerlink" title="SVM决策边界：线性分割案例"></a>SVM决策边界：线性分割案例</h3><p>具体而言，如果你仔细观察下面这个既有正样本又有负样本的数据集</p>
<p><img src="/img/17_02_06/009.png" width = "300" height = "200" align=center /></p>
<p>不难看出，这个数据集是线性可分的（即存在一条直线把正负样本分开）。可以看出有很多直线都可以把正负样本区分开，比如下面这两条看起来不太自然的直线：</p>
<p><img src="/img/17_02_06/010.png" width = "300" height = "200" align=center /></p>
<p>支持向量机会选择黑色的这一条直线：</p>
<p><img src="/img/17_02_06/011.png" width = "300" height = "200" align=center /></p>
<p>这条直线看起来好很多，因为它看起来更加稳健。在数学上来讲就是这条直线拥有相对于训练数据更大的最短距离，这个所谓的距离就是指<strong>间距(margin)</strong>：</p>
<p><img src="/img/17_02_06/012.png" width = "300" height = "200" align=center /></p>
<p>而之前两条粉线和蓝线距离训练样本非常近，在分离样本时就会表现的比黑线差。</p>
<p>这就是<strong>支持向量机</strong>拥有<a target="_blank" rel="noopener" href="http://baike.baidu.com/link?url=My7Y1mL_9uj-XdR2DC2kyGLop-AaPdzSgdNmgRmaJVYV77puxNs-_A7ERwLv3uWih02JCu6esljRn90mc3EkMwKXBckm_6wSqU42EX06vC4ouxQhinIZco7crxr7HetC">鲁棒性</a>的原因。因为它一直努力用一个最大间距来分离样本。因此支持向量机分类器有时又被称为<strong>大间距分类器</strong>。</p>
<p>也许你想知道支持向量机是如何做到产生这个大间距分类器的，目前我还没解释这一点，在下一节中我会直观的来解释这一点。目前这个例子只是用于理解<strong>支持向量机模型</strong>的做法，即努力将正负样本用最大的间距区分开。</p>
<h3 id="大间距分类器中的异常值"><a href="#大间距分类器中的异常值" class="headerlink" title="大间距分类器中的异常值"></a>大间距分类器中的异常值</h3><p>最后要讲的一点是对于支持向量机中的异常数据的处理。在下面这组训练集中：</p>
<p><img src="/img/17_02_06/013.png" width = "300" height = "200" align=center /></p>
<p>我们通过使用支持向量机来进行分类，会得到这条黑色的决策边界，从而最大间距的区分这两种数据：</p>
<p><img src="/img/17_02_06/014.png" width = "300" height = "200" align=center /></p>
<p>当有一个异常值产生时：</p>
<p><img src="/img/17_02_06/015.png" width = "300" height = "200" align=center /></p>
<p>我们的算法会受到异常值的影响。这时我们将支持向量机中的正则化因子$C$设置的非常大，那么我们会得到类似这样一条粉色的决策边界：</p>
<p><img src="/img/17_02_06/016.png" width = "300" height = "200" align=center /></p>
<p>那么我们仅仅通过一个异常值，就将我们的决策边界旋转了这么大的角度，实在是不明智的。</p>
<p><img src="/img/17_02_06/017.png" width = "300" height = "200" align=center /></p>
<p>当我们的正则化因子$C$的值非常大时，支持向量机确实会如此处理，但如果我们适当的减小$C$的值，你最终还是会得到那条黑色的决策边界的。</p>
<p>如果数据是线性不可分的话，像这样：</p>
<p><img src="/img/17_02_06/018.png" width = "300" height = "200" align=center /></p>
<p>支持向量机也可以恰当的将它们分开。</p>
<p>值得提醒的是$C$的作用其实等同于$\frac{1}{\lambda}$，$\lambda$就是我们之前用到的正则化参数。在支持向量机中，$C$不是很大的时候，可以对包含异常数据、以及线性不可分的数据有比较好的处理效果。</p>
<p>稍后我们还会介绍支持向量机的偏差和方差，希望到那时候关于如何处理参数的这种平衡会变得更加清晰。</p>
<h2 id="大间距分类器背后的数学原理-选学"><a href="#大间距分类器背后的数学原理-选学" class="headerlink" title="大间距分类器背后的数学原理(选学)"></a>大间距分类器背后的数学原理(选学)</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/3eNnh/mathematics-behind-large-margin-classification">视频地址</a></p>
<blockquote>
<p>这一节将介绍大间距分类背后的数学原理。</p>
<p>本节作为选学内容，你完全可以跳过，但是听听这节课可能让你对支持向量机中的优化问题，以及如何得到大间距分类器产生更好的直观理解。</p>
</blockquote>
<h3 id="向量内积"><a href="#向量内积" class="headerlink" title="向量内积"></a>向量内积</h3><p>首先，带大家复习一下<strong>向量内积</strong>的知识。</p>
<p>假设我们有两个二维向量：</p>
<script type="math/tex; mode=display">
u=
 \begin{bmatrix}
   u\_1 \\\
    u\_2
 \end{bmatrix}
 \\
 v=
 \begin{bmatrix}
   v\_1 \\\
   v\_2
 \end{bmatrix}</script><p>我们把</p>
<script type="math/tex; mode=display">
u^{T}v</script><p>的计算结果称作向量$u$和$v$之间的<strong>内积</strong>。</p>
<p>由于这里我们用的是二维向量，因此我们可以把这两个向量绘制在同一坐标系内，向量$u$和$v$如下：</p>
<p><img src="/img/17_02_06/019.png" width = "300" height = "200" align=center /></p>
<p>其中我们用$||u||$来表示$u$的<strong>范数</strong>（即$u$的长度），因此$||u||$的计算公式如下：</p>
<script type="math/tex; mode=display">
||u||=\sqrt{u\_1^2+u\_2^2}</script><p>下面我们来看看<strong>向量内积</strong>具体是如何计算的：</p>
<p>将向量$v$投影到向量$u$上，如下图我们对向量$v$做一个相对于向量$u$的直角投影：</p>
<p><img src="/img/17_02_06/020.png" width = "300" height = "200" align=center /></p>
<p>投影之后的长度就是图中红线$p$的长度：</p>
<script type="math/tex; mode=display">
p = 向量v投影到向量u上的长度</script><p>同时也有另外一种计算内积的方式：</p>
<script type="math/tex; mode=display">
u^Tv=p·||u||</script><p>通过这种方式计算出来的内积，答案和之前也是一样的。</p>
<p>事实上，如果你想要使用将$u$投影到$v$上来用这种方式来计算内积，得到的答案也是相同的。</p>
<p>要注意的一点是，这里的$p$是有符号的，如果两个向量的夹角大于90°，像下图中这种情形，如果将$v$投影到$u$上会得到这样一种投影：</p>
<p><img src="/img/17_02_06/021.png" width = "300" height = "200" align=center /></p>
<p>此时的$p$就是一个负数。</p>
<blockquote>
<p>在向量的内积问题中，如果两个向量的夹角小于90°，那么$p$的符号就是为正；如果两个向量的夹角大于90°，那么$p$的符号就为负。</p>
</blockquote>
<p>这就是<strong>向量内积</strong>的知识，接下来我们尝试使用它来理解支持向量机中的目标函数。</p>
<h3 id="SVM决策边界"><a href="#SVM决策边界" class="headerlink" title="SVM决策边界"></a>SVM决策边界</h3><p>下面是我们在支持向量机中的目标函数:</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2</script><p>其中</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>接下来为了让目标函数更容易被分析，我们来忽略掉截距的影响，令$\theta_0=0$，这样更容易绘制示意图。并且我们将特征数$n$设置为2，因此我们仅有两个特征$x_1$和$x_2$：</p>
<script type="math/tex; mode=display">
\begin{align\*}
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
&=
\frac{1}{2}
(\theta\_1^2+\theta\_2^2)
\\\\&=
\frac{1}{2}
(\sqrt{\theta\_1^2+\theta\_2^2})^2
\end{align\*}</script><p>其中</p>
<script type="math/tex; mode=display">
\sqrt{\theta\_1^2+\theta\_2^2} = ||\theta||</script><p>因此可以得出：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
=
\frac{1}{2}||\theta||^2</script><p>可见，<strong>支持向量机所做的事情，其实就是在极小化参数向量$\theta$范数的平方（或者说是长度的平方）</strong>。</p>
<hr>
<p>现在让我们来看看这两行的含义：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>想一想$\theta^{T}x^{(i)}$这一项等于什么呢？</p>
<p>在前面我们画出了$u^Tv$的示意图，这里$\theta^T$就相当于$u^T$、$x^{(i)}$就相当于$v$。让我们来看一下示意图：</p>
<p>我们考虑一个单一的样本$x^{(i)}$，其坐标为$(x^{(i)}_1,x^{(i)}_2)$</p>
<p><img src="/img/17_02_06/022.png" width = "300" height = "200" align=center /></p>
<p>这个训练样本点其实可以表示为一个训练样本向量：</p>
<p><img src="/img/17_02_06/023.png" width = "300" height = "200" align=center /></p>
<p>现在，我们有一个参数向量：</p>
<p><img src="/img/17_02_06/024.png" width = "300" height = "200" align=center /></p>
<p>那么我们向量内积的计算方式，通过使用之前的方法可以得出。训练样本向量投影到参数向量上的长度$p^{(i)}$，表示第i个训练样本在参数向量$\theta$上的投影：</p>
<p><img src="/img/17_02_06/025.png" width = "300" height = "200" align=center /></p>
<p>根据之前我们所学到的，我们可以知道：</p>
<script type="math/tex; mode=display">
\begin{align\*}
\theta^Tx^{(i)}
&=p^{(i)}·||\theta||
\\\\
&=\theta\_1x\_1^{(i)}+\theta\_2x\_2^{(i)}
\end{align\*}</script><p>那么，这告诉我们了什么呢？这说明：</p>
<ul>
<li>若$y^{(i)}=1$时，则$\theta^{T}x^{(i)}\ge1$</li>
<li>若$y^{(i)}=0$时，则$\theta^{T}x^{(i)}\le-1$</li>
</ul>
<p>这里的约束项是可以用$p^{(i)}·||\theta||$来替代的：</p>
<ul>
<li>若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$</li>
<li>若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$</li>
</ul>
<p>因此，将其写入我们的优化目标后，<strong>完整的目标函数</strong>为：</p>
<script type="math/tex; mode=display">
\mathop{min}\limits\_{θ}
\frac{1}{2}
\sum\_{j=1}^{n}
\theta\_{j}^2
=
\frac{1}{2}||\theta||^2</script><p>其中</p>
<ul>
<li>若$y^{(i)}=1$时，则$p^{(i)}·||\theta||\ge1$</li>
<li>若$y^{(i)}=0$时，则$p^{(i)}·||\theta||\le-1$</li>
</ul>
<hr>
<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>现在让我们考虑下面这里的训练样本：</p>
<p><img src="/img/17_02_06/026.png" width = "300" height = "200" align=center /></p>
<p>其中假设截距依然为0，即$\theta_0=0$，我们来看一下支持向量机会选择什么样的决策边界。</p>
<p>假设有这样一条决策边界：</p>
<p><img src="/img/17_02_06/027.png" width = "300" height = "200" align=center /></p>
<p>很明显，这不是一个好的决策边界，因为这个决策边界离训练样本很近，我们来看一下为什么支持向量机不会选择它。</p>
<p>由于<strong>决策边界和参数向量是正交的(斜率相乘结果为-1)</strong>(<a target="_blank" rel="noopener" href="https://zhidao.baidu.com/question/1992397864989257747.html">为什么决策边界和参数向量是正交的</a>)，我们可以绘制出对应的参数向量$\theta$：</p>
<p><img src="/img/17_02_06/028.png" width = "300" height = "200" align=center /></p>
<blockquote>
<p>这里由于我们指定了$\theta_0=0$，也就意味着决策边界是过原点的。</p>
</blockquote>
<p>假设我们以这一点为第一个训练样本：</p>
<p><img src="/img/17_02_06/029.png" width = "300" height = "200" align=center /></p>
<p>我们可以画出这个样本向量到$\theta$的投影$p^{(1)}$：</p>
<p><img src="/img/17_02_06/030.png" width = "300" height = "200" align=center /></p>
<p>类似的，我们也可以画出第二个样本向量到$\theta$的投影$p^{(2)}$：</p>
<p><img src="/img/17_02_06/031.png" width = "300" height = "200" align=center /></p>
<p>我们会发现，这些$p^{(i)}$将会是一些非常小的数。因此当我们考察优化目标函数的时候：</p>
<ul>
<li><p>对于<strong>正样本($y^{(i)}=1$，即图中的”x”样本)</strong>而言，我们需要$p^{(i)}·||\theta||\ge1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。</p>
</li>
<li><p>对于<strong>负样本($y^{(i)}=-1$，即图中的”o”样本)</strong>而言，我们需要$p^{(i)}·||\theta||\le-1$，由于$p^{(i)}$非常小，也就意味着$||\theta||$需要非常大。</p>
</li>
</ul>
<p>但我们的实际目标是希望找到一个参数$\theta$，使得它的范数$||\theta||$是尽可能小的，因此这并不是一个好的决策边界，因为我们的$||\theta||$比较大。</p>
<hr>
<p>对于下面这个决策边界来说，情况就会有很大的不同：</p>
<p><img src="/img/17_02_06/032.png" width = "300" height = "200" align=center /></p>
<p>这里，我们以纵坐标作为决策边界，那么我们的参数向量的方向就是垂直于它的方向：</p>
<p><img src="/img/17_02_06/033.png" width = "300" height = "200" align=center /></p>
<p>如果我们现在再来绘制出样本向量在参数向量上的投影$p^{(1)}$和$p^{(2)}$的话，你会发现这些投影的长度比之前长多了：</p>
<p><img src="/img/17_02_06/034.png" width = "300" height = "200" align=center /></p>
<p>因为投影$p$的长度变大了，随之$\theta$的范数$||\theta||$也相应的变小了。这就意味着通过选择第二种远离样本的决策边界，支持向量机可以使参数$\theta$的范数$||\theta||$变小很多。</p>
<p>这就是<strong>为什么支持向量机可以产生大间距分类的原因</strong>。</p>
<hr>
<p>最后一点，我们的推导自始至终都使用了<strong>截距为0（即$\theta_0=0$）</strong>这个简化假设。这样做的作用就是可以使得决策边界始终是通过原点的，如果你的决策边界不过原点，那么$\theta_0\ne0$，但支持向量机会产生大间距分类器的结论依然成立（具体推导过程不再叙述，和这里很类似）。但是可以说明的是，即便$\theta_0\ne0$，支持向量机仍然会找到正样本和负样本之间的大间距分隔。总之 我们解释了为什么支持向量机是一个大间距分类器。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/04/%E3%80%90Python3%E6%95%99%E7%A8%8B%20%E7%AC%AC3%E7%AB%A0%E3%80%91%E6%96%B9%E6%B3%95%E5%92%8C%E6%A8%A1%E5%9D%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/04/%E3%80%90Python3%E6%95%99%E7%A8%8B%20%E7%AC%AC3%E7%AB%A0%E3%80%91%E6%96%B9%E6%B3%95%E5%92%8C%E6%A8%A1%E5%9D%97/" class="post-title-link" itemprop="url">【Python3教程 第3章】方法和模块</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-04 17:17:58" itemprop="dateCreated datePublished" datetime="2017-02-04T17:17:58+00:00">2017-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="代码复用"><a href="#代码复用" class="headerlink" title="代码复用"></a>代码复用</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2017/02/04/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AD%E5%91%A8%20(5)%E4%BD%BF%E7%94%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="DannyLee">
      <meta itemprop="description" content="愿你的努力终取得成果">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="圣巢">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/02/04/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%20%E7%AC%AC%E5%85%AD%E5%91%A8%20(5)%E4%BD%BF%E7%94%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86/" class="post-title-link" itemprop="url">斯坦福机器学习课程 第六周 (5)使用大数据集</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-04 00:55:58" itemprop="dateCreated datePublished" datetime="2017-02-04T00:55:58+00:00">2017-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-11 03:45:02" itemprop="dateModified" datetime="2021-02-11T03:45:02+00:00">2021-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="机器学习的数据"><a href="#机器学习的数据" class="headerlink" title="机器学习的数据"></a>机器学习的数据</h2><p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning/lecture/XcNcz/data-for-machine-learning">视频地址</a></p>
<blockquote>
<p>在之前的视频中，我们讨论了评价指标。在本节课的视频中，我们来讨论一下机器学习系统设计中另一个重要的方面，这往往涉及到用来训练的数据有多少。</p>
<p>在之前的一些视频中，我曾告诫大家不要一开始就盲目地花大量的时间来收集大量的数据，因为这种做法只在某些情况下能起到实际的作用。但事实证明，在一定条件下，得到大量的数据，并在某种类型的学习算法中进行训练，却是一种有效的获得一个具有良好性能的学习算法的方法。而这种情况往往出现在这些条件对于你的问题都成立，并且你能够得到大量数据的情况下。这是一个很好的获得非常高性能的学习算法的方式。</p>
</blockquote>
<p>我先讲一个故事。很多很多年前，我认识的两位研究人员 Michele Banko 和 Eric Brill，他们进行了一项有趣的研究：研究在不同的数据集上使用不同的学习算法的效果。他们当时考虑的问题是混淆词分类问题，例如，下面这个句子：</p>
<ul>
<li><strong>For breakfast I ate __ eggs.</strong></li>
</ul>
<p>在这个句子中，空白处的可选词是：</p>
<ul>
<li><strong>to</strong></li>
<li><strong>two</strong></li>
<li><strong>too</strong></li>
</ul>
<p>在这个例子中应该填入<strong>two</strong>。</p>
<p>于是他们把诸如这样的机器学习问题当做一类监督学习问题，并尝试将其分类：什么样的词在一个英文句子特定的位置才是合适的。</p>
<p>他们用了下面几种不同的学习算法，这些算法在他们2001年进行研究的时候都已经 被公认是比较领先的学习算法：</p>
<ul>
<li><strong>感知器(Perceptron) (逻辑回归)</strong></li>
<li><strong>Winnow(类似于回归问题，但不完全相同。过去常用，但现在很少用到)</strong></li>
<li><strong>基于内存的学习方法(Memory-based)</strong></li>
<li><strong>朴素贝叶斯</strong></li>
</ul>
<p>这些算法的具体细节不重要，总之他们使用了这几种算法。他们所做的就是改变了训练数据集的大小，并尝试将这些学习算法用于不同大小的训练数据集中。这就是他们得到的结果：</p>
<p><img src="/img/17_02_04/001.png" width = "300" height = "200" align=center /></p>
<p>横轴代表以百万为单位的数据集大小，即从0.1百万到1000百万（10亿）规模的数据集；纵轴代表精确度，范围是0到1。</p>
<p>从图中可以看出趋势非常明显。首先大部分算法都具有相似的性能，其次随着训练数据集的增大，这些算法的准确性也都对应的增强了。事实上，如果你选择任意一个算法，例如选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些列子中看起来，它很有可能会其他算法更好，甚至会比”优等算法”更好。</p>
<p>由于这项原始的研究非常具有影响力，已经有一系列不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的效果，虽然这些效果的表现还取决于一些细节，但是真正能提高算法性能(准确度)的，就是你能够给到一个算法的大量的训练数据。类似这样的结论引起了一种在机器学习中的普遍共识：</p>
<ul>
<li><p><strong>It`s not who has the best algrithm that wins.It`s who has the most data.</strong></p>
</li>
<li><p><strong>取得成功的人不是因为拥有最好算法，而是因为拥有最多数据。</strong></p>
</li>
</ul>
<p>那么这种说法在什么时候是靠谱的，什么时候是不靠谱的呢？因为如果上面的说法在所有情况下都是对的话，那么我们保证我们得到一个高性能的学习算法的最佳方式应该是获取大量的数据，而不是考虑该使用什么学习算法。</p>
<p>其实，在下面这种情况下，获取大量的数据是提高算法性能的好方法：</p>
<p><strong>在特征值x包含足够多的用来准确预测y的信息时，获取大量的数据是提高算法性能的好方法。</strong></p>
<p>例如在上面介绍的混淆词的例子中：</p>
<ul>
<li><strong>For breakfast I ate __ eggs.</strong></li>
</ul>
<p>空白词附近的词就是我们需要捕捉的特征<strong>x</strong>，这些词中包含了大量的信息来告诉我空白处应该填写<strong>two</strong>而不是<strong>to</strong>或者<strong>too</strong>。实际上对于特征捕捉，哪怕是周围词语中的一个词，就能够给我足够的信息来确定出标签<strong>y</strong>是什么了。</p>
<p>这就是通过一个有充足的信息的特征值<strong>x</strong>的来确定<strong>y</strong>的例子。举一个反例：</p>
<p>设想在房屋价格预测问题中，我们获取的房屋信息中只有房屋面积信息，没有其他的特征值，那么如果我告诉你这个房子有500平方英尺，但是我没有告诉你其他的特征信息，我也不告诉你这个房子位于这个城市房价比较昂贵的区域还是便宜的区域，也不告诉你这所房子的房间数量，它里面陈设了多漂亮的家具，以及这个房子是新的还是旧的。我不告诉你除了这个房子有500平方英尺以外的其他任何信息，然而除此之外还有许多其他因素会影响房子的价格，不仅仅是房子的大小。但如果你仅仅知道房屋的尺寸，那么事实上是很难准确预测它的价格的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="DannyLee"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">DannyLee</p>
  <div class="site-description" itemprop="description">愿你的努力终取得成果</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">136</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DannyLee</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
