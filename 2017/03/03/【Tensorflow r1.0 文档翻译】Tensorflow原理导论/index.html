<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>【Tensorflow r1.0 文档翻译】Tensorflow原理导论 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="代码：tensorflow&#x2F;examples&#x2F;tutorials&#x2F;mnist&#x2F; 这篇教程的目的是为了展示如何使用TensorFlow来训练并评估一个简单的**前馈神经网络(feed-forward neural network)**用来识别MNIST手写数字数据集。本教程的目标读者是有兴趣使用TensorFlow的有经验的机器学习用户。 这部分教程不是为了教授普通的机器学习。 请确保您已按照说明安">
<meta property="og:type" content="article">
<meta property="og:title" content="【Tensorflow r1.0 文档翻译】Tensorflow原理导论">
<meta property="og:url" content="http://example.com/2017/03/03/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91Tensorflow%E5%8E%9F%E7%90%86%E5%AF%BC%E8%AE%BA/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="代码：tensorflow&#x2F;examples&#x2F;tutorials&#x2F;mnist&#x2F; 这篇教程的目的是为了展示如何使用TensorFlow来训练并评估一个简单的**前馈神经网络(feed-forward neural network)**用来识别MNIST手写数字数据集。本教程的目标读者是有兴趣使用TensorFlow的有经验的机器学习用户。 这部分教程不是为了教授普通的机器学习。 请确保您已按照说明安">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/17_03_03/001.png">
<meta property="og:image" content="http://example.com/img/17_03_03/002.png">
<meta property="og:image" content="http://example.com/img/17_03_03/003.png">
<meta property="article:published_time" content="2017-03-03T21:19:58.000Z">
<meta property="article:modified_time" content="2021-02-08T08:48:41.853Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Tensorflow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/17_03_03/001.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-【Tensorflow r1.0 文档翻译】Tensorflow原理导论" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/03/03/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91Tensorflow%E5%8E%9F%E7%90%86%E5%AF%BC%E8%AE%BA/" class="article-date">
  <time class="dt-published" datetime="2017-03-03T21:19:58.000Z" itemprop="datePublished">2017-03-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>►<a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/">Tensorflow</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      【Tensorflow r1.0 文档翻译】Tensorflow原理导论
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>代码：<a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/">tensorflow/examples/tutorials/mnist/</a></p>
<p>这篇教程的目的是为了展示如何使用TensorFlow来训练并评估一个简单的**前馈神经网络(feed-forward neural network)**用来识别MNIST手写数字数据集。本教程的目标读者是有兴趣使用TensorFlow的有经验的机器学习用户。</p>
<p>这部分教程不是为了教授普通的机器学习。</p>
<p>请确保您已按照说明<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/index">安装了TensorFlow</a>。</p>
<h2 id="教程文件"><a href="#教程文件" class="headerlink" title="教程文件"></a>教程文件</h2><p>本教程引用以下文件：</p>
<table>
<thead>
<tr>
<th align="left">文件</th>
<th align="left">目标</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/mnist.py"><code>mnist.py</code></a></td>
<td align="left">构建一个完全连接的MNIST模型的代码。</td>
</tr>
<tr>
<td align="left"><a target="_blank" rel="noopener" href="https://www.tensorflow.org/code/tensorflow/examples/tutorials/mnist/fully_connected_feed.py"><code>fully_connected_feed.py</code></a></td>
<td align="left">利用下载的数据集训练构建好的MNIST模型的主要代码，以数据反馈字典（feed dictionary）的形式作为输入模型。</td>
</tr>
</tbody></table>
<p>只需要运行<code>fully_connected_feed.py</code>文件，就可以开启训练：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python fully_connected_feed.py</span><br></pre></td></tr></table></figure>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>MNIST是机器学习中的经典问题。这个问题是查看28x28像素的手写数字灰度图像，并确定图像表示的数字，数字范围是0到9。</p>
<p><img src="/img/17_03_03/001.png"></p>
<p>更多的信息，参加<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">Yann LeCun’s MNIST page</a>或者<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/">Chris Olah’s visualizations of MNIST</a>。</p>
<h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><p>在<code>run_training()</code>方法的开始部分，<code>input_data.read_data_sets()</code>方法会确保你的本地训练文件夹中，已经下载了正确的数据，然后将这些数据解压并返回一个含有<code>DataSet</code>实例的字典。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_sets &#x3D; input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong><code>fake_data</code>标记是用于单元测试的，读者可以不必理会。</p>
<table>
<thead>
<tr>
<th align="left">数据集</th>
<th align="left">目标</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>data_sets.train</code></td>
<td align="left">55000图像和标签，用于初级训练。</td>
</tr>
<tr>
<td align="left"><code>data_sets.validation</code></td>
<td align="left">5000图像和标签，用于迭代验证训练准确性。</td>
</tr>
<tr>
<td align="left"><code>data_sets.test</code></td>
<td align="left">10000图像和标签，用于最终测试训练的准确性。</td>
</tr>
</tbody></table>
<h3 id="输入和占位符"><a href="#输入和占位符" class="headerlink" title="输入和占位符"></a>输入和占位符</h3><p><code>placeholder_inputs()</code>方法创建了两个<code>tf.placeholder</code>操作，用于定义输入的形状。形状参数中包含<code>batch_size</code>值，后续还会将实际的训练样本传入图中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">images_placeholder &#x3D; tf.placeholder(tf.float32, shape&#x3D;(batch_size,</span><br><span class="line">                                                       mnist.IMAGE_PIXELS))</span><br><span class="line">labels_placeholder &#x3D; tf.placeholder(tf.int32, shape&#x3D;(batch_size))</span><br></pre></td></tr></table></figure>
<p>在训练的循环代码的下方，传入的整个图像和标签数据集会被切片，以符合每一个操作所设置的<code>batch_size</code>值，占位符操作将会填补以符合这个<code>batch_size</code>值。然后使用<code>feed_dict</code>参数，将数据传入<code>sess.run()</code>函数。</p>
<h2 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h2><p>在为数据创建占位符之后，就可以运行<code>mnist.py</code>文件，经过三阶段的模式函数操作：<code>inference()</code>， <code>loss()</code>，和<code>training()</code>。图表就构建完成了。</p>
<ul>
<li>1.<code>inference()</code>-尽可能地构建好图表，满足促使神经网络向前反馈并做出预测的要求。</li>
<li>2.<code>loss()</code>-往inference图表中添加生成损失（loss）所需要的操作（ops）。</li>
<li>3.<code>training()</code>-往损失图表中添加计算并应用梯度（gradients）所需的操作。</li>
</ul>
<p><img src="/img/17_03_03/002.png"></p>
<h3 id="推理-Inference"><a href="#推理-Inference" class="headerlink" title="推理(Inference)"></a>推理(Inference)</h3><p><code>inference()</code>函数会尽可能地构建图表，做到返回包含了预测结果（output prediction）的Tensor。</p>
<p>它采用图像占位符作为输入，并在其上借助<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>激活函数构建一对完全连接层，以及一个有着十个节点、指明了输出logtis模型的线性层。</p>
<p>每个图层都在唯一的<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/name_scope"><code>tf.name_scope</code></a>下创建，创建于该作用域之下的所有元素都将带有其前缀。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#39;hidden1&#39;):</span><br></pre></td></tr></table></figure>
<p>在定义的范围内，由这些层中的每一个使用的权重和偏差被生成为<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Variable"><code>tf.Variable</code></a>实例，具有它们期望的形状：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weights &#x3D; tf.Variable(</span><br><span class="line">    tf.truncated_normal([IMAGE_PIXELS, hidden1_units],</span><br><span class="line">                        stddev&#x3D;1.0 &#x2F; math.sqrt(float(IMAGE_PIXELS))),</span><br><span class="line">    name&#x3D;&#39;weights&#39;)</span><br><span class="line">biases &#x3D; tf.Variable(tf.zeros([hidden1_units]),</span><br><span class="line">                     name&#x3D;&#39;biases&#39;)</span><br></pre></td></tr></table></figure>
<p>例如，当在<code>hidden1</code>范围下创建这些时，赋予权重变量的唯一名称将是“<code>hidden1 / weights</code>”。</p>
<p>每个变量在构建时，都会执行初始化操作。</p>
<p>在大多数情况下，通过<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal"><code>tf.truncated_normal</code></a>函数初始化权重变量，给赋予的shape则是一个二维tensor，其中第一个维度代表该层中权重变量所连接（connect from）的单元数量，第二个维度代表该层中权重变量所连接到的（connect to）单元数量。第一层，名字为<code>hidden1</code>，它的尺寸是<code>[IMAGE_PIXELS, hidden1_units]</code>，因为权重变量将图像输入连接到了<code>hidden1</code>层。<code>tf.truncated_normal</code>初始函数将根据所得到的均值和标准差，生成一个随机分布。</p>
<p>然后，通过<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/zeros"><code>tf.zeros</code></a>函数初始化偏差变量（biases），确保所有偏差的起始值都是0，而它们的形状则是其在该层中所接到的（connect to）单元数量。</p>
<p>图表的三个主要操作，分别是两个<code>tf.nn.relu</code>操作，它们中嵌入了隐藏层所需的<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/matmul"><code>tf.matmul</code></a>；以及logits模型所需的另外一个<code>tf.matmul</code>。三者依次生成，各自的<code>tf.Variable</code>实例则与输入占位符或下一层的输出tensor所连接。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden1 &#x3D; tf.nn.relu(tf.matmul(images, weights) + biases)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden2 &#x3D; tf.nn.relu(tf.matmul(hidden1, weights) + biases)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits &#x3D; tf.matmul(hidden2, weights) + biases</span><br></pre></td></tr></table></figure>
<p>最终，程序会返回包含了输出结果的<code>logits</code>Tensor。</p>
<h3 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h3><p><code>loss()</code>函数通过添加所需的损失操作，进一步构建图表。</p>
<p>首先，来自<code>labels_placeholder</code>的值将转换为64位整数。然后，添加一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits">tf.nn.sparse_softmax_cross_entropy_with_logits</a>操作，以从<code>labels_placeholder</code>自动生成1-hot标签，并且与<code>inference()</code>函数的输出logits与那些1-hot标签进行比较。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">labels &#x3D; tf.to_int64(labels)</span><br><span class="line">cross_entropy &#x3D; tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    labels&#x3D;labels, logits&#x3D;logits, name&#x3D;&#39;xentropy&#39;)</span><br></pre></td></tr></table></figure>
<p>然后使用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean"><code>tf.reduce_mean</code></a>来求在批量维度（第一维度）上的交叉熵的平均值，作为总损失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; tf.reduce_mean(cross_entropy, name&#x3D;&#39;xentropy_mean&#39;)</span><br></pre></td></tr></table></figure>
<p>然后将包含损失值的张量返回。</p>
<blockquote>
<p><strong>注意：</strong>交叉熵是信息论中的一种理论，它用于描述神经网络的预测结果相对于实际所给定的真实结果的偏差程度。更多的信息，请参阅博文<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-09-Visual-Information/">《可视化信息理论》</a>。</p>
</blockquote>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><code>training()</code>方法通过添加<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降</a>的操作来最小化损失。</p>
<p>首先，它通过<code>loss()</code>方法接受损失tensor，然后传递到<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar"><code>tf.summary.scalar</code></a>，用于在与<code>SummaryWriter</code>（见下文）一起使用时生成事件文件中的摘要值的操作。在这里，它将在每次写出摘要时发出损失的快照值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.summary.scalar(&#39;loss&#39;, loss)</span><br></pre></td></tr></table></figure>
<p>接下来，我们实例化一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"><code>tf.train.GradientDescentOptimizer</code></a>，负责按照所要求的学习效率（learning rate）应用梯度下降法（gradients）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer &#x3D; tf.train.GradientDescentOptimizer(learning_rate)</span><br></pre></td></tr></table></figure>
<p>之后，我们生成一个单个的变量用于统计全局训练的次数，<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#minimize"><code>tf.train.Optimizer.minimize</code></a>操作被同时用作在系统中更新可训练的权值，以及增加全局步长（global step）。按照惯例，这个操作被称为<code>train_op</code>，TensorFlow会话必须运行的，以便引入一个完整的训练步骤（见下文）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">global_step &#x3D; tf.Variable(0, name&#x3D;&#39;global_step&#39;, trainable&#x3D;False)</span><br><span class="line">train_op &#x3D; optimizer.minimize(loss, global_step&#x3D;global_step)</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>一旦图被构建，它就可以在由<code>fully_connected_feed.py</code>中的用户代码控制的循环中迭代地训练和求值。</p>
<h3 id="图"><a href="#图" class="headerlink" title="图"></a>图</h3><p>在<code>run_training()</code>方法的一开始的部分，是一个python的<code>with</code>命令，这表示所有构建的操作将与默认全局<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Graph"><code>tf.Graph</code></a>实例相关联。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.Graph().as_default():</span><br></pre></td></tr></table></figure>
<p><code>tf.Graph</code>实例是一系列可以作为整体执行的操作。TensorFlow的大部分场景只需要依赖默认图表一个实例即可。</p>
<p>利用多个图表的更加复杂的使用场景也是可能的，但是超出了本教程的范围。</p>
<h3 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h3><p>完成全部的构建准备、生成全部所需的操作之后，我们就可以创建一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Session">tf.Session</a>，用于运行图表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess &#x3D; tf.Session()</span><br></pre></td></tr></table></figure>
<p>另外，也可以利用<code>with</code>代码块生成<code>Session</code>，限制作用域：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br></pre></td></tr></table></figure>
<p><code>Session</code>函数中没有传入参数，表明该代码将会依附于（如果还没有创建会话，则会创建新的会话）默认的本地会话。</p>
<p>生成会话之后，所有<code>tf.Variable</code>实例都会立即通过调用各自初始化操作中的<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Session#run"><code>tf.Session.run</code></a>函数进行初始化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init &#x3D; tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/Session#run"><code>tf.Session.run</code></a>方法将会运行图表中与作为参数传入的操作相对应的完整子集。在初次调用时，<code>init</code>操作只包含了变量初始化程序<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/group"><code>tf.group</code></a>。图表的其他部分不会在这里，而是在下面的训练循环运行。</p>
<h3 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h3><p>在通过会话来初始化变量后，就可以开始训练了。</p>
<p>训练的每一步都是通过用户代码控制，而能实现有效训练的最简单循环就是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for step in xrange(FLAGS.max_steps):</span><br><span class="line">    sess.run(train_op)</span><br></pre></td></tr></table></figure>
<p>但是，本教程中的例子要更为复杂一点，原因是我们必须把输入的数据根据每一步的情况进行切分，以匹配之前生成的占位符。</p>
<h3 id="向图表提供反馈"><a href="#向图表提供反馈" class="headerlink" title="向图表提供反馈"></a>向图表提供反馈</h3><p>执行每一步时，我们的代码会生成一个反馈字典（feed dictionary），其中包含对应步骤中训练所要使用的样本，这些样本的key就是其所代表的占位符操作。</p>
<p><code>fill_feed_dict</code>函数会查询给定的<code>DataSet</code>，索要下一批次`batch_size的图像和标签，与占位符相匹配的Tensor则会包含下一批次的图像和标签。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">images_feed, labels_feed &#x3D; data_set.next_batch(FLAGS.batch_size,</span><br><span class="line">                                               FLAGS.fake_data)</span><br></pre></td></tr></table></figure>
<p>然后，以占位符作为键，创建一个Python字典对象，值则是其代表的反馈Tensor。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feed_dict &#x3D; &#123;</span><br><span class="line">    images_placeholder: images_feed,</span><br><span class="line">    labels_placeholder: labels_feed,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个字典随后作为<code>feed_dict</code>参数，传入<code>sess.run()</code>函数中，为这一步的训练提供输入样本。</p>
<h3 id="检查状态"><a href="#检查状态" class="headerlink" title="检查状态"></a>检查状态</h3><p>在运行<code>sess.run</code>时，要在代码中明确其需要获取的两个值：<code>[train_op, loss]</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for step in xrange(FLAGS.max_steps):</span><br><span class="line">    feed_dict &#x3D; fill_feed_dict(data_sets.train,</span><br><span class="line">                               images_placeholder,</span><br><span class="line">                               labels_placeholder)</span><br><span class="line">    _, loss_value &#x3D; sess.run([train_op, loss],</span><br><span class="line">                             feed_dict&#x3D;feed_dict)</span><br></pre></td></tr></table></figure>
<p>因为要获取这两个值，<code>sess.run()</code>会返回一个有两个元素的元组。其中每一个<code>Tensor</code>对象，对应了返回的元组中的numpy数组，而这些数组中包含了当前这步训练中对应Tensor的值。由于<code>train_op</code>并不会产生输出，其在返回的元祖中的对应元素就是<code>None</code>，所以会被抛弃。但是，如果模型在训练中出现偏差，<code>loss</code> Tensor的值可能会变成NaN，所以我们要获取它的值，并记录下来。</p>
<p>假设训练一切正常，没有出现NaN，训练循环会每隔100个训练步骤，就打印一行简单的状态文本，告知用户当前的训练状态。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if step % 100 &#x3D;&#x3D; 0:</span><br><span class="line">    print &#39;Step %d: loss &#x3D; %.2f (%.3f sec)&#39; % (step, loss_value, duration)</span><br></pre></td></tr></table></figure>
<h3 id="状态可视化"><a href="#状态可视化" class="headerlink" title="状态可视化"></a>状态可视化</h3><p>为了发出<a target="_blank" rel="noopener" href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a>所使用的事件文件（events file），所有的摘要（在这里只有一个）都要在图构建阶段合并至一个Tensor中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary &#x3D; tf.summary.merge_all()</span><br></pre></td></tr></table></figure>
<p>在创建好会话（session）之后，可以实例化一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter"><code>tf.summary.FileWriter</code></a>，用于写入包含了图表本身和即时数据具体值的事件文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary_writer &#x3D; tf.summary.FileWriter(FLAGS.train_dir, sess.graph)</span><br></pre></td></tr></table></figure>
<p>最后，每次评估<code>summary</code>(摘要)并将输出传递给<code>add_summary()</code>函数时，事件文件将被新的摘要值更新。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">summary_str &#x3D; sess.run(summary, feed_dict&#x3D;feed_dict)</span><br><span class="line">summary_writer.add_summary(summary_str, step)</span><br></pre></td></tr></table></figure>
<p>事件文件写入完毕之后，可以就训练文件夹打开一个TensorBoard，查看即时数据的情况。</p>
<p><img src="/img/17_03_03/003.png"></p>
<blockquote>
<p><strong>注意：</strong>了解更多如何构建并运行TensorBoard的信息，请查看相关教程<a target="_blank" rel="noopener" href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">Tensorboard：训练过程可视化</a>。</p>
</blockquote>
<h3 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h3><p>为了得到可以用来后续恢复模型以进一步训练或评估的检查点文件（checkpoint file），我们实例化一个<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Saver"><code>tf.train.Saver</code></a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver &#x3D; tf.train.Saver()</span><br></pre></td></tr></table></figure>
<p>在训练循环中，将定期调用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#save"><code>tf.train.Saver.save</code></a>方法，使用所有可训练变量的当前值将检查点文件写入训练目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.save(sess, FLAGS.train_dir, global_step&#x3D;step)</span><br></pre></td></tr></table></figure>
<p>在将来的某个时间点，可以通过使用<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/train/Saver#restore"><code>tf.train.Saver.restore</code></a>方法重新加载模型参数来恢复训练。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.restore(sess, FLAGS.train_dir)</span><br></pre></td></tr></table></figure>
<h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><p>每隔一千个训练步骤，我们的代码会尝试使用训练数据集与测试数据集，对模型进行评估。<code>do_eval</code>函数会被调用三次，分别使用训练数据集、验证数据集合测试数据集。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">print &#39;Training Data Eval:&#39;</span><br><span class="line">do_eval(sess,</span><br><span class="line">        eval_correct,</span><br><span class="line">        images_placeholder,</span><br><span class="line">        labels_placeholder,</span><br><span class="line">        data_sets.train)</span><br><span class="line">print &#39;Validation Data Eval:&#39;</span><br><span class="line">do_eval(sess,</span><br><span class="line">        eval_correct,</span><br><span class="line">        images_placeholder,</span><br><span class="line">        labels_placeholder,</span><br><span class="line">        data_sets.validation)</span><br><span class="line">print &#39;Test Data Eval:&#39;</span><br><span class="line">do_eval(sess,</span><br><span class="line">        eval_correct,</span><br><span class="line">        images_placeholder,</span><br><span class="line">        labels_placeholder,</span><br><span class="line">        data_sets.test)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意，更复杂的使用场景通常是，先隔绝<code>data_sets.test</code>测试数据集，只有在大量的超参数优化调整（hyperparameter tuning）之后才进行检查。但是，由于MNIST问题比较简单，我们在这里一次性评估所有的数据。</p>
</blockquote>
<h3 id="构建评估图-Eval-Graph"><a href="#构建评估图-Eval-Graph" class="headerlink" title="构建评估图(Eval Graph)"></a>构建评估图(Eval Graph)</h3><p>在打开默认图表（Graph）之前，我们应该先调用get_data(train=False)函数，抓取测试数据集。</p>
<p>在进入训练循环之前，评估操作应该通过<code>mnist.py</code>中的<code>evaluate()</code>函数来构建。<code>evaluate()</code>传入的<code>logist</code>和标签参数与<code>loss()</code>函数相同。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval_correct &#x3D; mnist.evaluation(logits, labels_placeholder)</span><br></pre></td></tr></table></figure>
<p><code>evaluation()</code>函数会生成<a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k"><code>tf.nn.in_top_k</code></a>操作，如果在K个最有可能的预测中可以发现真的标签，那么这个操作就会将模型输出标记为正确。在本文中，我们把K的值设置为1，也就是只有在预测是真的标签时，才判定它是正确的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eval_correct &#x3D; tf.nn.in_top_k(logits, labels, 1)</span><br></pre></td></tr></table></figure>
<h3 id="评估输出"><a href="#评估输出" class="headerlink" title="评估输出"></a>评估输出</h3><p>之后，我们可以创建一个循环，往其中添加<code>feed_dict</code>，并在调用<code>sess.run()</code>函数时传入<code>eval_correct</code>操作，目的就是用给定的数据集评估模型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for step in xrange(steps_per_epoch):</span><br><span class="line">    feed_dict &#x3D; fill_feed_dict(data_set,</span><br><span class="line">                               images_placeholder,</span><br><span class="line">                               labels_placeholder)</span><br><span class="line">    true_count +&#x3D; sess.run(eval_correct, feed_dict&#x3D;feed_dict)</span><br></pre></td></tr></table></figure>
<p><code>true_count</code>变量会累加所有<code>in_top_k</code>操作判定为正确的预测之和。接下来，只需要将正确测试的总数，除以例子总数，就可以得出准确率了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">precision &#x3D; true_count &#x2F; num_examples</span><br><span class="line">print(&#39;  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f&#39; %</span><br><span class="line">      (num_examples, true_count, precision))</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/03/03/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91Tensorflow%E5%8E%9F%E7%90%86%E5%AF%BC%E8%AE%BA/" data-id="ckkwc436j004dpas91t9e8dam" data-title="【Tensorflow r1.0 文档翻译】Tensorflow原理导论" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/03/05/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E3%80%90tf.contrib.learn%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E3%80%91/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          【Tensorflow r1.0 文档翻译】【tf.contrib.learn快速入门】
        
      </div>
    </a>
  
  
    <a href="/2017/02/26/%E3%80%90Tensorflow%20r1.0%20%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%E3%80%91%E6%B7%B1%E5%85%A5MNIST--%E4%B8%93%E5%AE%B6%E7%BA%A7/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">【Tensorflow r1.0 文档翻译】深入MNIST--专家级</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/%E5%B7%A5%E5%85%B7/">工具</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/R/">R</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/django/">django</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/gradle/">gradle</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/other/">other</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7%E5%AD%A6%E4%B9%A0/">工具学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/">数学</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/%E5%85%AC%E5%BC%80%E8%AF%BE/">公开课</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Tensorflow/">Tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/cs231n/">cs231n</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">线性代数</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-QI/" rel="tag">AI-QI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Android/" rel="tag">Android</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/" rel="tag">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/" rel="tag">django</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gradle/" rel="tag">gradle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kaggle/" rel="tag">kaggle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/" rel="tag">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag">人工智能</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" rel="tag">傅里叶变换</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%85%B6%E4%BB%96/" rel="tag">其他</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E5%85%B7%E5%AD%A6%E4%B9%A0/" rel="tag">工具学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag">数学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%85%AC%E5%BC%80%E8%AF%BE/" rel="tag">斯坦福大学公开课</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B/" rel="tag">斯坦福课程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" rel="tag">线性代数</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BF%BB%E8%AF%91/" rel="tag">翻译</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/" rel="tag">论文翻译</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" rel="tag">设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag">读书笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI-QI/" style="font-size: 10px;">AI-QI</a> <a href="/tags/Android/" style="font-size: 18px;">Android</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/R/" style="font-size: 11px;">R</a> <a href="/tags/Tensorflow/" style="font-size: 17px;">Tensorflow</a> <a href="/tags/django/" style="font-size: 11px;">django</a> <a href="/tags/gradle/" style="font-size: 16px;">gradle</a> <a href="/tags/java/" style="font-size: 11px;">java</a> <a href="/tags/kaggle/" style="font-size: 10px;">kaggle</a> <a href="/tags/linux/" style="font-size: 13px;">linux</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/scala/" style="font-size: 10px;">scala</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 10px;">人工智能</a> <a href="/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/" style="font-size: 11px;">傅里叶变换</a> <a href="/tags/%E5%85%B6%E4%BB%96/" style="font-size: 10px;">其他</a> <a href="/tags/%E5%B7%A5%E5%85%B7%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">工具学习</a> <a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 12px;">数学</a> <a href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%85%AC%E5%BC%80%E8%AF%BE/" style="font-size: 11px;">斯坦福大学公开课</a> <a href="/tags/%E6%96%AF%E5%9D%A6%E7%A6%8F%E8%AF%BE%E7%A8%8B/" style="font-size: 19px;">斯坦福课程</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">神经网络</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" style="font-size: 16px;">线性代数</a> <a href="/tags/%E7%BF%BB%E8%AF%91/" style="font-size: 10px;">翻译</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/" style="font-size: 10px;">论文翻译</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" style="font-size: 15px;">设计模式</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 11px;">读书笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/11/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/">隐马尔科夫模型</a>
          </li>
        
          <li>
            <a href="/2018/10/29/%E3%80%90%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E3%80%9102-%E5%B0%86%E4%B8%80%E8%88%AC%E5%91%A8%E6%9C%9F%E5%87%BD%E6%95%B0%E8%A1%A8%E7%A4%BA%E4%B8%BA%E7%AE%80%E5%8D%95%E5%91%A8%E6%9C%9F/">【傅里叶变换及其应用讲义】第一章 傅里叶级数</a>
          </li>
        
          <li>
            <a href="/2018/10/27/%E3%80%90%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%E3%80%9101-%E5%91%A8%E6%9C%9F%E6%80%A7%EF%BC%8C%E4%B8%89%E8%A7%92%E5%87%BD%E6%95%B0%E8%A1%A8%E7%A4%BA%E5%A4%8D%E6%9D%82%E5%87%BD%E6%95%B0/">【傅里叶变换及其应用】01-周期性，三角函数表示复杂函数</a>
          </li>
        
          <li>
            <a href="/2018/05/11/NumPy%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">NumPy入门教程</a>
          </li>
        
          <li>
            <a href="/2018/05/03/Docker%E5%85%A5%E9%97%A8Part6-%E5%8F%91%E5%B8%83%E4%BD%A0%E7%9A%84app/">Docker入门Part6-发布你的app</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>